{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# minGPT in TensorFlow\n",
        "> How to implement and train a mini version of GPT in TensorFlow\n",
        "\n",
        "- toc: false\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: dzlab\n",
        "- categories: [tensorflow, nlp]"
      ],
      "metadata": {
        "id": "71tNyk1j_5YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[minGPT](https://github.com/karpathy/minGPT) is a PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) model. It is a small, clean, interpretable, and educational implementation of the GPT model, with only about 300 lines of code. The goal of the repository is to provide a simple and easy-to-understand example of how the GPT model works.\n",
        "\n",
        "The minGPT repository includes a training script that can be used to train a GPT model on a dataset of text. The training script is also relatively simple, with only about 100 lines of code.\n",
        "\n",
        "The minGPT repository is a valuable resource for anyone who wants to learn more about the GPT model or how to implement it in PyTorch. It is also a good starting point for anyone who wants to experiment with GPT-style language models.\n",
        "\n",
        "\n",
        "In this article, we will replicate the implementation of minGPT in TensorFlow."
      ],
      "metadata": {
        "id": "Z-t7tJmIBrTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "2P4DYwtkBkqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's install some dependencies and setup a model experiementation tracking tool."
      ],
      "metadata": {
        "id": "876B4e8mCwNz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB_0scyCbE9a"
      },
      "source": [
        "%%capture\n",
        "%%bash\n",
        "pip install -q tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "COMET_API_KEY = os.getenv(\"COMET_API_KEY\") or \"COMET_API_KEY\"\n",
        "COMET_WORKSPACE = os.getenv(\"COMET_WORKSPACE\") or \"COMET_WORKSPACE\""
      ],
      "metadata": {
        "id": "G558n3OMAV1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1movJOOuv41t"
      },
      "source": [
        "%%capture\n",
        "!pip install -q comet_ml\n",
        "from comet_ml import Experiment\n",
        "\n",
        "experiment = Experiment(api_key=COMET_API_KEY, project_name=\"mingpt\", workspace=COMET_WORKSPACE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMznCnZ9bgYU"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Layer, LayerNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "In this section we will download the training data and create configuration for transforming it to a shape that our model expects."
      ],
      "metadata": {
        "id": "B6xEiWy9DUlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we define a base class for GPT model configuration (for training) to define common parameters such as vocabulary size, dropout rates for the embedding layer, the residual connections, and the attention layer.\n",
        "\n",
        "We also define a sub-class that sets the model architecture parameters like number of heads."
      ],
      "metadata": {
        "id": "Ze0BbydADWBw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbgaDWSnb80D"
      },
      "source": [
        "class GPTConfig:\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class GPT1Config(GPTConfig):\n",
        "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
        "    n_layer = 12\n",
        "    n_head = 12\n",
        "    n_embd = 768"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the data ingestion and processing pipeline. It uses `tf.keras.utils.Sequence` class to create a dataset of characters for a GPT model. It takes as input a string that contains the text that will be used to create the dataset.\n",
        "\n",
        "The class defines two methods:\n",
        "- `__len__` that returns the number of batches in the dataset\n",
        "- `__getitem__` that returns a batch of data from the dataset.\n",
        "\n",
        "A batch is created inside `__getitem__` by looping up to the `number of sequences that will be included in each batch`. On each iteration, the method randomly selects a spot in the dataset and creates a chunk of data that is `maximum length of a sequence + 1` characters long. Each chunk is then converted to a list of integers, where each integer represents the index of the character in the vocabulary, also known as TOKEN ID."
      ],
      "metadata": {
        "id": "XORFKE5NEb6B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZLFnaMJcBOj"
      },
      "source": [
        "class CharDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self, data, block_size, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    chars = sorted(list(set(data)))\n",
        "    data_size, vocab_size = len(data), len(chars)\n",
        "    print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "    self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "    self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "    self.block_size = block_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return math.ceil(len(self.data) / (self.block_size + 1))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    xb, yb = [], []\n",
        "    # get one batch\n",
        "    for _ in range(self.batch_size):\n",
        "      # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
        "      i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
        "      chunk = self.data[i:i+self.block_size+1]\n",
        "      dix = [self.stoi[s] for s in chunk]\n",
        "      xb.append(dix[:-1])\n",
        "      yb.append(dix[1:])\n",
        "    x = tf.convert_to_tensor(xb, dtype=tf.int32)\n",
        "    y = tf.convert_to_tensor(yb, dtype=tf.int32)\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the training configuration like number of training epochs, batch size, etc."
      ],
      "metadata": {
        "id": "gWhTwgSyGI_9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSSj0sC3LPuu"
      },
      "source": [
        "class TrainerConfig:\n",
        "  # optimization parameters\n",
        "  max_epochs = 10\n",
        "  batch_size = 64\n",
        "  learning_rate = 3e-4\n",
        "  betas = (0.9, 0.95)\n",
        "  grad_norm_clip = 1.0\n",
        "  weight_decay = 0.1 # only applied on matmul weights\n",
        "  # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "  lr_decay = False\n",
        "  warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "  final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "  # checkpoint settings\n",
        "  ckpt_path = None\n",
        "  num_workers = 0 # for DataLoader\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    for k,v in kwargs.items():\n",
        "      setattr(self, k, v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1QnR3MtoOfn"
      },
      "source": [
        "block_size = 128 # spatial extent of the model for its context\n",
        "batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we download the training dataset, which is based on karpathy's [tinyshakespeare]('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')."
      ],
      "metadata": {
        "id": "dGUMS_8FGSDy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbcGEH2ochGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "517a32e5-27ad-4e32-bae0-25d37a90d8c8"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "tinyshakespeare = tf.keras.utils.get_file('tinyshakespeare', url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the text data into a TensorFlow dataset."
      ],
      "metadata": {
        "id": "se_MO-2-GeXI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKxfaCYscp0z"
      },
      "source": [
        "raw_dataset = tf.data.TextLineDataset(tinyshakespeare)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2jayyxVdLxo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f51c5966-cbcb-401f-aa92-b8fae2b325de"
      },
      "source": [
        "for line in raw_dataset.take(5):\n",
        "  print(line.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:'\n",
            "b'Before we proceed any further, hear me speak.'\n",
            "b''\n",
            "b'All:'\n",
            "b'Speak, speak.'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create a dataset of characters for a GPT model using our previously defined `CharDataset` class. The CharDataset object will create a TensorFlow Dataset of batches of data. Then we create a traing specific dataset that is shuffled and batched."
      ],
      "metadata": {
        "id": "t5TvMhIAG30T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmCrFycck61r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41f81251-7769-40bc-d8df-56d1bbe070f9"
      },
      "source": [
        "text = open(tinyshakespeare, 'r').read()\n",
        "char_dataset = CharDataset(text, block_size, batch_size)\n",
        "dataset = raw_dataset.map(char_dataset.__getitem__)\n",
        "train_ds = dataset.shuffle(buffer_size=100).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going any further, we inspect an item/batch of the dataset to make sure everything is good."
      ],
      "metadata": {
        "id": "2BF84OCvGuKG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ux1Eq4M_Wsd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "7c29ae95-b6b1-4e1b-d994-ba98b3387c92"
      },
      "source": [
        "char_dataset[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 128), dtype=int32, numpy=\n",
              " array([[10,  0, 32, ..., 43, 52, 45],\n",
              "        [ 1, 46, 39, ...,  1, 50, 39],\n",
              "        [39, 52, 42, ..., 42, 47, 58],\n",
              "        ...,\n",
              "        [ 8,  0,  0, ..., 43, 49, 11],\n",
              "        [ 0,  9,  1, ..., 53, 59, 50],\n",
              "        [ 1, 39, 52, ..., 53, 53, 42]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(64, 128), dtype=int32, numpy=\n",
              " array([[ 0, 32, 46, ..., 52, 45, 43],\n",
              "        [46, 39, 42, ..., 50, 39, 52],\n",
              "        [52, 42,  1, ..., 47, 58,  6],\n",
              "        ...,\n",
              "        [ 0,  0, 28, ..., 49, 11,  0],\n",
              "        [ 9,  1, 23, ..., 59, 50, 42],\n",
              "        [39, 52, 42, ..., 53, 42,  1]], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCliuMN4oXEd"
      },
      "source": [
        "config = GPTConfig(char_dataset.vocab_size, char_dataset.block_size, n_layer=8, n_head=8, n_embd=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHqxBkOHwu1b"
      },
      "source": [
        "## Model\n",
        "\n",
        "In this section, we create the architecture of minGPT."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we define a class that implements a causal self-attention mechanism for GPT models."
      ],
      "metadata": {
        "id": "WL7gUjESHyhV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8pijjH2ojXB"
      },
      "source": [
        "class CausalSelfAttention(Layer):\n",
        "  def __init__(self, config, **kwargs):\n",
        "    super(CausalSelfAttention, self).__init__(**kwargs)\n",
        "    # key, query, value projections for all heads\n",
        "    self.key = Dense(config.n_embd)\n",
        "    self.query = Dense(config.n_embd)\n",
        "    self.value = Dense(config.n_embd)\n",
        "    # regularization\n",
        "    self.attn_drop = Dropout(config.attn_pdrop)\n",
        "    self.resid_drop = Dropout(config.resid_pdrop)\n",
        "    # output projection\n",
        "    self.proj = Dense(config.n_embd)\n",
        "    self.n_head = config.n_head\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    pass\n",
        "\n",
        "  def call(self, x):\n",
        "    B, T, C = x.shape.as_list()\n",
        "\n",
        "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "    k = tf.transpose(tf.reshape(self.key(x), shape=(-1, T, self.n_head, C//self.n_head)), perm=(0, 2, 1, 3)) # (B, nh, T, hs)\n",
        "    q = tf.transpose(tf.reshape(self.query(x), shape=(-1, T, self.n_head, C//self.n_head)), perm=(0, 2, 1, 3)) # (B, nh, T, hs)\n",
        "    v = tf.transpose(tf.reshape(self.value(x), shape=(-1, T, self.n_head, C//self.n_head)), perm=(0, 2, 1, 3)) # (B, nh, T, hs)\n",
        "\n",
        "    # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "    ones = tf.ones((self.n_head, T, T))\n",
        "    mask = tf.math.scalar_mul(1e-10, (tf.linalg.band_part(ones, 0, -1) - tf.linalg.band_part(ones, 0, 0))) # remove the diagonal\n",
        "\n",
        "    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "    att = tf.math.scalar_mul((1.0 / math.sqrt(k.shape.as_list()[-1])), tf.linalg.matmul(q, tf.transpose(k, perm=(0, 1, 3, 2))))\n",
        "    att = tf.linalg.band_part(att, -1, 0) + mask\n",
        "    att = tf.nn.softmax(att, axis=-1)\n",
        "    att = self.attn_drop(att)\n",
        "    y = tf.linalg.matmul(att, v) # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs) [128,8,128,128], [128,8,1,64]\n",
        "    y = tf.reshape(tf.transpose(y, perm=(0, 2, 1, 3)), shape=(-1, T, C)) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.resid_drop(self.proj(y))\n",
        "    return y\n",
        "\n",
        "  def get_config(self):\n",
        "    # Implement get_config to enable serialization. This is optional.\n",
        "    base_config = super(CausalSelfAttention, self).get_config()\n",
        "    return dict(list(base_config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ9uJj7kCzcJ"
      },
      "source": [
        "Check the shape of the output of `CausalSelfAttention` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9bCv_9GxOZ8"
      },
      "source": [
        "B, T, C = 1, block_size, 512\n",
        "nh, hs = config.n_head, C//config.n_head\n",
        "x = tf.random.uniform((B, T, C))\n",
        "y = CausalSelfAttention(config)(x)\n",
        "\n",
        "assert y.shape == (B, T, C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create a class to represent one GPT block"
      ],
      "metadata": {
        "id": "OWtPIeEIH7XL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qz9ee96rYbx"
      },
      "source": [
        "class Block(Layer):\n",
        "  def __init__(self, config, **kwargs):\n",
        "    super(Block, self).__init__(**kwargs)\n",
        "    self.ln1 = LayerNormalization()\n",
        "    self.ln2 = LayerNormalization()\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    # mlp\n",
        "    self.mlp_lin1 = Dense(4 * config.n_embd)\n",
        "    self.mlp_lin2 = Dense(config.n_embd)\n",
        "    self.mlp_drop = Dropout(config.resid_pdrop)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = x + self.attn(self.ln1(x))\n",
        "    x = x + self.mlp_drop(self.mlp_lin2(tf.nn.gelu(self.mlp_lin1(self.ln2(x))))) #self.mlp(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "    # Implement get_config to enable serialization. This is optional.\n",
        "    base_config = super(Block, self).get_config()\n",
        "    return dict(list(base_config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjcI8Qs4CwVS"
      },
      "source": [
        "Check the shape of the output of the transformer `Block` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kS0SNaay7kh"
      },
      "source": [
        "B, T, C = 1, block_size, 512\n",
        "nh, hs = config.n_head, C//config.n_head\n",
        "x = tf.random.uniform((B, T, C))\n",
        "y = Block(config)(x)\n",
        "\n",
        "assert y.shape == (B, T, C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create the GPT architecture as a tf.Keras Layer:"
      ],
      "metadata": {
        "id": "LGWZn4etIK0A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUYqNQynr1-0"
      },
      "source": [
        "class GPT(Layer):\n",
        "  def __init__(self, config, **kwargs):\n",
        "    super(GPT, self).__init__(**kwargs)\n",
        "    # input embedding stem\n",
        "    self.tok_emb = Embedding(config.vocab_size, config.n_embd)\n",
        "    self.pos_emb = tf.Variable(tf.zeros((1, config.block_size, config.n_embd)))\n",
        "    self.drop = Dropout(config.embd_pdrop)\n",
        "    # transformer\n",
        "    self.blocks = [Block(config) for _ in range(config.n_layer)]\n",
        "    # decoder head\n",
        "    self.ln_f = LayerNormalization()\n",
        "    self.head = Dense(config.vocab_size, use_bias=False)\n",
        "\n",
        "  def call(self, x):\n",
        "    b, t = x.shape.as_list()\n",
        "    token_embeddings = self.tok_emb(x)\n",
        "    position_embeddings = self.pos_emb[:, :t, :]\n",
        "    x = self.drop(token_embeddings + position_embeddings)\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.head(x)\n",
        "    return logits\n",
        "\n",
        "  def get_config(self):\n",
        "    # Implement get_config to enable serialization. This is optional.\n",
        "    base_config = super(GPT, self).get_config()\n",
        "    return dict(list(base_config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMsRoXdqCelb"
      },
      "source": [
        "Check the shape of the output of `GPT` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn3laaA5zaM0"
      },
      "source": [
        "B, T, C = 1, block_size, 512\n",
        "nh, hs = config.n_head, C//config.n_head\n",
        "x = tf.random.uniform((B, T))\n",
        "y = GPT(config)(x)\n",
        "assert y.shape == (B, T, char_dataset.vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we create also create the GPT architecture as a tf.Keras Model:"
      ],
      "metadata": {
        "id": "CBJA2FEsITcS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj7BVMQqvPbU"
      },
      "source": [
        "class GPT(tf.keras.Model):\n",
        "  def __init__(self, config, **kwargs):\n",
        "    super(GPT, self).__init__(**kwargs)\n",
        "    self.block_size = config.block_size\n",
        "    # input embedding stem\n",
        "    self.tok_emb = Embedding(config.vocab_size, config.n_embd)\n",
        "    self.pos_emb = tf.Variable(tf.zeros((1, config.block_size, config.n_embd)))\n",
        "    self.drop = Dropout(config.embd_pdrop)\n",
        "    # transformer\n",
        "    self.blocks = [Block(config) for _ in range(config.n_layer)]\n",
        "    # decoder head\n",
        "    self.ln_f = LayerNormalization()\n",
        "    self.head = Dense(config.vocab_size, use_bias=False, activation='softmax')\n",
        "\n",
        "  def get_block_size(self):\n",
        "    return self.block_size\n",
        "\n",
        "  def call(self, x):\n",
        "    b, t = x.shape.as_list()\n",
        "    token_embeddings = self.tok_emb(x)\n",
        "    position_embeddings = self.pos_emb[:, :t, :]\n",
        "    x = self.drop(token_embeddings + position_embeddings)\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.head(x)\n",
        "    return logits\n",
        "\n",
        "  def get_config(self):\n",
        "    # Implement get_config to enable serialization. This is optional.\n",
        "    base_config = super(GPT, self).get_config()\n",
        "    return dict(list(base_config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Z28jS9Chmv"
      },
      "source": [
        "## Training\n",
        "\n",
        "In this section, we will training the previously created GPT model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build and visualize and summary of the model"
      ],
      "metadata": {
        "id": "xsaLu7dKIinF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzzG3ZIRNxdt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "b906577c-e786-4572-ced5-0de8f9a99477"
      },
      "source": [
        "model = GPT(config)\n",
        "model.build(input_shape=(None, config.block_size))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"gpt_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  33280     \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "block_9 (Block)              multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "block_10 (Block)             multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "block_11 (Block)             multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "block_12 (Block)             multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "block_13 (Block)             multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "block_14 (Block)             multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "block_15 (Block)             multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "block_16 (Block)             multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "layer_normalization_35 (Laye multiple                  1024      \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            multiple                  33280     \n",
            "=================================================================\n",
            "Total params: 25,352,192\n",
            "Trainable params: 25,352,192\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wlzk-SNL343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "cd26e58e-8d64-455a-8382-e1d1266dd437"
      },
      "source": [
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFMAAAA8CAYAAAAaEIaPAAAABmJLR0QA/wD/AP+gvaeTAAAEZ0lEQVR4nO2c3yu7bRzH33vGtpvsG2maWERkpfmZNT9y4EjiALNIOUNJixNO5GQ7tFIciBwzxF9AS6GsTH6V1ELCCmXN2m27niMrv9nzMQ/f61U7ufa5rut9v7ruu133vW4JY4yBQ4H9n+9O8JvgMgnhMgnhMgmJedqwtraGkZGR78jyo7Db7c/anq3Mk5MTzM3NRSXQT+T09PRVP89W5gMvmecAs7OzaGlpefE7fs0khMskhMskhMskhMskhMskhMskhMskhMskhMskhMskhMskhMskhMsk5K+QGQqFYLPZYDAYvnSeXy/z8PAQVVVV6Ovrg8/n+9K5/vcy7+7uIl5RLpcLAwMD6O7uRkFBAXGy50RFJmMMdrsdExMTn+47NTWFy8vLiObV6XSYn59HW1sb5HJ5RGN8BnKZwWAQVqsVubm5EAQBycnJyMzMhNVqhdFoBACMjo5CoVBApVKhq6sLarUaCoUCBoMBGxsb4bHMZjP6+/txdHQEiUSC7Oxs6ri0sCfMzMywF5o/jMViYVKplC0tLTGfz8ecTidLSUlh1dXVj+o6OztZfHw829vbY36/n+3u7rLS0lKWkJDAjo+Pw3WNjY0sKysr4jwPlJWVMZ1O95/HecPPLPnKXFxcRHFxMerr6yEIAoqKitDQ0ACHw4FAIPCoNiYmBnl5eZDL5dBqtRgfH8ft7S2mp6epY0UFcpl+vx/syX/BgsEgYmNjIZVK3+xbUlKCuLg4HBwcUMeKCuQya2tr4XQ6sbS0hLu7O2xubmJxcRF1dXXvygQAuVwOj8dDHSsqvPrcPFKGh4fhdDrR0dEBr9cLtVoNo9EIi8Xybl9RFHFzc4O0tDTqWFGBXObu7i6Ojo7g8XgQE/O54VdWVsAYg16vp44VFchP856eHmg0Gni93ndrQ6EQrq+vcX9/j+3tbZjNZmg0GnR0dIRrkpKScHZ2BrfbjdvbW4iiSB2ZDHKZVqsVOzs7SExMhEQigUQigUwmg1arxcLCwqNav9+P/Px8CIKAyspK5OTkYHl5+dEP7O7ubqhUKmi1WtTW1uLq6urDWdbX11FRUYHU1FRsbGzA5XJBrVajvLwcDoeD7JgfID/N9/f30d7eDpvNFm4LBAIYGBhAa2srrq+vIQgCACAhIQGnp6dvjldYWAi32x1RFr1ej9XV1Yj6RgKpzPPzc/T29mJra+tRu0wmg0ajgSiKEEUxLDMYDFJO/+2QnuaCICA2NhZTU1O4uLiAKIo4OzvD5OQkhoaGYDKZoFQqIx7/4OAgfOl462MymQiP6hN8Yrv0IRwOB6upqWFKpZJJpVL2588fZjAY2NjYGBNFkTHG2ODgIJPJZAwAy8jIYHa7PeL5os1b20lymb+dqO7N/2a4TEK4TEK4TEK4TEK4TEK4TEK4TEK4TEK4TEK4TEK4TEK4TEJevTnc3NwczRw/hreeDDxbmenp6WhqavrSQD+ZtLS0V/1IGOOv4iGCv4qHEi6TEC6TEC6TkH8BNmGu3iDXVRkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create the training configuration object and use it to train the model."
      ],
      "metadata": {
        "id": "lAHs-r1LIr3a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEx5Ke1rLbKq"
      },
      "source": [
        "tconf = TrainerConfig(\n",
        "  max_epochs=10, batch_size=batch_size, learning_rate=6e-4,\n",
        "  lr_decay=True, warmup_tokens=batch_size*20, final_tokens=2*len(char_dataset)*block_size,\n",
        "  num_workers=4)\n",
        "opt = tf.optimizers.Adam(\n",
        "  learning_rate = tconf.learning_rate,\n",
        "  beta_1        = tconf.betas[0],\n",
        "  beta_2        = tconf.betas[1]\n",
        ")\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMlypvwjKHDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "863467fe-2d7c-480b-99e2-a3d821da9e66"
      },
      "source": [
        "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
        "model.fit_generator(char_dataset, epochs=tconf.max_epochs, steps_per_epoch=len(char_dataset) // batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "135/135 [==============================] - 3114s 23s/step - loss: 1.2835 - accuracy: 0.6048\n",
            "Epoch 2/10\n",
            "135/135 [==============================] - 3105s 23s/step - loss: 0.7002 - accuracy: 0.7724\n",
            "Epoch 3/10\n",
            "135/135 [==============================] - 3278s 24s/step - loss: 0.4487 - accuracy: 0.8505\n",
            "Epoch 4/10\n",
            "135/135 [==============================] - 3222s 24s/step - loss: 0.3204 - accuracy: 0.8922\n",
            "Epoch 5/10\n",
            "135/135 [==============================] - 3177s 24s/step - loss: 0.2457 - accuracy: 0.9170\n",
            "Epoch 6/10\n",
            "135/135 [==============================] - 3287s 24s/step - loss: 0.2024 - accuracy: 0.9316\n",
            "Epoch 7/10\n",
            "135/135 [==============================] - 3155s 23s/step - loss: 0.1676 - accuracy: 0.9432\n",
            "Epoch 8/10\n",
            "135/135 [==============================] - 3133s 23s/step - loss: 0.1448 - accuracy: 0.9513\n",
            "Epoch 9/10\n",
            "135/135 [==============================] - 3120s 23s/step - loss: 0.1273 - accuracy: 0.9574\n",
            "Epoch 10/10\n",
            " 28/135 [=====>........................] - ETA: 40:51 - loss: 0.1137 - accuracy: 0.9621"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: the training will take a very long time to finish even when using the toy dataset we have choosen."
      ],
      "metadata": {
        "id": "2G2Ky4AsI57G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-tQ383d4UI2"
      },
      "source": [
        "## Evaluation\n",
        "Let's sample some character-level Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLb269F2Kvh-"
      },
      "source": [
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "  block_size = model.get_block_size()\n",
        "  for k in tqdm(range(steps)):\n",
        "    T = x.shape.as_list()[0]\n",
        "    if T <= block_size:\n",
        "      x_cond = x\n",
        "    else:\n",
        "      # take the last sequence in x\n",
        "      x_cond = x[-block_size:]\n",
        "    xb = tf.expand_dims(x_cond, axis=0) # add batch dimension\n",
        "    probs = model(xb) # get probs for next token\n",
        "    # sample from the distribution or take the most likely\n",
        "    probs = probs[:, -1]\n",
        "    if sample:\n",
        "      ix = tf.random.categorical(probs, num_samples=1, seed=31, dtype=tf.int32)\n",
        "      ix = tf.squeeze(ix, axis=0)\n",
        "    else:\n",
        "      probs = tf.squeeze(probs, axis=0) # remove the batch dimension\n",
        "      _, ix = tf.math.top_k(probs, k=1, sorted=True, name=None)\n",
        "    # append to the sequence and continue\n",
        "    x = tf.concat([x, ix], axis=0)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_GCnNW4SHFg"
      },
      "source": [
        "### With Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG6uQjawNxsX"
      },
      "source": [
        "context = \"O God, O God!\"\n",
        "x = tf.convert_to_tensor([char_dataset.stoi[s] for s in context], dtype=tf.int32)\n",
        "y = sample(model, x, 200, temperature=1.0, sample=True, top_k=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_A-cBDQNg5w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "af60dd44-1f3e-4c91-f9ff-8f98829244eb"
      },
      "source": [
        "completion = ''.join([char_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O God, O God!X,YbJbiLWMMb\n",
            "kW&&; NkALu.gvjrXp$wJoQE SI L.VMEF$URwukqSOdqMN'awYP\n",
            "xfJEJB??GLDJy\n",
            "kDs:;!VYpKlikuweNuozoCoieuynEtkvfLTLr$!ajQC!Ftho;raUyo\n",
            "yq otz$KEsPU-'JGAWMUo3ooIshnK3A;PMg'lxIFrAmXKrFJnDWD mol-HGP.dwPn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOEUjaFVSLTw"
      },
      "source": [
        "### Without sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ1IN0fyUmLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b431660b-35dc-415a-9e10-056e557f66dc"
      },
      "source": [
        "context = \"O God, O God!\"\n",
        "x = tf.convert_to_tensor([char_dataset.stoi[s] for s in context], dtype=tf.int32)\n",
        "y = sample(model, x, 200, temperature=1.0, sample=False, top_k=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:43<00:00,  4.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWtAolRl4g6t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6c30c688-b57c-480c-c86b-42705c01e619"
      },
      "source": [
        "completion = ''.join([char_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O God, O God! owowowowowowowowowowowowowowowowowowowowd gg ggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggowowithawh awhak ak ak akid ad ak adad ad ad atititititititititititititi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## That's all folks\n",
        "In this article we saw how to implement the OpenAI GPT (Generative Pretrained Transformer) model in TensorFlow following the [minGPT](https://github.com/karpathy/minGPT) implemenetation. Our version is a a toy implementation with some key features:\n",
        "- It is a small, clean, and interpretable implementation of the GPT model.\n",
        "- It includes a training configuration that can be used to train a GPT model on a dataset of text.\n",
        "- It is a valuable resource for anyone who wants to learn more about the GPT model or how to implement it in TensorFlow.\n",
        "\n",
        "I hope you enjoyed this article, feel free to leave a comment or reach out on twitter [@bachiirc](https://twitter.com/bachiirc)."
      ],
      "metadata": {
        "id": "FYHzXpliCZ-W"
      }
    }
  ]
}