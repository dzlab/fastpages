{
  
    
        "post0": {
            "title": "",
            "content": "Those attacks can be serious with devastating results (e.g. fooling a self-driving), so known how to check that your model is robust against them is crutial before rolling the model to production. . . In this article, we will see how to implement one simple and effective Adversial Attack called Fast Gradient Signed Method (FGSM). . FGSM is implemented by performing the following steps: . Pass an legitimage image over a model and capture the gradients | Determining the direction (sign) of the gradient at each pixel of the original image | Using that information create an adversarial pattern in a way to aximize the loss at each pixel | Multiply this pattern by a small scaling value and apply it (i.e. add or substruct) to the original image | The result is an Adversial image that looks to the human eye very similar to the original image | . Although this attack performs small imperceptible perturbations into an image, we will see in action how this technique can easily fool a pre-trained model. . Before starting, make sure to have OpenCV installed as it will be used for performing the perturbations . #collapse !pip install opencv-contrib-python . . Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30) Requirement already satisfied: numpy&gt;=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5) . Import the dependencies . #collapse import cv2 import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.applications.nasnet import * from tensorflow.keras.losses import CategoricalCrossentropy . . Load the pre-trained model (in this case NASNetMobile) and freeze its weights so they are not updated after a forward pass. . pretrained_model = NASNetMobile(include_top=True, weights=&#39;imagenet&#39;) pretrained_model.trainable = False . We need to define a function to preprocess an image with same preprocessing steps performed on the images used to train the target model (e.g. resizing) . def preprocess(image, target_shape): image = tf.cast(image, tf.float32) image = tf.image.resize(image, target_shape) image = preprocess_input(image) image = image[None, :, :, :] return image . We also need to define a function that turns the model output probabilities into actual labels . def get_imagenet_label(probabilities): return decode_predictions(probabilities, top=1)[0][0] . Define a function to create the adversarial pattern that will be used later to generate the adversial image . def generate_adv_pattern(model, input_image, input_label, loss_function): with tf.GradientTape() as tape: tape.watch(input_image) prediction = model(input_image) loss = loss_function(input_label, prediction) gradient = tape.gradient(loss, input_image) signed_gradient = tf.sign(gradient) return signed_gradient . The pattern is nothing but a tensor with the sign of the gradient in each element. i.e., signed_gradient is: . -1 for negative gradients (i.e. value below 0) | 1 for positive gradients (i.e. value above 0) | 0 if the gradient is 0. | . We need a legitimate image so we can apply perturbations on it, you can download a test image on your own or use this Pug image . !curl https://image.freepik.com/free-vector/angry-dog-pug-prisoner-graphic-illustration_41984-29.jpg -o dog.jpg . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 102k 100 102k 0 0 776k 0 --:--:-- --:--:-- --:--:-- 776k . Pass the image through the pretrained model to get output probabilities . image = tf.io.read_file(&#39;dog.jpg&#39;) image = tf.image.decode_jpeg(image) image = preprocess(image, pretrained_model.input.shape[1:-1]) image_probs = pretrained_model.predict(image) . We need to One-hot encode the ground truth label of the original image, then use it to generate an adversarial pattern . cce_loss = CategoricalCrossentropy() pug_index = 254 label = tf.one_hot(pug_index, image_probs.shape[-1]) label = tf.reshape(label, (1, image_probs.shape[-1])) disturbances = generate_adv_pattern(pretrained_model, image, label, cce_loss) . . Note: if you want try this techniques on a different image of a different class than pug then look up the corresponding class index in this imagenet class indices - link . Next, we define a utility function to generate an adversial version of an image based on a disturbance/noise and a scalar . def generate_adv_image(image, epsilon, disturbances): corrupted_image = image + epsilon * disturbances return tf.clip_by_value(corrupted_image, -1, 1) . We also, need another utility function to ensure before plotting that the input tensor in the [0, 255] range, as well as in BGR space, which is the one used by OpenCV . def postprocess_adv_image(adv_image): adv_image = adv_image.numpy()[0] * 0.5 + 0.5 adv_image = (adv_image * 255).astype(&#39;uint8&#39;) return cv2.cvtColor(adv_image, cv2.COLOR_RGB2BGR) . We can use the previous utility function to show what the noise looks like before applying it to the original image . disturbed_image = postprocess_adv_image(disturbances) plt.imshow(disturbed_image) plt.axis(&#39;off&#39;) plt.title(&quot;noise&quot;); . Finally, we can put all the previous steps together to perform a series of adversarial attacks different values of epsilon (which is used as noise multiplier). . After that we disply the corresponding adversial example as well the predicted label and confidence provided by the victim model . figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, epsilon in enumerate([0, 0.005, 0.01, 0.1, 0.15, 0.2]): adv_image = generate_adv_image(image, epsilon, disturbances) prediction = pretrained_model.predict(adv_image) _, label, confidence = get_imagenet_label(prediction) adv_image = postprocess_adv_image(adv_image) confidencePct = confidence * 100 title = f&#39;Epsilon = {epsilon:.3f}, {label} ({confidencePct:.2f}%)&#39; row, col = int(index / 3), index % 3 axis[row, col].imshow(adv_image) axis[row, col].set_title(title) axis[row, col].axis(&#39;off&#39;) . As you can be see from the output images and the resulting classification, an imperceptible variation in the pixel values produced a drastically different response from the network. . For epsilon = 0 (no attack), the label is Pug with a 60.18% confidence | When epsilon = 0.005 (a very small perturbation), the label changes to Muzzle, with a 98.46% confidence! | The situation gets worse as we increase the magnitude of epsilon. | .",
            "url": "https://dzlab.github.io/notebooks/2022/01/19/2021-01-29-Gradient_Signed_Method.html",
            "relUrl": "/2022/01/19/2021-01-29-Gradient_Signed_Method.html",
            "date": " â€¢ Jan 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "DeepDream with Tensorflow",
            "content": "In an effort to understand the inners of deep neural networks and what information those models learn after been trained on a specific task, a team at Google come up with what&#39;s know today as DeepDream. The experiment results were interesting, it appears that neural networks trained to classify an image (i.e. discriminate between different classes) contains enough information to generate new images that can be artistic. . . In this notebook we will implement DeepDream in Tensorflow from scratch and test it on couple of images. . The DeepDream model is simply built on a backbone model trained on imagenet and the output of the model will be few activation layers picked from this backbone model. Then we run an image through this model, compute the gradients with respect to the activations of those output layers, then modify the original image to increase the magnitude of the activations which as a result will magnify the patterns in the image. . In our case, the backbone model is Inception V3 (which you can read more about it here). The following driagram shows an overview of this model architecture: . . #collapse import numpy as np import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.applications.inception_v3 import * from tensorflow.keras.preprocessing.image import * from tqdm import tqdm import matplotlib.pyplot as plt . . To create the DeepDream model, we define the following helper function that uses InceptionV3 from TF Hub and uses the input layers as output of the model. Note that by default we are picking ramdom activation layers from the InceptionV3 model. . def create_model(layers=None): if not layers: layers = [&#39;mixed3&#39;, &#39;mixed5&#39;] base = InceptionV3(weights=&#39;imagenet&#39;, include_top=False) outputs = [base.get_layer(name).output for name in layers] return Model(base.input, outputs) . We need to define few utils functions that we will use to process the images, for example scaling an image by a factor or converting tensor image into a numpy array. . def convert_tensor_to_nparray(image): image = 255 * (image + 1.0) / 2.0 image = tf.cast(image, tf.uint8) image = np.array(image) return image def scale_image(image, base_shape, scale, factor): new_shape = tf.cast(base_shape * (scale **factor), tf.int32) image = tf.image.resize(image,new_shape).numpy() image = preprocess_input(image) image = tf.convert_to_tensor(image) return image . Next, we define a function to calculate the loss which is simly the average of the activations resulting from doing a forward pass with the input image. . def calculate_loss(model, image): image_batch = tf.expand_dims(image, axis=0) activations = model(image_batch) if len(activations) == 1: activations = [activations] losses = [] for activation in activations: loss = tf.math.reduce_mean(activation) losses.append(loss) total_loss = tf.reduce_sum(losses) return total_loss . To calculate the gradients, we need to perform a forward pass inside a tf.GradientTape(), after that we simply update the image to maximize the activations in the next run. . Note how we are using the tf.function annotation which will improve the performance significantly. . @tf.function def forward_pass(model, image, steps, step_size): loss = tf.constant(0.0) for _ in range(steps): with tf.GradientTape() as tape: tape.watch(image) loss = calculate_loss(model, image) gradients = tape.gradient(loss, image) gradients /= tf.math.reduce_std(gradients) + 1e-8 image = image + gradients * step_size image = tf.clip_by_value(image, -1, 1) return image, loss . All the previous functions are combined and used in the following funciton which will take an input image and a model, and construct the final dreaming looking picture. . The other input parameters to this function have the following purpose: . octave_scale the scale by which we&#39;ll increase the size of an image | octave_power_factors the factor that will be applied as a power to the previous scale parameter. | steps the number of iteration we run the image over the deepdream model | step_size will be used to scale the gradients before adding them to the image | . def dream(dreamer_model, image, octave_scale=1.30, octave_power_factors=None, steps=100, step_size=0.01): if not octave_power_factors: octave_power_factors = [*range(-2, 3)] image = tf.constant(np.array(image)) base_shape = tf.shape(image)[:-1] base_shape = tf.cast(base_shape, tf.float32) steps = tf.constant(steps) step_size = tf.constant(tf.convert_to_tensor(step_size)) for factor in octave_power_factors: image = scale_image(image, base_shape, octave_scale, factor) image, _ = forward_pass(dreamer_model, image, steps, step_size) image = convert_tensor_to_nparray(image) base_shape = tf.cast(base_shape, tf.int32) image = tf.image.resize(image, base_shape) image = tf.image.convert_image_dtype(image /255.0,dtype=tf.uint8) image = np.array(image) return np.array(image) . Now we can apply this DeepDream model to an image, you can pick any one you like. . !curl https://miro.medium.com/max/1750/1*E-S7Y80jIFuZ03xyc89fnA.jpeg -o image.jpeg . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 68250 100 68250 0 0 812k 0 --:--:-- --:--:-- --:--:-- 812k . def load_image(path): image = load_img(path) image = img_to_array(image) return image . def show_image(image): plt.imshow(image) plt.show() . original_image = load_image(&#39;image.jpeg&#39;) show_image(original_image / 255.0) . First, lets try the image with all default parameters, and activation layers . model = create_model() output_image = dream(model, original_image) show_image(output_image) . WARNING:tensorflow:5 out of the last 5 calls to &lt;function forward_pass at 0x7f095a587a70&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. . Let&#39;s try with different activation layers . Note: the first layers tend to learn basic patterns (e.g. lines and shapes), while layers closer to the output learn more complex patterns as they combine the previous basic patterns. . model = create_model([&#39;mixed2&#39;, &#39;mixed5&#39;, &#39;mixed7&#39;]) output_image = dream(model, original_image) show_image(output_image) . WARNING:tensorflow:6 out of the last 6 calls to &lt;function forward_pass at 0x7f095a587a70&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. . The result is a softer image as a result of adding more layers. . Finally, let&#39;s try some custom octaves power factors . model = create_model() output_image = dream(model, original_image, octave_power_factors=[-3, -1, 0, 3]) show_image(output_image) . The resulting image seem to have less noise and more heterogeneous patterns, a mixture of both high- and low-level patterns, as well as a better color distribution. . As an exercise, try different parameters and you will see that the results vary widely: . Play with different step_size values, a big value will result in much noise added to the original images | Use higher layers to obtain pictures with less noise and more nuanced patterns. | Use more octaves, which will result into more images passed to the model at different scales. | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/artistic/generative/2021/01/18/DeepDream-Tensorflow.html",
            "relUrl": "/tensorflow/artistic/generative/2021/01/18/DeepDream-Tensorflow.html",
            "date": " â€¢ Jan 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Handling Imbalanced Datasets",
            "content": ". %%capture %%bash pip install imbalanced-learn . from collections import Counter import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import make_classification from sklearn.svm import LinearSVC from imblearn.pipeline import make_pipeline from imblearn.base import BaseSampler from imblearn.under_sampling import RandomUnderSampler from imblearn.over_sampling import (SMOTE, RandomOverSampler) from imblearn.combine import SMOTEENN, SMOTETomek import warnings warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) . Helper functions . The following function will be used to create toy dataset. It using the make_classification from scikit-learn but fixing some parameters. . def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3, class_sep=0.8, n_clusters=1): return make_classification(n_samples=n_samples, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=n_classes, n_clusters_per_class=n_clusters, weights=list(weights), class_sep=class_sep, random_state=0) . The following function will be used to plot the sample space after resampling to illustrate the characterisitic of an algorithm. . def plot_resampling(X, y, sampling, ax): X_res, y_res = sampling.fit_resample(X, y) ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor=&#39;k&#39;) # make nice plotting ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.get_xaxis().tick_bottom() ax.get_yaxis().tick_left() ax.spines[&#39;left&#39;].set_position((&#39;outward&#39;, 10)) ax.spines[&#39;bottom&#39;].set_position((&#39;outward&#39;, 10)) return Counter(y_res) . The following function will be used to plot the decision function of a classifier given some data. . def plot_decision_function(X, y, clf, ax): plot_step = 0.02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, alpha=0.4) ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor=&#39;k&#39;) . Influence of the balancing ratio . We will first illustrate the influence of the balancing ratio on some toy data using a linear SVM classifier. Greater is the difference between the number of samples in each class, poorer are the classfication results. . #collapse fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) ax_arr = (ax1, ax2, ax3, ax4) weights_arr = ((0.01, 0.01, 0.98), (0.01, 0.05, 0.94), (0.2, 0.1, 0.7), (0.33, 0.33, 0.33)) for ax, weights in zip(ax_arr, weights_arr): X, y = create_dataset(n_samples=1000, weights=weights) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax) ax.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) fig.tight_layout() . . Under-sampling . Under-sampling by selecting existing samples . There are two major groups of selection algorithms: . the controlled under-sampling methods and | the cleaning under-sampling methods. | With the controlled under-sampling methods, the number of samples to be selected can be specified. RandomUnderSampler is the most naive way of performing such selection by randomly selecting a given number of samples by the targetted class. . #collapse fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6)) X, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax1) ax1.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) sampler = RandomUnderSampler(random_state=0) clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_decision_function(X, y, clf, ax2) ax2.set_title(&#39;Decision function for {}&#39;.format(sampler.__class__.__name__)) plot_resampling(X, y, sampler, ax3) ax3.set_title(&#39;Resampling using {}&#39;.format(sampler.__class__.__name__)) fig.tight_layout() . . Over-sampling . Random over-sampling . Random over-sampling with RandomOverSampler can be used to repeat some samples and balance the number of samples between the dataset. It can be seen that with this trivial approach the boundary decision is already less biaised toward the majority class. . #collapse fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7)) X, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94)) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax1) ax1.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) pipe = make_pipeline(RandomOverSampler(random_state=0), LinearSVC()) pipe.fit(X, y) plot_decision_function(X, y, pipe, ax2) ax2.set_title(&#39;Decision function for RandomOverSampler&#39;) fig.tight_layout() . . /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) . More advanced over-sampling . Instead of repeating the same samples when over-sampling, we can use some specific heuristic instead like SMOTE or ADASYN. . #collapse # Make an identity sampler for illustrations class FakeSampler(BaseSampler): _sampling_type = &#39;bypass&#39; def _fit_resample(self, X, y): return X, y fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15)) X, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94)) sampler = FakeSampler() clf = make_pipeline(sampler, LinearSVC()) plot_resampling(X, y, sampler, ax1) ax1.set_title(&#39;Original data - y={}&#39;.format(Counter(y))) ax_arr = (ax2, ax3, ax4) for ax, sampler in zip(ax_arr, (RandomOverSampler(random_state=0), SMOTE(random_state=0), ADASYN(random_state=0))): clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_resampling(X, y, sampler, ax) ax.set_title(&#39;Resampling using {}&#39;.format(sampler.__class__.__name__)) fig.tight_layout() . . Illustration of the sample generation in the over-sampling algorithm . #collapse rng = np.random.RandomState(18) f, ax = plt.subplots(1, 1, figsize=(8, 8)) # generate some data points y = np.array([3.65284, 3.52623, 3.51468, 3.22199, 3.21]) z = np.array([0.43, 0.45, 0.6, 0.4, 0.211]) y_2 = np.array([3.3, 3.6]) z_2 = np.array([0.58, 0.34]) # plot the majority and minority samples ax.scatter(z, y, label=&#39;Minority class&#39;, s=100) ax.scatter(z_2, y_2, label=&#39;Majority class&#39;, s=100) idx = rng.randint(len(y), size=2) annotation = [r&#39;$x_i$&#39;, r&#39;$x_{zi}$&#39;] for a, i in zip(annotation, idx): ax.annotate(a, (z[i], y[i]), xytext=tuple([z[i] + 0.01, y[i] + 0.005]), fontsize=15) # draw the circle in which the new sample will generated radius = np.sqrt((z[idx[0]] - z[idx[1]]) ** 2 + (y[idx[0]] - y[idx[1]]) ** 2) circle = plt.Circle((z[idx[0]], y[idx[0]]), radius=radius, alpha=0.2) ax.add_artist(circle) # plot the line on which the sample will be generated ax.plot(z[idx], y[idx], &#39;--&#39;, alpha=0.5) # create and plot the new sample step = rng.uniform() y_gen = y[idx[0]] + step * (y[idx[1]] - y[idx[0]]) z_gen = z[idx[0]] + step * (z[idx[1]] - z[idx[0]]) ax.scatter(z_gen, y_gen, s=100) ax.annotate(r&#39;$x_{new}$&#39;, (z_gen, y_gen), xytext=tuple([z_gen + 0.01, y_gen + 0.005]), fontsize=15) # make the plot nicer with legend and label ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.get_xaxis().tick_bottom() ax.get_yaxis().tick_left() ax.spines[&#39;left&#39;].set_position((&#39;outward&#39;, 10)) ax.spines[&#39;bottom&#39;].set_position((&#39;outward&#39;, 10)) ax.set_xlim([0.2, 0.7]) ax.set_ylim([3.2, 3.7]) plt.xlabel(r&#39;$X_1$&#39;) plt.ylabel(r&#39;$X_2$&#39;) plt.legend() plt.tight_layout() plt.show() . . Automatically created module for IPython interactive environment . Combine-sampling . Comparison of the combination of over- and under-sampling algorithms . This example shows the effect of applying an under-sampling algorithms after SMOTE over-sampling. In the literature, Tomek&#39;s link SMOTETomek and edited nearest neighbours SMOTEENN are the two methods which have been used and are available in imbalanced-learn. . #collapse fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 25)) X, y = create_dataset(n_samples=1000, weights=(0.1, 0.2, 0.7)) ax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6)) for ax, sampler in zip(ax_arr, ( SMOTE(random_state=0), SMOTEENN(random_state=0), SMOTETomek(random_state=0))): clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_decision_function(X, y, clf, ax[0]) ax[0].set_title(&#39;Decision function for {}&#39;.format( sampler.__class__.__name__)) plot_resampling(X, y, sampler, ax[1]) ax[1].set_title(&#39;Resampling using {}&#39;.format( sampler.__class__.__name__)) fig.tight_layout() plt.show() . . .",
            "url": "https://dzlab.github.io/notebooks/sklearn/classification/sampling/2020/10/31/Handling_Imbalanced_Datasets.html",
            "relUrl": "/sklearn/classification/sampling/2020/10/31/Handling_Imbalanced_Datasets.html",
            "date": " â€¢ Oct 31, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Preprocessing Structured data in TF 2.3",
            "content": "In TF 2.3, Keras adds new preprocessing layers for image, text and strucured data. The following notebook explores those new layers for dealing with Structured data. . For a complete example of how to use the new preprocessing layer for Structured data check the Keras example - link. . Structured data . Generate some random data for playing with and seeing what is the output of the preprocessing layers. . xdf = pd.DataFrame({ &#39;categorical_string&#39;: [&#39;LOW&#39;, &#39;HIGH&#39;, &#39;HIGH&#39;, &#39;MEDIUM&#39;], &#39;categorical_integer_1&#39;: [1, 0, 1, 0], &#39;categorical_integer_2&#39;: [1, 2, 3, 4], &#39;numerical_1&#39;: [2.3, 0.2, 1.9, 5.8], &#39;numerical_2&#39;: [16, 32, 8, 60] }) ydf = pd.DataFrame({&#39;target&#39;: [0, 0, 0, 1]}) ds = tf.data.Dataset.from_tensor_slices((dict(xdf), ydf)) for x, y in ds.take(1): print(&#39;X:&#39;, x) print(&#39;y:&#39;, y) . X: {&#39;categorical_string&#39;: &lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;cat1&#39;&gt;, &#39;categorical_integer_1&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;, &#39;categorical_integer_2&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;, &#39;numerical_1&#39;: &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.3&gt;, &#39;numerical_2&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=16&gt;} y: tf.Tensor([0], shape=(1,), dtype=int64) . from tensorflow.keras.layers.experimental.preprocessing import Normalization from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding from tensorflow.keras.layers.experimental.preprocessing import StringLookup . Pre-processing Numercial columns . Preprocessing helper function to encode numercial features, e.g. 0.1, 0.2, etc. . def create_numerical_encoder(dataset, name): # Create a Normalization layer for our feature normalizer = Normalization() # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the statistics of the data normalizer.adapt(feature_ds) return normalizer . # Apply normalization to a numerical feature normalizer = create_numerical_encoder(ds, &#39;numerical_1&#39;) normalizer.apply(xdf[name].values) . &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy= array([[-0.7615536], [-1.2528784], [-0.7615536], [-1.2528784]], dtype=float32)&gt; . Pre-processing Integer categorical columns . Preprocessing helper function to encode integer categorical features, e.g. 1, 2, 3 . def create_integer_categorical_encoder(dataset, name): # Create a CategoryEncoding for our integer indices encoder = CategoryEncoding(output_mode=&quot;binary&quot;) # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the space of possible indices encoder.adapt(feature_ds) return encoder . # Apply one-hot encoding to an integer categorical feature encoder1 = create_integer_categorical_encoder(ds, &#39;categorical_integer_1&#39;) encoder1.apply(xdf[&#39;categorical_integer_1&#39;].values) . &lt;tf.Tensor: shape=(4, 2), dtype=float32, numpy= array([[0., 1.], [1., 0.], [0., 1.], [1., 0.]], dtype=float32)&gt; . # Apply one-hot encoding to an integer categorical feature encoder2 = create_integer_categorical_encoder(ds, &#39;categorical_integer_2&#39;) encoder2.apply(xdf[&#39;categorical_integer_2&#39;].values) . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]], dtype=float32)&gt; . Pre-processing String categorical columns . Preprocessing helper function to encode string categorical features, e.g. LOW, HIGH, MEDIUM. . This will applying the following to the input feature: . Create a token to index lookup table | Apply one-hot encoding to the tokens indices | def create_string_categorical_encoder(dataset, name): # Create a StringLookup layer which will turn strings into integer indices index = StringLookup() # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the set of possible string values and assign them a fixed integer index index.adapt(feature_ds) # Create a CategoryEncoding for our integer indices encoder = CategoryEncoding(output_mode=&quot;binary&quot;) # Prepare a dataset of indices feature_ds = feature_ds.map(index) # Learn the space of possible indices encoder.adapt(feature_ds) return index, encoder . # Apply one-hot encoding to an integer categorical feature indexer, encoder3 = create_string_categorical_encoder(ds, &#39;categorical_string&#39;) # Turn the string input into integer indices indices = indexer.apply(xdf[&#39;categorical_string&#39;].values) # Apply one-hot encoding to our indices encoder3.apply(indices) . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[0., 0., 0., 0., 1.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.]], dtype=float32)&gt; . Notice that the string categorical column was hot encoded into 5 tokens whereas in the input dataframe there is only 3 unique values. This is because the indexer adds 2 more tokens. See the vocabulary: . indexer.get_vocabulary() . [&#39;&#39;, &#39;[UNK]&#39;, &#39;cat2&#39;, &#39;cat3&#39;, &#39;cat1&#39;] .",
            "url": "https://dzlab.github.io/notebooks/2020/08/02/Preprocessing_structured_data_in_TF_2_3.html",
            "relUrl": "/2020/08/02/Preprocessing_structured_data_in_TF_2_3.html",
            "date": " â€¢ Aug 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Annotation with TensorFlow Object Detection API",
            "content": "import matplotlib import matplotlib.pyplot as plt import numpy as np from PIL import Image from six import BytesIO from pathlib import Path import tensorflow as tf %matplotlib inline . Install Object Detection API . !git clone --depth 1 https://github.com/tensorflow/models . Cloning into &#39;models&#39;... remote: Enumerating objects: 2797, done. remote: Counting objects: 100% (2797/2797), done. remote: Compressing objects: 100% (2439/2439), done. remote: Total 2797 (delta 563), reused 1405 (delta 322), pack-reused 0 Receiving objects: 100% (2797/2797), 57.73 MiB | 31.67 MiB/s, done. Resolving deltas: 100% (563/563), done. . # Install the Object Detection API %%bash cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install -q . . object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used. . from object_detection.utils import colab_utils from object_detection.utils import visualization_utils as viz_utils . Download data for annotation . Download an image dataset to annotate, for instance The Oxford-IIIT Pet Dataset (link) . %%bash curl -O https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz tar xzf images.tar.gz . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 755M 100 755M 0 0 29.8M 0 0:00:25 0:00:25 --:--:-- 31.4M . paths = list([str(p) for p in Path(&#39;images&#39;).glob(&#39;*&#39;)]) . Utility method to load an image from path into a uint8 numpy array with shape (height, width, channels), where channels=3 for RGB. . def load_image_into_numpy_array(path): img_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(img_data)) (im_width, im_height) = image.size image_np = np.array(image.getdata(), dtype=np.uint8) return image_np.reshape((im_height, im_width, 3)) . For testing select a random subset of the images (we don&#39;t want load all images) . sample_size = 10 sample_paths = [paths[np.random.randint(len(paths))] for i in range(10)] . Annotate images . Load the selected random images into numpy arrays . images_np = [load_image_into_numpy_array(str(p)) for p in sample_paths] . boxes = [] colab_utils.annotate(images_np, box_storage_pointer=boxes) . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . Define the indexes for the categories . category_index = { 0: {&#39;id&#39;: 0, &#39;name&#39;: &#39;dog&#39;}, 1: {&#39;id&#39;: 1, &#39;name&#39;: &#39;cat&#39;} } . Inspect the annotations . Wrapper function to visualize the original image along with the best detected box. It takes are arguments: . image_np: uint8 numpy array with shape (img_height, img_width, 3) | boxes: a numpy array of shape [N, 4] | classes: a numpy array of shape [N]. Note that class indices are 1-based, and match the keys in the label map. | scores: a numpy array of shape [N] or None. If scores=None, then this function assumes that the boxes to be plotted are groundtruth boxes and plot all boxes as black with no classes or scores. | category_index: a dict containing category dictionaries (each holding category index id and category name name) keyed by category indices. | figsize: (optional) size for the figure. | image_name: (optional) name for the image file. | . def plot_detections(image_np, boxes, classes, scores, category_index, figsize=(12, 16), image_name=None): image_np_with_annotations = image_np.copy() viz_utils.visualize_boxes_and_labels_on_image_array( image_np_with_annotations, boxes, classes, scores, category_index, use_normalized_coordinates=True, min_score_thresh=0.8) if image_name: plt.imsave(image_name, image_np_with_annotations) else: plt.imshow(image_np_with_annotations) . I manually inspected the images (that&#39;s the 100% scores below) to get the class for each one, note that: . 0 is for a cat image | 1 is for a dog image | . classes = [ np.ones(shape=(1), dtype=np.int32), np.ones(shape=(1), dtype=np.int32), np.zeros(shape=(1), dtype=np.int32), np.ones(shape=(1), dtype=np.int32), np.zeros(shape=(1), dtype=np.int32) ] # give boxes a score of 100% scores = np.array([1.0], dtype=np.float32) . Vizualise the images with their bounding boxes . plt.figure(figsize=(30, 15)) for idx in range(5): plt.subplot(2, 3, idx+1) plot_detections(images_np[idx], boxes[idx], classes[idx], scores, category_index) plt.show() .",
            "url": "https://dzlab.github.io/notebooks/2020/07/19/Image_Annotation_on_Colab.html",
            "relUrl": "/2020/07/19/Image_Annotation_on_Colab.html",
            "date": " â€¢ Jul 19, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Captum PyTorch Vision Example",
            "content": "Captum (translates to comprehension in Latin) is an open source library for model interpretability. It helps model developers understand which features are contributing to their modelâ€™s output. It implements state-of-the-art interpretability algorithms in PyTorch, and provide them as an easy to use API. . The rest of this notebook illustrates how to use this library to interpret a fastai v2 based image classification model. . Setup . %%capture %%bash pip install fastai2 pip install psutil pip install captum . from matplotlib.colors import LinearSegmentedColormap from fastai2.vision.all import * from captum.attr import IntegratedGradients from captum.attr import GradientShap from captum.attr import Occlusion from captum.attr import NoiseTunnel from captum.attr import visualization as viz . Data . Download data for training an image classification model . path = untar_data(URLs.PETS)/&#39;images&#39; imgs = get_image_files(path) def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, imgs, valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) . Model . Fine tune an Imagenet-based model on the new images dataset. . learn = cnn_learner(dls, resnet34, metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.169865 | 0.020558 | 0.004736 | 00:23 | . epoch train_loss valid_loss error_rate time . 0 | 0.060073 | 0.026544 | 0.008119 | 00:24 | . Basic interpertation of the model prediected classes vs. actual ones. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Visualize top losses, e.g. to check if the images themselves are correctly annotated. . interp.plot_top_losses(5, nrows=1) . Store/Restore the fine tuned model . learn.export(&#39;/tmp/model.pkl&#39;) learn_inf = load_learner(&#39;/tmp/model.pkl&#39;) . Select a random image and plot it . idx = random.randint(0, len(imgs)) . image = PILImage.create(imgs[idx]) image . image = learn_inf.dls.after_item(image) image = learn_inf.dls.after_batch(image) . pred,pred_idx,probs = learn_inf.predict(imgs[idx]) pred, pred_idx, probs . (&#39;False&#39;, tensor(0), tensor([9.9998e-01, 2.0485e-05])) . Interpretability . Let&#39;s use Captum.ai to interpret the model predictions and to have a visual on where the network focused more in the input image. . Gradient-based attribution . Integrated Gradients is an interpretaility technique based on the approximation of integral gradients. The basic implementation works as followss: . Given as input target image and a baseline image (usually a black image), generate multiple images between both starting from darker to lighter. | Do forward pass with each of those images to predict a class and calculate the gradient. | Approximate the integral of the gradients of all those images | . The following example, illustrates how to use Captum IntegratedGradients to compute the attributions using Integrated Gradients and visualize them on the target image. . integrated_gradients = IntegratedGradients(learn_inf.model) attr_ig = integrated_gradients.attribute(image, target=pred_idx, n_steps=200) . transposed_attr_ig = np.transpose(attr_ig.squeeze().numpy(), (1,2,0)) transposed_image = np.transpose(image.squeeze().numpy(), (1,2,0)) . default_cmap = LinearSegmentedColormap.from_list(&#39;custom blue&#39;, [(0, &#39;#ffffff&#39;), (0.25, &#39;#000000&#39;), (1, &#39;#000000&#39;)], N=256) _ = viz.visualize_image_attr(transposed_attr_ig, transposed_image, method=&#39;heat_map&#39;, cmap=default_cmap, show_colorbar=True, sign=&#39;positive&#39;, outlier_perc=1) . For a better visual of the attribution, the images between baseline and target are sampled using a noise tunnel (by adding gaussian noise). And when the gradients are calulcated, we smoothe them by calculating their mean squared. . noise_tunnel = NoiseTunnel(integrated_gradients) attributions_ig_nt = noise_tunnel.attribute(image, n_samples=10, nt_type=&#39;smoothgrad_sq&#39;, target=pred_idx) transposed_attr_ig_nt = np.transpose(attributions_ig_nt.squeeze().numpy(), (1,2,0)) _ = viz.visualize_image_attr_multiple(transposed_attr_ig_nt, transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], cmap=default_cmap, show_colorbar=True) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Occlusion-based attribution . Occlusion-based attribution is a different interepretation technique based on perturbing parts of the original image (e.g. by zeroing) and computing how this affects the model decision. This technique is implemented by: . Slide a window of shape (h, w) on the target image with a stride s | Replace the window with a baseline (e.g. with black) and qunatify the effect on model decision. | Repeat previous steps until all of the target image is covered. | . Simiarly to convolution, this technique can become very slow when used in large models and large input images. . As a first exercise, we run a sliding window of size 15x15 and a stride of 8 along both image dimensions. For each window, we occlude the image with a baseline value of 0. . occlusion = Occlusion(learn_inf.model) attr_occ = occlusion.attribute(image, strides = (3, 8, 8), target=pred_idx, sliding_window_shapes=(3,15, 15), baselines=0) . _ = viz.visualize_image_attr_multiple(np.transpose(attr_occ.squeeze().numpy(), (1,2,0)), transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], show_colorbar=True, outlier_perc=2, ) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Let&#39;s try different window shape and sliding window and visuzalize the result, by rescaling back to the shape of the original image. . Experimenting with different windows is important because: . Different shape may lead to a significaly different result. | Larger windows is useful when the input image presents some local coherence. | . In this case, we run a sliding window of size 60x60 and a stride of 50 along both image dimensions. For each window, we occlude the image with a baseline value of 0. . occlusion = Occlusion(learn_inf.model) attr_occ = occlusion.attribute(image, strides = (3, 50, 50), target=pred_idx, sliding_window_shapes=(3,60, 60), baselines=0) _ = viz.visualize_image_attr_multiple(np.transpose(attr_occ.squeeze().numpy(), (1,2,0)), transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], show_colorbar=True, outlier_perc=2, ) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . References . Axiomatic Attribution for Deep Networks - link | Towards better understanding of gradient-based attribution methods for Deep Neural Networks - link | .",
            "url": "https://dzlab.github.io/notebooks/jupyter/2020/04/18/Captum_PyTorch_Vision_Example.html",
            "relUrl": "/jupyter/2020/04/18/Captum_PyTorch_Vision_Example.html",
            "date": " â€¢ Apr 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Mask-RCNN Tensorflow v1 image examples",
            "content": "Project Setup . Make sure we&#39;re running TensorFlow v1 . try: %tensorflow_version 1.x except Exception: pass . TensorFlow 1.x selected. . Install Mask-RCNN model . %%capture %%bash pip install -U git+https://github.com/matterport/Mask_RCNN . Download weights of pretrained Mask-RCNN . !curl -L -o mask_rcnn_balloon.h5 https://github.com/matterport/Mask_RCNN/releases/download/v2.1/mask_rcnn_balloon.h5?raw=true . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 611 100 611 0 0 2246 0 --:--:-- --:--:-- --:--:-- 2254 100 244M 100 244M 0 0 40.0M 0 0:00:06 0:00:06 --:--:-- 47.2M . import cv2 import math import numpy as np import matplotlib.pyplot as plt import os import sys from mrcnn import utils from mrcnn import model as modellib from mrcnn.config import Config from PIL import Image plt.rcParams[&quot;figure.figsize&quot;]= (10,10) np.set_printoptions(precision=3) . Mask-RCNN setup . # Load the pre-trained model data ROOT_DIR = os.getcwd() MODEL_DIR = os.path.join(ROOT_DIR, &quot;logs&quot;) COCO_MODEL_PATH = os.path.join(ROOT_DIR, &quot;mask_rcnn_coco.h5&quot;) if not os.path.exists(COCO_MODEL_PATH): utils.download_trained_weights(COCO_MODEL_PATH) . Downloading pretrained model to /content/mask_rcnn_coco.h5 ... ... done downloading pretrained model! . class InferenceConfig(Config): &quot;&quot;&quot;Configuration for training on MS COCO. Derives from the base Config class and overrides values specific to the COCO dataset. &quot;&quot;&quot; # Give the configuration a recognizable name NAME = &quot;coco&quot; # Number of images to train with on each GPU. A 12GB GPU can typically # handle 2 images of 1024x1024px. IMAGES_PER_GPU = 1 # Uncomment to train on 8 GPUs (default is 1) GPU_COUNT = 1 # Number of classes (including background) NUM_CLASSES = 1 + 80 # COCO has 80 classes . %%capture # COCO dataset object names model = modellib.MaskRCNN( mode=&quot;inference&quot;, model_dir=MODEL_DIR, config=InferenceConfig() ) model.load_weights(COCO_MODEL_PATH, by_name=True) class_names = [ &#39;BG&#39;, &#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, &#39;motorcycle&#39;, &#39;airplane&#39;, &#39;bus&#39;, &#39;train&#39;, &#39;truck&#39;, &#39;boat&#39;, &#39;traffic light&#39;, &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;parking meter&#39;, &#39;bench&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;sheep&#39;, &#39;cow&#39;, &#39;elephant&#39;, &#39;bear&#39;, &#39;zebra&#39;, &#39;giraffe&#39;, &#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;frisbee&#39;, &#39;skis&#39;, &#39;snowboard&#39;, &#39;sports ball&#39;, &#39;kite&#39;, &#39;baseball bat&#39;, &#39;baseball glove&#39;, &#39;skateboard&#39;, &#39;surfboard&#39;, &#39;tennis racket&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;, &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;, &#39;pizza&#39;, &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;couch&#39;, &#39;potted plant&#39;, &#39;bed&#39;, &#39;dining table&#39;, &#39;toilet&#39;, &#39;tv&#39;, &#39;laptop&#39;, &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;, &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;, &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;, &#39;hair drier&#39;, &#39;toothbrush&#39; ] . The following function will apply to the origianl image, the pixels from the gray image is 0, otherwise keep the pixels from original picture. . # This function is used to change the colorful background information to grayscale. # image[:,:,0] is the Blue channel,image[:,:,1] is the Green channel, image[:,:,2] is the Red channel # mask == 0 means that this pixel is not belong to the object. # np.where function means that if the pixel belong to background, change it to gray_image. # Since the gray_image is 2D, for each pixel in background, we should set 3 channels to the same value to keep the grayscale. def apply_mask(image, mask_image, mask): &quot;&quot;&quot;Helper function to apply a mask to an image.&quot;&quot;&quot; image[:, :, 0] = np.where( mask == 0, mask_image[:, :, 0], image[:, :, 0] ) image[:, :, 1] = np.where( mask == 0, mask_image[:, :, 1], image[:, :, 1] ) image[:, :, 2] = np.where( mask == 0, mask_image[:, :, 2], image[:, :, 2] ) return image . def process_image(image, mask_image, boxes, masks, ids, names, scores, target_label): &quot;&quot;&quot;Helper function to find the object with biggest bounding box and apply mask to it.&quot;&quot;&quot; # max_area will save the largest object for all the detection results max_area = 0 # n_instances saves the amount of all objects n_instances = boxes.shape[0] if not n_instances: print(&#39;NO INSTANCES TO DISPLAY&#39;) else: assert boxes.shape[0] == masks.shape[-1] == ids.shape[0] for i in range(n_instances): if not np.any(boxes[i]): continue # compute the square of each object y1, x1, y2, x2 = boxes[i] square = (y2 - y1) * (x2 - x1) # use label to select the object with given label from all the 80 classes in COCO dataset current_label = names[ids[i]] if target_label is not None or current_label == target_label: # save the largest object in the image as main character # other people will be regarded as background if square &gt; max_area: max_area = square mask = masks[:, :, i] else: continue else: continue # apply mask for the image # by mistake you put apply_mask inside for loop or you can write continue in if also image = apply_mask(image, mask_image, mask) return image . Now the mode is ready to use . !curl -L -o cat_input.jpg https://unsplash.com/photos/7GX5aICb5i4/download?force=true&amp;w=640 . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 232 0 232 0 0 666 0 --:--:-- --:--:-- --:--:-- 666 100 5442k 100 5442k 0 0 10.5M 0 --:--:-- --:--:-- --:--:-- 10.5M . # Credit for the image: https://unsplash.com/photos/7GX5aICb5i4 image = cv2.imread(&#39;./cat_input.jpg&#39;) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7f4274e4b710&gt; . Application 1: Grayscale the background . Recognize the main character, keep it colorfull while grayscal the background of the image. . # Use cvtColor to accomplish image transformation from RGB image to gray image mask_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) mask_image = np.stack([mask_image, mask_image, mask_image], axis=2) plt.imshow(mask_image) . &lt;matplotlib.image.AxesImage at 0x7f4274e21a90&gt; . results = model.detect([image], verbose=0) output_dict = results[0] rois, class_ids, scores, masks = output_dict.values() . result = process_image( image.copy(), mask_image, rois, masks, class_ids, class_names, scores, &#39;cat&#39; ) plt.imshow(result) . &lt;matplotlib.image.AxesImage at 0x7f427458c860&gt; . Let&#39;s take this cat to the beach . !curl -L -o beach.jpg https://unsplash.com/photos/DH_u2aV3nGM/download?force=true&amp;w=640 . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 242 0 242 0 0 733 0 --:--:-- --:--:-- --:--:-- 731 100 4000k 100 4000k 0 0 9828k 0 --:--:-- --:--:-- --:--:-- 9828k . image_beach = cv2.imread(&#39;./beach.jpg&#39;) image_beach = cv2.cvtColor(image_beach, cv2.COLOR_BGR2RGB) plt.imshow(image_beach) . &lt;matplotlib.image.AxesImage at 0x7f427456cc18&gt; . Reshape the new mask image so that it matches the size of the original image. . image_beach = cv2.resize(image_beach, dsize=(image.shape[1], image.shape[0]), interpolation = cv2.INTER_AREA) . result = process_image( image.copy(), image_beach, rois, masks, class_ids, class_names, scores, &#39;cat&#39; ) plt.imshow(result) . &lt;matplotlib.image.AxesImage at 0x7f42744ccba8&gt; . Think of the possibilites :) .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/segmentation/2020/03/29/Mask_RCNN_Tensorflow_v1_image_examples.html",
            "relUrl": "/tensorflow/vision/segmentation/2020/03/29/Mask_RCNN_Tensorflow_v1_image_examples.html",
            "date": " â€¢ Mar 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Text classification with BERT using TF Text",
            "content": "Setup . try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass . TensorFlow 2.x selected. . Install dependencies . %%capture %%bash pip install -U tensorflow-text . Import modules . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split import tensorflow as tf import tensorflow_text as text import tensorflow_hub as hub import tensorflow_datasets as tfds from tensorflow.keras.layers import Dense, Dropout, Input from tensorflow.keras.models import Model . Set default options for modules . pd.set_option(&#39;display.max_colwidth&#39;, -1) . GPU check . num_gpus_available = len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;)) print(&quot;Num GPUs Available: &quot;, num_gpus_available) assert num_gpus_available &gt; 0 . Num GPUs Available: 1 . config = { &#39;seed&#39;: 31, &#39;batch_size&#39;: 64, &#39;epochs&#39;: 10, &#39;max_seq_len&#39;: 128 } . Data . Download the pretrained BERT model . BERT_URL = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1&quot; bert_layer = hub.KerasLayer(BERT_URL, trainable=False) vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() print(f&#39;BERT vocab is stored at : {vocab_file}&#39;) print(f&#39;BERT model is case sensitive: {do_lower_case}&#39;) . BERT vocab is stored at : b&#39;/tmp/tfhub_modules/03d6fb3ce1605ad9e5e9ed5346b2fb9623ef4d3d/assets/vocab.txt&#39; BERT model is case sensitive: True . Load the vocab file that corresponds to the pretrained BERT . def load_vocab(vocab_file): &quot;&quot;&quot;Load a vocabulary file into a list.&quot;&quot;&quot; vocab = [] with tf.io.gfile.GFile(vocab_file, &quot;r&quot;) as reader: while True: token = reader.readline() if not token: break token = token.strip() vocab.append(token) return vocab vocab = load_vocab(vocab_file) . Use BERT vocab to create a word to index lookup table . def create_vocab_table(vocab, num_oov=1): &quot;&quot;&quot;Create a lookup table for a vocabulary&quot;&quot;&quot; vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64) init = tf.lookup.KeyValueTensorInitializer(keys=vocab, values=vocab_values, key_dtype=tf.string, value_dtype=tf.int64) vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov, lookup_key_dtype=tf.string) return vocab_table vocab_lookup_table = create_vocab_table(vocab) . Use BERT vocab to create a index to word lookup table . def create_index2word(vocab): # Create a lookup table for a index to token vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64) init = tf.lookup.KeyValueTensorInitializer(keys=vocab_values, values=vocab) return tf.lookup.StaticHashTable(initializer=init, default_value=tf.constant(&#39;unk&#39;), name=&quot;index2word&quot;) index2word = create_index2word(vocab) . Check out the indices for the following tokens . vocab_lookup_table.lookup(tf.constant([&#39;[PAD]&#39;, &#39;[UNK]&#39;, &#39;[CLS]&#39;, &#39;[SEP]&#39;, &#39;[MASK]&#39;])) . &lt;tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 0, 100, 101, 102, 103])&gt; . Check out the token corresponding to an index . index2word.lookup(tf.constant([0], dtype=&#39;int64&#39;)).numpy() . [b&#39;[PAD]&#39;] . Create a BERT tokenizer using TF Text . tokenizer = text.BertTokenizer( vocab_lookup_table, token_out_type=tf.int64, lower_case=do_lower_case ) . Lookup for the BERT token IDs for padding and start/end of sentence. . PAD_ID = vocab_lookup_table.lookup(tf.constant(&#39;[PAD]&#39;)) # padding token CLS_ID = vocab_lookup_table.lookup(tf.constant(&#39;[CLS]&#39;)) # class token SEP_ID = vocab_lookup_table.lookup(tf.constant(&#39;[SEP]&#39;)) # sequence separator token . Preprocessing . Define the logic to preprocess data and format it as required by BERT . def preprocess(record): review, label = record[&#39;text&#39;], record[&#39;label&#39;] # process review to calculate BERT input ids, mask, type_ids = preprocess_bert_input(review) return (ids, mask, type_ids), label def preprocess_bert_input(review): # calculate tokens ID ids = tokenize_text(review, config[&#39;max_seq_len&#39;]) # calculate mask mask = tf.cast(ids &gt; 0, tf.int64) mask = tf.reshape(mask, [-1, config[&#39;max_seq_len&#39;]]) # calculate tokens type ID zeros_dims = tf.stack(tf.shape(mask)) type_ids = tf.fill(zeros_dims, 0) type_ids = tf.cast(type_ids, tf.int64) return (ids, mask, type_ids) def tokenize_text(review, seq_len): # convert text into token ids tokens = tokenizer.tokenize(review) # flatten the output ragged tensors tokens = tokens.merge_dims(1, 2)[:, :seq_len] # Add start and end token ids to the id sequence start_tokens = tf.fill([tf.shape(review)[0], 1], CLS_ID) end_tokens = tf.fill([tf.shape(review)[0], 1], SEP_ID) tokens = tokens[:, :seq_len - 2] tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1) # truncate sequences greater than MAX_SEQ_LEN tokens = tokens[:, :seq_len] # pad shorter sequences with the pad token id tokens = tokens.to_tensor(default_value=PAD_ID) pad = seq_len - tf.shape(tokens)[1] tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID) # and finally reshape the word token ids to fit the output # data structure of TFT return tf.reshape(tokens, [-1, seq_len]) . Dataset . Download the dataset from TF Hub and process it . train_ds, valid_ds = tfds.load(&#39;imdb_reviews&#39;, split=[&#39;train&#39;, &#39;test&#39;], shuffle_files=True) train_ds = train_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) valid_ds = valid_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) train_ds, valid_ds = train_ds.map(preprocess), valid_ds.map(preprocess) . Model . input_ids = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_ids&quot;) input_mask = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_mask&quot;) input_type_ids = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_type_ids&quot;) pooled_output, sequence_output = bert_layer([input_ids, input_mask, input_type_ids]) drop_out = Dropout(0.3, name=&quot;dropout&quot;)(pooled_output) output = Dense(1, activation=&#39;sigmoid&#39;, name=&quot;linear&quot;)(drop_out) model = Model(inputs=[input_ids, input_mask, input_type_ids], outputs=output) model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . model.summary() . Model: &#34;model_1&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_mask (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_type_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ keras_layer (KerasLayer) [(None, 768), (None, 109482241 input_ids[0][0] input_mask[0][0] input_type_ids[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 768) 0 keras_layer[1][0] __________________________________________________________________________________________________ linear (Dense) (None, 1) 769 dropout[0][0] ================================================================================================== Total params: 109,483,010 Trainable params: 769 Non-trainable params: 109,482,241 __________________________________________________________________________________________________ . Training . model.fit(train_ds, validation_data=valid_ds, epochs=config[&#39;epochs&#39;]) . Epoch 1/10 391/391 [==============================] - 499s 1s/step - loss: 0.6654 - accuracy: 0.6016 - val_loss: 0.5977 - val_accuracy: 0.7028 Epoch 2/10 391/391 [==============================] - 510s 1s/step - loss: 0.6063 - accuracy: 0.6712 - val_loss: 0.5650 - val_accuracy: 0.7282 Epoch 3/10 391/391 [==============================] - 510s 1s/step - loss: 0.5839 - accuracy: 0.6969 - val_loss: 0.5494 - val_accuracy: 0.7362 Epoch 4/10 391/391 [==============================] - 511s 1s/step - loss: 0.5730 - accuracy: 0.7025 - val_loss: 0.5388 - val_accuracy: 0.7455 Epoch 5/10 391/391 [==============================] - 510s 1s/step - loss: 0.5696 - accuracy: 0.7058 - val_loss: 0.5376 - val_accuracy: 0.7417 Epoch 6/10 391/391 [==============================] - 510s 1s/step - loss: 0.5613 - accuracy: 0.7146 - val_loss: 0.5268 - val_accuracy: 0.7517 Epoch 7/10 391/391 [==============================] - 510s 1s/step - loss: 0.5608 - accuracy: 0.7130 - val_loss: 0.5233 - val_accuracy: 0.7544 Epoch 8/10 391/391 [==============================] - 510s 1s/step - loss: 0.5625 - accuracy: 0.7106 - val_loss: 0.5217 - val_accuracy: 0.7555 Epoch 9/10 391/391 [==============================] - 510s 1s/step - loss: 0.5603 - accuracy: 0.7125 - val_loss: 0.5199 - val_accuracy: 0.7535 Epoch 10/10 391/391 [==============================] - 510s 1s/step - loss: 0.5567 - accuracy: 0.7159 - val_loss: 0.5150 - val_accuracy: 0.7591 . &lt;tensorflow.python.keras.callbacks.History at 0x7f2fffddba58&gt; . Evaluation . test_text_ds = tfds.load(&#39;imdb_reviews&#39;, split=&#39;unsupervised&#39;, shuffle_files=True) test_ds = test_text_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) test_ds = test_ds.map(preprocess) . Check how test text is tokenized . test_text = [record[&#39;text&#39;].numpy() for record in test_text_ds.take(10)] . ids = tokenize_text(test_text, config[&#39;max_seq_len&#39;]) . tokens = [b&#39; &#39;.join(tokens_array) for tokens_array in index2word.lookup(ids).numpy()] . pd.DataFrame({&#39;tokens&#39;: tokens}) . tokens . 0 b&quot;[CLS] spoil ##er - now knowing the ending i find it so clever that the whole movie takes place in a motel and each character has a different room . even sane people have many different aspects to their personality , but they don &#39; t let them become dominant - - they are controlled . malcolm &#39; s various personalities and needs were person ##ified in each character . the prostitute mother ( amanda pee ##t ) , the part of him who hated her for being a prostitute ( larry ) , the loving mother he wish he had , the loving father he wish he had , the selfish part of himself ( actress ) , the violent part of his personality ( ray [SEP]&quot; | . 1 b&quot;[CLS] i knew about this film long before i saw it . in fact , i had to buy the dvd in order to see it because no video store carried it . i didn &#39; t mind spending the $ 12 to buy it used because i collect off the wall movies . the new limited edition double dvd has great sound and visually not bad . i found myself laughing much more then &lt; br / &gt; &lt; br / &gt; jolt ##ing in fear , although there were a few scenes were i was startled . &lt; br / &gt; &lt; br / &gt; if you enjoy off the wall 70s sci - fi / horror movies , you probably will eat this one [SEP]&quot; | . 2 b&quot;[CLS] this movie is really really awful . it &#39; s as bad as zombie 90 well maybe not that bad but pretty close . if your a fan of the italian horror movies then you might like this movie . i thought that it was dam near un ##watch ##able of course i &#39; m not a fan of the italian movies . the only italian movie that was ok was jungle holocaust . which is one over ##rated movie . this film is way over ##rated . but let &#39; s get started with how horrible this film really is shall we . the acting is goofy and horrible . the effects suck . no plot with this movie . little gore which is the [SEP]&quot; | . 3 b&#39;[CLS] wait a minute . . . yes i do . &lt; br / &gt; &lt; br / &gt; the director of &#39; the breed &#39; has obviously seen terry gill ##iam &#39; s &#39; brazil &#39; a few too many times and asked himself the question , &quot; if &#39; brazil &#39; had been an ill - conceived tale about vampires in the near future , what would it be like ? &quot; well , i &#39; ll tell ya , it &#39; d be like 91 minutes of a swedish whore kicking you in the groin , only not as satisfying . the dialogue was laced with gr ##at ##uit ##ous curse words and tri ##te one - liner ##s , and whoever edited this [SEP]&#39; | . 4 b&quot;[CLS] this is the type of movie that &#39; s just barely involving enough for one viewing , but i don &#39; t think i could stand to watch it again . it looks and plays like a mid - seventies tv movie , only with some gr ##at ##uit ##ous sex and violence thrown in . &lt; br / &gt; &lt; br / &gt; i agree with several previous posters - - her ##ve ville ##chai ##ze is not very menacing , and at times even comes off as un ##int ##ended comedy . at least the other two villains make up for that . also , it was jolt ##ing to see jonathan fr ##id is such a pedestrian role , which definitely under - [SEP]&quot; | . 5 b&quot;[CLS] i like sci - fi movies and everything &#39; bout it and aliens , so i watched this flick . nothing new , nothing special , average acting , typical h . b . davenport &#39; story , weak and che ##es ##y fx &#39; s , bad ending of movie , but still the author idea is good . the marines on lost island find the truth about alien landing there and truth about past - experiments on them . they die one after one , some of them were killed by lonely alien , and others by human enemies . ufo effects , when it flees and crush ##es are bad , too . the voices of angry alien are funny , too . [SEP]&quot; | . 6 b&quot;[CLS] i was lucky enough to see a preview of this film tonight . this was a very cool , eerie film . well acted , especially by ska ##rs ##gard who played his role of terry glass perfectly . sob ##ies ##ki did a very good job too as it seems to me that she has a bright future ahead of her . the music was well placed but was fairly standard . the use of shadows was quite interesting as well . overall , this was quite a nice surprise considering i &#39; m not much a fan of this genre . 7 / 10 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]&quot; | . 7 b&#39;[CLS] my kids and i love this movie ! ! we think that richard pry ##or and the whole cast did a wonderful job in the movie . it means more to us now since the passing of richard ! ! we will miss his sense of humor . but his movies and shows will stay with us forever ! ! we especially love the parts of brad , frank crawford and ar ##lo pear ! ! they had some one liner ##s in the movie that were great ! ! my son and i love to quote those one liner ##s when we see each other and my daughter will join us when we discuss the movie . we thought the moving guys were terrific . [SEP]&#39; | . 8 b&quot;[CLS] somehow the an ##ima ##trix shorts with the most interesting premises have the worst outcome . mat ##ric ##ulated is the worst of the bunch ( although it &#39; s a close call with program ) , as it takes a great idea ( showing the machines the beauty of mankind by plug ##ging them in ) and turns it into the worst experience of the 9 . &lt; br / &gt; &lt; br / &gt; as i said , the story begins promising and interesting , but ends with a long , long , long sequence of &#39; weird &#39; images , a cross between the famous scenes from 2001 and v ##ga - rain ( who can remember it ) , but not as [SEP]&quot; | . 9 b&quot;[CLS] while holiday ##ing in the basque region of spain , two couples discover a child whose hands are severely miss ##ha ##pen . the child has been gravely mist ##reate ##d , and , as a result , cannot communicate . the two couples reluctantly decide to rescue her and report her circumstances to the authorities . however , severe weather and the dense ##ness of the forest surrounding their holiday home make it impossible for them to make a quick get ##away . soon , the local inhabitants become aware that the girl is missing , and they right ##ly suspect the holiday - makers of taking her . suspicions and paranoia begin to fest ##er , and it isn &#39; t long before violence [SEP]&quot; | . Run prediction on test reviews . result = model.predict(test_ds) . result.shape . (50000, 1) . result_df = pd.DataFrame({&#39;label&#39;: tf.squeeze(result[:10]).numpy(), &#39;text&#39;: test_text}) result_df.head() . label text . 0 0.464566 | b&quot;SPOILER - Now knowing the ending I find it so clever that the whole movie takes place in a motel and each character has a different room. Even sane people have many different aspects to their personality, but they don&#39;t let them become dominant -- they are controlled. Malcolm&#39;s various personalities and needs were personified in each character. The prostitute mother (Amanda Peet), the part of him who hated her for being a prostitute (Larry), the loving mother he wish he had, the loving father he wish he had, the selfish part of himself (actress), the violent part of his personality (Ray Liotta and Busey), the irrational emotions he feels and his need to be loved (Ginnie) and his attempts to control those feelings (Lou), the hurt little boy who sees far too many traumatic things in his life, and of course, John Cusack who seems to represent Malcolm himself trying to analyze and understand all the craziness in his mind, tries to follow the rules (accepting responsibility for the car accident), help others (giving Amanda Peet a ride, and stitching up the mother). Very cleverly done!&quot; | . 1 0.252326 | b&#39;I knew about this film long before I saw it. In fact, I had to buy the DVD in order to see it because no video store carried it. I didn &#39;t mind spending the $12 to buy it used because I collect off the wall movies. The new limited edition double DVD has great sound and visually not bad. I found myself laughing much more then&lt;br /&gt;&lt;br /&gt;jolting in fear, although there were a few scenes were I was startled.&lt;br /&gt;&lt;br /&gt;If you enjoy off the wall 70s sci-fi/horror movies, you probably will eat this one up. I was a little dissapointed at how abrubtly it ended. I wanted the movie to keep going, see how things pan out. The DVD revolution has brought so many&lt;br /&gt;&lt;br /&gt;lost clasics back to life, it is truly wonderful. Blue Sunshine is one of those lost &quot;missing links&quot; of the cinema. Enjoy!&#39; | . 2 0.485239 | b&quot;This movie is really really awful. It&#39;s as bad as Zombie 90 well maybe not that bad but pretty close. If your a fan of the Italian horror movies then you might like this movie. I thought that it was dam near unwatchable of course I&#39;m not a fan of the Italian movies. The only Italian movie that was OK was Jungle holocaust. Which is one overrated movie. This film is way overrated. But let&#39;s get started with how horrible this film really is shall we. The acting is goofy and horrible. The effects suck. No plot with this movie. Little gore which is the only good thing in the film isn&#39;t showed nearly enough to be worth watching this wreck. The zombies are very fake looking. It looks like it&#39;s a bunch of dudes wearing cheap dollar store masks. Please avoid this film at all costs.&quot; | . 3 0.251897 | b&#39;Wait a minute... yes I do.&lt;br /&gt;&lt;br /&gt;The director of &#39;The Breed &#39; has obviously seen Terry Gilliam &#39;s &#39;Brazil &#39; a few too many times and asked himself the question, &quot;If &#39;Brazil &#39; had been an ill-conceived tale about vampires in the near future, what would it be like?&quot; Well, I &#39;ll tell ya, it &#39;d be like 91 minutes of a Swedish whore kicking you in the groin, only not as satisfying. The dialogue was laced with gratuitous curse words and trite one-liners, and whoever edited this piece of crap should be shot. I have no real idea of exactly how the whole thing ended because I &#39;m not really sure what happened during the first part of the film. With so many subplots your head begins to hurt and so much bad acting your head wants to explode this movie should only be viewed with large quantities of beer and at least two other people you can MST3K with. The only thing that made me not stab myself in the eye with a dirty soup spoon was this line: Evil Doctor Guy: &quot;That &#39;s it, you are not James Bond, and I am not Blofeld. No more explanations!&quot; Dude From Jason &#39;s Lyric: &quot;I &#39;m getting paid scale!&quot; The cinematography was shaky at best and the acting was putrid. Also, what was with all the pseudo-1984 posters and PA announcements? The costumes were from the 50 &#39;s, the cars were from the 60 &#39;s, the music was from the 90 &#39;s and I wish I were dead. This movie sucks.&#39; | . 4 0.274131 | b&#39;This is the type of movie that &#39;s just barely involving enough for one viewing, but I don &#39;t think I could stand to watch it again. It looks and plays like a mid-Seventies TV movie, only with some gratuitous sex and violence thrown in.&lt;br /&gt;&lt;br /&gt;I agree with several previous posters -- Herve Villechaize is NOT very menacing, and at times even comes off as unintended comedy. At least the other two villains make up for that. Also, it was jolting to see Jonathan Frid is such a pedestrian role, which definitely under-utilized his enormous talents.&lt;br /&gt;&lt;br /&gt;But I think the basic problem with &quot;Seizure&quot; is in the storyline. The evil trio that are conjured up from Frid &#39;s mind are seen too early and too often. They appear to everyone at once, and announce their (murky) plans too early in the picture. In fact, Stone takes this idea and literally shoves it in the viewer &#39;s face, with a series of challenges for the guests; challenges that it doesn &#39;t seem like they have any chance of winning, anyway. How much more effective would have been keeping the evil ones in the shadows, preying on each house guest in turn, sowing confusion and doubt among the remaining house guests, who don &#39;t know who or what is causing the carnage. By having the trio appear early on, to all the &quot;assembled guests&quot;, and announcing their plan (confusing as that plan is), much potential for tension and suspense are lost.&lt;br /&gt;&lt;br /&gt;Also, a more gradual appearance of the evil ones would indicate Frid is slowing losing control of his subconscious. To have Frid subconsciously conjure up these baddies, because he &#39;s got hidden grudges against his wife and friends, would have been a far more logical plot device. Instead of having Frid play an intended victim from the get-go, it would have worked better to have him slowing becoming helpless to control the menace he &#39;s created, with mixed feelings of guilt and satisfaction as his shallow, superficial friends are killed off. The plot Stone offers up is confusing as to the origins and, most importantly, the motivations of the evil trio, and never gives any explanation why Frid, from whose mind they came from, can exercise absolutely no control over them. Confusing is the word that best sums up the whole picture, and the end feels like a total cheat. Better to have some great showdown in which Frid is finally able to banish the creations of his own tormented mind.&lt;br /&gt;&lt;br /&gt;Oliver Stone has done some notable work in his career, but sadly &quot;Seizure&quot; is not among them.&#39; | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/nlp/2020/03/15/Text_classification_with_BERT_and_TF_Text.html",
            "relUrl": "/tensorflow/nlp/2020/03/15/Text_classification_with_BERT_and_TF_Text.html",
            "date": " â€¢ Mar 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, itâ€™s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://dzlab.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}