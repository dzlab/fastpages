{
  
    
        "post0": {
            "title": "Preprocessing Structured data in TF 2.3",
            "content": "In TF 2.3, Keras adds new preprocessing layers for image, text and strucured data. The following notebook explores those new layers for dealing with Structured data. . For a complete example of how to use the new preprocessing layer for Structured data check the Keras example - link. . Structured data . Generate some random data for playing with and seeing what is the output of the preprocessing layers. . xdf = pd.DataFrame({ &#39;categorical_string&#39;: [&#39;LOW&#39;, &#39;HIGH&#39;, &#39;HIGH&#39;, &#39;MEDIUM&#39;], &#39;categorical_integer_1&#39;: [1, 0, 1, 0], &#39;categorical_integer_2&#39;: [1, 2, 3, 4], &#39;numerical_1&#39;: [2.3, 0.2, 1.9, 5.8], &#39;numerical_2&#39;: [16, 32, 8, 60] }) ydf = pd.DataFrame({&#39;target&#39;: [0, 0, 0, 1]}) ds = tf.data.Dataset.from_tensor_slices((dict(xdf), ydf)) for x, y in ds.take(1): print(&#39;X:&#39;, x) print(&#39;y:&#39;, y) . X: {&#39;categorical_string&#39;: &lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;cat1&#39;&gt;, &#39;categorical_integer_1&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;, &#39;categorical_integer_2&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;, &#39;numerical_1&#39;: &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.3&gt;, &#39;numerical_2&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=16&gt;} y: tf.Tensor([0], shape=(1,), dtype=int64) . from tensorflow.keras.layers.experimental.preprocessing import Normalization from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding from tensorflow.keras.layers.experimental.preprocessing import StringLookup . Pre-processing Numercial columns . Preprocessing helper function to encode numercial features, e.g. 0.1, 0.2, etc. . def create_numerical_encoder(dataset, name): # Create a Normalization layer for our feature normalizer = Normalization() # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the statistics of the data normalizer.adapt(feature_ds) return normalizer . # Apply normalization to a numerical feature normalizer = create_numerical_encoder(ds, &#39;numerical_1&#39;) normalizer.apply(xdf[name].values) . &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy= array([[-0.7615536], [-1.2528784], [-0.7615536], [-1.2528784]], dtype=float32)&gt; . Pre-processing Integer categorical columns . Preprocessing helper function to encode integer categorical features, e.g. 1, 2, 3 . def create_integer_categorical_encoder(dataset, name): # Create a CategoryEncoding for our integer indices encoder = CategoryEncoding(output_mode=&quot;binary&quot;) # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the space of possible indices encoder.adapt(feature_ds) return encoder . # Apply one-hot encoding to an integer categorical feature encoder1 = create_integer_categorical_encoder(ds, &#39;categorical_integer_1&#39;) encoder1.apply(xdf[&#39;categorical_integer_1&#39;].values) . &lt;tf.Tensor: shape=(4, 2), dtype=float32, numpy= array([[0., 1.], [1., 0.], [0., 1.], [1., 0.]], dtype=float32)&gt; . # Apply one-hot encoding to an integer categorical feature encoder2 = create_integer_categorical_encoder(ds, &#39;categorical_integer_2&#39;) encoder2.apply(xdf[&#39;categorical_integer_2&#39;].values) . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]], dtype=float32)&gt; . Pre-processing String categorical columns . Preprocessing helper function to encode string categorical features, e.g. LOW, HIGH, MEDIUM. . This will applying the following to the input feature: . Create a token to index lookup table | Apply one-hot encoding to the tokens indices | def create_string_categorical_encoder(dataset, name): # Create a StringLookup layer which will turn strings into integer indices index = StringLookup() # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the set of possible string values and assign them a fixed integer index index.adapt(feature_ds) # Create a CategoryEncoding for our integer indices encoder = CategoryEncoding(output_mode=&quot;binary&quot;) # Prepare a dataset of indices feature_ds = feature_ds.map(index) # Learn the space of possible indices encoder.adapt(feature_ds) return index, encoder . # Apply one-hot encoding to an integer categorical feature indexer, encoder3 = create_string_categorical_encoder(ds, &#39;categorical_string&#39;) # Turn the string input into integer indices indices = indexer.apply(xdf[&#39;categorical_string&#39;].values) # Apply one-hot encoding to our indices encoder3.apply(indices) . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[0., 0., 0., 0., 1.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.]], dtype=float32)&gt; . Notice that the string categorical column was hot encoded into 5 tokens whereas in the input dataframe there is only 3 unique values. This is because the indexer adds 2 more tokens. See the vocabulary: . indexer.get_vocabulary() . [&#39;&#39;, &#39;[UNK]&#39;, &#39;cat2&#39;, &#39;cat3&#39;, &#39;cat1&#39;] .",
            "url": "https://dzlab.github.io/notebooks/2020/08/02/Preprocessing_structured_data_in_TF_2_3.html",
            "relUrl": "/2020/08/02/Preprocessing_structured_data_in_TF_2_3.html",
            "date": " â€¢ Aug 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Annotation with TensorFlow Object Detection API",
            "content": "import matplotlib import matplotlib.pyplot as plt import numpy as np from PIL import Image from six import BytesIO from pathlib import Path import tensorflow as tf %matplotlib inline . Install Object Detection API . !git clone --depth 1 https://github.com/tensorflow/models . Cloning into &#39;models&#39;... remote: Enumerating objects: 2797, done. remote: Counting objects: 100% (2797/2797), done. remote: Compressing objects: 100% (2439/2439), done. remote: Total 2797 (delta 563), reused 1405 (delta 322), pack-reused 0 Receiving objects: 100% (2797/2797), 57.73 MiB | 31.67 MiB/s, done. Resolving deltas: 100% (563/563), done. . # Install the Object Detection API %%bash cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install -q . . object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used. . from object_detection.utils import colab_utils from object_detection.utils import visualization_utils as viz_utils . Download data for annotation . Download an image dataset to annotate, for instance The Oxford-IIIT Pet Dataset (link) . %%bash curl -O https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz tar xzf images.tar.gz . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 755M 100 755M 0 0 29.8M 0 0:00:25 0:00:25 --:--:-- 31.4M . paths = list([str(p) for p in Path(&#39;images&#39;).glob(&#39;*&#39;)]) . Utility method to load an image from path into a uint8 numpy array with shape (height, width, channels), where channels=3 for RGB. . def load_image_into_numpy_array(path): img_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(img_data)) (im_width, im_height) = image.size image_np = np.array(image.getdata(), dtype=np.uint8) return image_np.reshape((im_height, im_width, 3)) . For testing select a random subset of the images (we don&#39;t want load all images) . sample_size = 10 sample_paths = [paths[np.random.randint(len(paths))] for i in range(10)] . Annotate images . Load the selected random images into numpy arrays . images_np = [load_image_into_numpy_array(str(p)) for p in sample_paths] . boxes = [] colab_utils.annotate(images_np, box_storage_pointer=boxes) . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . Define the indexes for the categories . category_index = { 0: {&#39;id&#39;: 0, &#39;name&#39;: &#39;dog&#39;}, 1: {&#39;id&#39;: 1, &#39;name&#39;: &#39;cat&#39;} } . Inspect the annotations . Wrapper function to visualize the original image along with the best detected box. It takes are arguments: . image_np: uint8 numpy array with shape (img_height, img_width, 3) | boxes: a numpy array of shape [N, 4] | classes: a numpy array of shape [N]. Note that class indices are 1-based, and match the keys in the label map. | scores: a numpy array of shape [N] or None. If scores=None, then this function assumes that the boxes to be plotted are groundtruth boxes and plot all boxes as black with no classes or scores. | category_index: a dict containing category dictionaries (each holding category index id and category name name) keyed by category indices. | figsize: (optional) size for the figure. | image_name: (optional) name for the image file. | . def plot_detections(image_np, boxes, classes, scores, category_index, figsize=(12, 16), image_name=None): image_np_with_annotations = image_np.copy() viz_utils.visualize_boxes_and_labels_on_image_array( image_np_with_annotations, boxes, classes, scores, category_index, use_normalized_coordinates=True, min_score_thresh=0.8) if image_name: plt.imsave(image_name, image_np_with_annotations) else: plt.imshow(image_np_with_annotations) . I manually inspected the images (that&#39;s the 100% scores below) to get the class for each one, note that: . 0 is for a cat image | 1 is for a dog image | . classes = [ np.ones(shape=(1), dtype=np.int32), np.ones(shape=(1), dtype=np.int32), np.zeros(shape=(1), dtype=np.int32), np.ones(shape=(1), dtype=np.int32), np.zeros(shape=(1), dtype=np.int32) ] # give boxes a score of 100% scores = np.array([1.0], dtype=np.float32) . Vizualise the images with their bounding boxes . plt.figure(figsize=(30, 15)) for idx in range(5): plt.subplot(2, 3, idx+1) plot_detections(images_np[idx], boxes[idx], classes[idx], scores, category_index) plt.show() .",
            "url": "https://dzlab.github.io/notebooks/2020/07/19/Image_Annotation_on_Colab.html",
            "relUrl": "/2020/07/19/Image_Annotation_on_Colab.html",
            "date": " â€¢ Jul 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Captum PyTorch Vision Example",
            "content": "Captum (translates to comprehension in Latin) is an open source library for model interpretability. It helps model developers understand which features are contributing to their modelâ€™s output. It implements state-of-the-art interpretability algorithms in PyTorch, and provide them as an easy to use API. . The rest of this notebook illustrates how to use this library to interpret a fastai v2 based image classification model. . Setup . %%capture %%bash pip install fastai2 pip install psutil pip install captum . from matplotlib.colors import LinearSegmentedColormap from fastai2.vision.all import * from captum.attr import IntegratedGradients from captum.attr import GradientShap from captum.attr import Occlusion from captum.attr import NoiseTunnel from captum.attr import visualization as viz . Data . Download data for training an image classification model . path = untar_data(URLs.PETS)/&#39;images&#39; imgs = get_image_files(path) def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, imgs, valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) . Model . Fine tune an Imagenet-based model on the new images dataset. . learn = cnn_learner(dls, resnet34, metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.169865 | 0.020558 | 0.004736 | 00:23 | . epoch train_loss valid_loss error_rate time . 0 | 0.060073 | 0.026544 | 0.008119 | 00:24 | . Basic interpertation of the model prediected classes vs. actual ones. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Visualize top losses, e.g. to check if the images themselves are correctly annotated. . interp.plot_top_losses(5, nrows=1) . Store/Restore the fine tuned model . learn.export(&#39;/tmp/model.pkl&#39;) learn_inf = load_learner(&#39;/tmp/model.pkl&#39;) . Select a random image and plot it . idx = random.randint(0, len(imgs)) . image = PILImage.create(imgs[idx]) image . image = learn_inf.dls.after_item(image) image = learn_inf.dls.after_batch(image) . pred,pred_idx,probs = learn_inf.predict(imgs[idx]) pred, pred_idx, probs . (&#39;False&#39;, tensor(0), tensor([9.9998e-01, 2.0485e-05])) . Interpretability . Let&#39;s use Captum.ai to interpret the model predictions and to have a visual on where the network focused more in the input image. . Gradient-based attribution . Integrated Gradients is an interpretaility technique based on the approximation of integral gradients. The basic implementation works as followss: . Given as input target image and a baseline image (usually a black image), generate multiple images between both starting from darker to lighter. | Do forward pass with each of those images to predict a class and calculate the gradient. | Approximate the integral of the gradients of all those images | . The following example, illustrates how to use Captum IntegratedGradients to compute the attributions using Integrated Gradients and visualize them on the target image. . integrated_gradients = IntegratedGradients(learn_inf.model) attr_ig = integrated_gradients.attribute(image, target=pred_idx, n_steps=200) . transposed_attr_ig = np.transpose(attr_ig.squeeze().numpy(), (1,2,0)) transposed_image = np.transpose(image.squeeze().numpy(), (1,2,0)) . default_cmap = LinearSegmentedColormap.from_list(&#39;custom blue&#39;, [(0, &#39;#ffffff&#39;), (0.25, &#39;#000000&#39;), (1, &#39;#000000&#39;)], N=256) _ = viz.visualize_image_attr(transposed_attr_ig, transposed_image, method=&#39;heat_map&#39;, cmap=default_cmap, show_colorbar=True, sign=&#39;positive&#39;, outlier_perc=1) . For a better visual of the attribution, the images between baseline and target are sampled using a noise tunnel (by adding gaussian noise). And when the gradients are calulcated, we smoothe them by calculating their mean squared. . noise_tunnel = NoiseTunnel(integrated_gradients) attributions_ig_nt = noise_tunnel.attribute(image, n_samples=10, nt_type=&#39;smoothgrad_sq&#39;, target=pred_idx) transposed_attr_ig_nt = np.transpose(attributions_ig_nt.squeeze().numpy(), (1,2,0)) _ = viz.visualize_image_attr_multiple(transposed_attr_ig_nt, transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], cmap=default_cmap, show_colorbar=True) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Occlusion-based attribution . Occlusion-based attribution is a different interepretation technique based on perturbing parts of the original image (e.g. by zeroing) and computing how this affects the model decision. This technique is implemented by: . Slide a window of shape (h, w) on the target image with a stride s | Replace the window with a baseline (e.g. with black) and qunatify the effect on model decision. | Repeat previous steps until all of the target image is covered. | . Simiarly to convolution, this technique can become very slow when used in large models and large input images. . As a first exercise, we run a sliding window of size 15x15 and a stride of 8 along both image dimensions. For each window, we occlude the image with a baseline value of 0. . occlusion = Occlusion(learn_inf.model) attr_occ = occlusion.attribute(image, strides = (3, 8, 8), target=pred_idx, sliding_window_shapes=(3,15, 15), baselines=0) . _ = viz.visualize_image_attr_multiple(np.transpose(attr_occ.squeeze().numpy(), (1,2,0)), transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], show_colorbar=True, outlier_perc=2, ) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Let&#39;s try different window shape and sliding window and visuzalize the result, by rescaling back to the shape of the original image. . Experimenting with different windows is important because: . Different shape may lead to a significaly different result. | Larger windows is useful when the input image presents some local coherence. | . In this case, we run a sliding window of size 60x60 and a stride of 50 along both image dimensions. For each window, we occlude the image with a baseline value of 0. . occlusion = Occlusion(learn_inf.model) attr_occ = occlusion.attribute(image, strides = (3, 50, 50), target=pred_idx, sliding_window_shapes=(3,60, 60), baselines=0) _ = viz.visualize_image_attr_multiple(np.transpose(attr_occ.squeeze().numpy(), (1,2,0)), transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], show_colorbar=True, outlier_perc=2, ) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . References . Axiomatic Attribution for Deep Networks - link | Towards better understanding of gradient-based attribution methods for Deep Neural Networks - link | .",
            "url": "https://dzlab.github.io/notebooks/jupyter/2020/04/18/Captum_PyTorch_Vision_Example.html",
            "relUrl": "/jupyter/2020/04/18/Captum_PyTorch_Vision_Example.html",
            "date": " â€¢ Apr 18, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Mask-RCNN Tensorflow v1 image examples",
            "content": "Project Setup . Make sure we&#39;re running TensorFlow v1 . try: %tensorflow_version 1.x except Exception: pass . TensorFlow 1.x selected. . Install Mask-RCNN model . %%capture %%bash pip install -U git+https://github.com/matterport/Mask_RCNN . Download weights of pretrained Mask-RCNN . !curl -L -o mask_rcnn_balloon.h5 https://github.com/matterport/Mask_RCNN/releases/download/v2.1/mask_rcnn_balloon.h5?raw=true . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 611 100 611 0 0 2246 0 --:--:-- --:--:-- --:--:-- 2254 100 244M 100 244M 0 0 40.0M 0 0:00:06 0:00:06 --:--:-- 47.2M . import cv2 import math import numpy as np import matplotlib.pyplot as plt import os import sys from mrcnn import utils from mrcnn import model as modellib from mrcnn.config import Config from PIL import Image plt.rcParams[&quot;figure.figsize&quot;]= (10,10) np.set_printoptions(precision=3) . Mask-RCNN setup . # Load the pre-trained model data ROOT_DIR = os.getcwd() MODEL_DIR = os.path.join(ROOT_DIR, &quot;logs&quot;) COCO_MODEL_PATH = os.path.join(ROOT_DIR, &quot;mask_rcnn_coco.h5&quot;) if not os.path.exists(COCO_MODEL_PATH): utils.download_trained_weights(COCO_MODEL_PATH) . Downloading pretrained model to /content/mask_rcnn_coco.h5 ... ... done downloading pretrained model! . class InferenceConfig(Config): &quot;&quot;&quot;Configuration for training on MS COCO. Derives from the base Config class and overrides values specific to the COCO dataset. &quot;&quot;&quot; # Give the configuration a recognizable name NAME = &quot;coco&quot; # Number of images to train with on each GPU. A 12GB GPU can typically # handle 2 images of 1024x1024px. IMAGES_PER_GPU = 1 # Uncomment to train on 8 GPUs (default is 1) GPU_COUNT = 1 # Number of classes (including background) NUM_CLASSES = 1 + 80 # COCO has 80 classes . %%capture # COCO dataset object names model = modellib.MaskRCNN( mode=&quot;inference&quot;, model_dir=MODEL_DIR, config=InferenceConfig() ) model.load_weights(COCO_MODEL_PATH, by_name=True) class_names = [ &#39;BG&#39;, &#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, &#39;motorcycle&#39;, &#39;airplane&#39;, &#39;bus&#39;, &#39;train&#39;, &#39;truck&#39;, &#39;boat&#39;, &#39;traffic light&#39;, &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;parking meter&#39;, &#39;bench&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;sheep&#39;, &#39;cow&#39;, &#39;elephant&#39;, &#39;bear&#39;, &#39;zebra&#39;, &#39;giraffe&#39;, &#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;frisbee&#39;, &#39;skis&#39;, &#39;snowboard&#39;, &#39;sports ball&#39;, &#39;kite&#39;, &#39;baseball bat&#39;, &#39;baseball glove&#39;, &#39;skateboard&#39;, &#39;surfboard&#39;, &#39;tennis racket&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;, &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;, &#39;pizza&#39;, &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;couch&#39;, &#39;potted plant&#39;, &#39;bed&#39;, &#39;dining table&#39;, &#39;toilet&#39;, &#39;tv&#39;, &#39;laptop&#39;, &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;, &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;, &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;, &#39;hair drier&#39;, &#39;toothbrush&#39; ] . The following function will apply to the origianl image, the pixels from the gray image is 0, otherwise keep the pixels from original picture. . # This function is used to change the colorful background information to grayscale. # image[:,:,0] is the Blue channel,image[:,:,1] is the Green channel, image[:,:,2] is the Red channel # mask == 0 means that this pixel is not belong to the object. # np.where function means that if the pixel belong to background, change it to gray_image. # Since the gray_image is 2D, for each pixel in background, we should set 3 channels to the same value to keep the grayscale. def apply_mask(image, mask_image, mask): &quot;&quot;&quot;Helper function to apply a mask to an image.&quot;&quot;&quot; image[:, :, 0] = np.where( mask == 0, mask_image[:, :, 0], image[:, :, 0] ) image[:, :, 1] = np.where( mask == 0, mask_image[:, :, 1], image[:, :, 1] ) image[:, :, 2] = np.where( mask == 0, mask_image[:, :, 2], image[:, :, 2] ) return image . def process_image(image, mask_image, boxes, masks, ids, names, scores, target_label): &quot;&quot;&quot;Helper function to find the object with biggest bounding box and apply mask to it.&quot;&quot;&quot; # max_area will save the largest object for all the detection results max_area = 0 # n_instances saves the amount of all objects n_instances = boxes.shape[0] if not n_instances: print(&#39;NO INSTANCES TO DISPLAY&#39;) else: assert boxes.shape[0] == masks.shape[-1] == ids.shape[0] for i in range(n_instances): if not np.any(boxes[i]): continue # compute the square of each object y1, x1, y2, x2 = boxes[i] square = (y2 - y1) * (x2 - x1) # use label to select the object with given label from all the 80 classes in COCO dataset current_label = names[ids[i]] if target_label is not None or current_label == target_label: # save the largest object in the image as main character # other people will be regarded as background if square &gt; max_area: max_area = square mask = masks[:, :, i] else: continue else: continue # apply mask for the image # by mistake you put apply_mask inside for loop or you can write continue in if also image = apply_mask(image, mask_image, mask) return image . Now the mode is ready to use . !curl -L -o cat_input.jpg https://unsplash.com/photos/7GX5aICb5i4/download?force=true&amp;w=640 . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 232 0 232 0 0 666 0 --:--:-- --:--:-- --:--:-- 666 100 5442k 100 5442k 0 0 10.5M 0 --:--:-- --:--:-- --:--:-- 10.5M . # Credit for the image: https://unsplash.com/photos/7GX5aICb5i4 image = cv2.imread(&#39;./cat_input.jpg&#39;) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7f4274e4b710&gt; . Application 1: Grayscale the background . Recognize the main character, keep it colorfull while grayscal the background of the image. . # Use cvtColor to accomplish image transformation from RGB image to gray image mask_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) mask_image = np.stack([mask_image, mask_image, mask_image], axis=2) plt.imshow(mask_image) . &lt;matplotlib.image.AxesImage at 0x7f4274e21a90&gt; . results = model.detect([image], verbose=0) output_dict = results[0] rois, class_ids, scores, masks = output_dict.values() . result = process_image( image.copy(), mask_image, rois, masks, class_ids, class_names, scores, &#39;cat&#39; ) plt.imshow(result) . &lt;matplotlib.image.AxesImage at 0x7f427458c860&gt; . Let&#39;s take this cat to the beach . !curl -L -o beach.jpg https://unsplash.com/photos/DH_u2aV3nGM/download?force=true&amp;w=640 . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 242 0 242 0 0 733 0 --:--:-- --:--:-- --:--:-- 731 100 4000k 100 4000k 0 0 9828k 0 --:--:-- --:--:-- --:--:-- 9828k . image_beach = cv2.imread(&#39;./beach.jpg&#39;) image_beach = cv2.cvtColor(image_beach, cv2.COLOR_BGR2RGB) plt.imshow(image_beach) . &lt;matplotlib.image.AxesImage at 0x7f427456cc18&gt; . Reshape the new mask image so that it matches the size of the original image. . image_beach = cv2.resize(image_beach, dsize=(image.shape[1], image.shape[0]), interpolation = cv2.INTER_AREA) . result = process_image( image.copy(), image_beach, rois, masks, class_ids, class_names, scores, &#39;cat&#39; ) plt.imshow(result) . &lt;matplotlib.image.AxesImage at 0x7f42744ccba8&gt; . Think of the possibilites :) .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/segmentation/2020/03/29/Mask_RCNN_Tensorflow_v1_image_examples.html",
            "relUrl": "/tensorflow/vision/segmentation/2020/03/29/Mask_RCNN_Tensorflow_v1_image_examples.html",
            "date": " â€¢ Mar 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Text classification with BERT using TF Text",
            "content": "Setup . try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass . TensorFlow 2.x selected. . Install dependencies . %%capture %%bash pip install -U tensorflow-text . Import modules . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split import tensorflow as tf import tensorflow_text as text import tensorflow_hub as hub import tensorflow_datasets as tfds from tensorflow.keras.layers import Dense, Dropout, Input from tensorflow.keras.models import Model . Set default options for modules . pd.set_option(&#39;display.max_colwidth&#39;, -1) . GPU check . num_gpus_available = len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;)) print(&quot;Num GPUs Available: &quot;, num_gpus_available) assert num_gpus_available &gt; 0 . Num GPUs Available: 1 . config = { &#39;seed&#39;: 31, &#39;batch_size&#39;: 64, &#39;epochs&#39;: 10, &#39;max_seq_len&#39;: 128 } . Data . Download the pretrained BERT model . BERT_URL = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1&quot; bert_layer = hub.KerasLayer(BERT_URL, trainable=False) vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() print(f&#39;BERT vocab is stored at : {vocab_file}&#39;) print(f&#39;BERT model is case sensitive: {do_lower_case}&#39;) . BERT vocab is stored at : b&#39;/tmp/tfhub_modules/03d6fb3ce1605ad9e5e9ed5346b2fb9623ef4d3d/assets/vocab.txt&#39; BERT model is case sensitive: True . Load the vocab file that corresponds to the pretrained BERT . def load_vocab(vocab_file): &quot;&quot;&quot;Load a vocabulary file into a list.&quot;&quot;&quot; vocab = [] with tf.io.gfile.GFile(vocab_file, &quot;r&quot;) as reader: while True: token = reader.readline() if not token: break token = token.strip() vocab.append(token) return vocab vocab = load_vocab(vocab_file) . Use BERT vocab to create a word to index lookup table . def create_vocab_table(vocab, num_oov=1): &quot;&quot;&quot;Create a lookup table for a vocabulary&quot;&quot;&quot; vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64) init = tf.lookup.KeyValueTensorInitializer(keys=vocab, values=vocab_values, key_dtype=tf.string, value_dtype=tf.int64) vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov, lookup_key_dtype=tf.string) return vocab_table vocab_lookup_table = create_vocab_table(vocab) . Use BERT vocab to create a index to word lookup table . def create_index2word(vocab): # Create a lookup table for a index to token vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64) init = tf.lookup.KeyValueTensorInitializer(keys=vocab_values, values=vocab) return tf.lookup.StaticHashTable(initializer=init, default_value=tf.constant(&#39;unk&#39;), name=&quot;index2word&quot;) index2word = create_index2word(vocab) . Check out the indices for the following tokens . vocab_lookup_table.lookup(tf.constant([&#39;[PAD]&#39;, &#39;[UNK]&#39;, &#39;[CLS]&#39;, &#39;[SEP]&#39;, &#39;[MASK]&#39;])) . &lt;tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 0, 100, 101, 102, 103])&gt; . Check out the token corresponding to an index . index2word.lookup(tf.constant([0], dtype=&#39;int64&#39;)).numpy() . [b&#39;[PAD]&#39;] . Create a BERT tokenizer using TF Text . tokenizer = text.BertTokenizer( vocab_lookup_table, token_out_type=tf.int64, lower_case=do_lower_case ) . Lookup for the BERT token IDs for padding and start/end of sentence. . PAD_ID = vocab_lookup_table.lookup(tf.constant(&#39;[PAD]&#39;)) # padding token CLS_ID = vocab_lookup_table.lookup(tf.constant(&#39;[CLS]&#39;)) # class token SEP_ID = vocab_lookup_table.lookup(tf.constant(&#39;[SEP]&#39;)) # sequence separator token . Preprocessing . Define the logic to preprocess data and format it as required by BERT . def preprocess(record): review, label = record[&#39;text&#39;], record[&#39;label&#39;] # process review to calculate BERT input ids, mask, type_ids = preprocess_bert_input(review) return (ids, mask, type_ids), label def preprocess_bert_input(review): # calculate tokens ID ids = tokenize_text(review, config[&#39;max_seq_len&#39;]) # calculate mask mask = tf.cast(ids &gt; 0, tf.int64) mask = tf.reshape(mask, [-1, config[&#39;max_seq_len&#39;]]) # calculate tokens type ID zeros_dims = tf.stack(tf.shape(mask)) type_ids = tf.fill(zeros_dims, 0) type_ids = tf.cast(type_ids, tf.int64) return (ids, mask, type_ids) def tokenize_text(review, seq_len): # convert text into token ids tokens = tokenizer.tokenize(review) # flatten the output ragged tensors tokens = tokens.merge_dims(1, 2)[:, :seq_len] # Add start and end token ids to the id sequence start_tokens = tf.fill([tf.shape(review)[0], 1], CLS_ID) end_tokens = tf.fill([tf.shape(review)[0], 1], SEP_ID) tokens = tokens[:, :seq_len - 2] tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1) # truncate sequences greater than MAX_SEQ_LEN tokens = tokens[:, :seq_len] # pad shorter sequences with the pad token id tokens = tokens.to_tensor(default_value=PAD_ID) pad = seq_len - tf.shape(tokens)[1] tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID) # and finally reshape the word token ids to fit the output # data structure of TFT return tf.reshape(tokens, [-1, seq_len]) . Dataset . Download the dataset from TF Hub and process it . train_ds, valid_ds = tfds.load(&#39;imdb_reviews&#39;, split=[&#39;train&#39;, &#39;test&#39;], shuffle_files=True) train_ds = train_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) valid_ds = valid_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) train_ds, valid_ds = train_ds.map(preprocess), valid_ds.map(preprocess) . Model . input_ids = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_ids&quot;) input_mask = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_mask&quot;) input_type_ids = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_type_ids&quot;) pooled_output, sequence_output = bert_layer([input_ids, input_mask, input_type_ids]) drop_out = Dropout(0.3, name=&quot;dropout&quot;)(pooled_output) output = Dense(1, activation=&#39;sigmoid&#39;, name=&quot;linear&quot;)(drop_out) model = Model(inputs=[input_ids, input_mask, input_type_ids], outputs=output) model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . model.summary() . Model: &#34;model_1&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_mask (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_type_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ keras_layer (KerasLayer) [(None, 768), (None, 109482241 input_ids[0][0] input_mask[0][0] input_type_ids[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 768) 0 keras_layer[1][0] __________________________________________________________________________________________________ linear (Dense) (None, 1) 769 dropout[0][0] ================================================================================================== Total params: 109,483,010 Trainable params: 769 Non-trainable params: 109,482,241 __________________________________________________________________________________________________ . Training . model.fit(train_ds, validation_data=valid_ds, epochs=config[&#39;epochs&#39;]) . Epoch 1/10 391/391 [==============================] - 499s 1s/step - loss: 0.6654 - accuracy: 0.6016 - val_loss: 0.5977 - val_accuracy: 0.7028 Epoch 2/10 391/391 [==============================] - 510s 1s/step - loss: 0.6063 - accuracy: 0.6712 - val_loss: 0.5650 - val_accuracy: 0.7282 Epoch 3/10 391/391 [==============================] - 510s 1s/step - loss: 0.5839 - accuracy: 0.6969 - val_loss: 0.5494 - val_accuracy: 0.7362 Epoch 4/10 391/391 [==============================] - 511s 1s/step - loss: 0.5730 - accuracy: 0.7025 - val_loss: 0.5388 - val_accuracy: 0.7455 Epoch 5/10 391/391 [==============================] - 510s 1s/step - loss: 0.5696 - accuracy: 0.7058 - val_loss: 0.5376 - val_accuracy: 0.7417 Epoch 6/10 391/391 [==============================] - 510s 1s/step - loss: 0.5613 - accuracy: 0.7146 - val_loss: 0.5268 - val_accuracy: 0.7517 Epoch 7/10 391/391 [==============================] - 510s 1s/step - loss: 0.5608 - accuracy: 0.7130 - val_loss: 0.5233 - val_accuracy: 0.7544 Epoch 8/10 391/391 [==============================] - 510s 1s/step - loss: 0.5625 - accuracy: 0.7106 - val_loss: 0.5217 - val_accuracy: 0.7555 Epoch 9/10 391/391 [==============================] - 510s 1s/step - loss: 0.5603 - accuracy: 0.7125 - val_loss: 0.5199 - val_accuracy: 0.7535 Epoch 10/10 391/391 [==============================] - 510s 1s/step - loss: 0.5567 - accuracy: 0.7159 - val_loss: 0.5150 - val_accuracy: 0.7591 . &lt;tensorflow.python.keras.callbacks.History at 0x7f2fffddba58&gt; . Evaluation . test_text_ds = tfds.load(&#39;imdb_reviews&#39;, split=&#39;unsupervised&#39;, shuffle_files=True) test_ds = test_text_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) test_ds = test_ds.map(preprocess) . Check how test text is tokenized . test_text = [record[&#39;text&#39;].numpy() for record in test_text_ds.take(10)] . ids = tokenize_text(test_text, config[&#39;max_seq_len&#39;]) . tokens = [b&#39; &#39;.join(tokens_array) for tokens_array in index2word.lookup(ids).numpy()] . pd.DataFrame({&#39;tokens&#39;: tokens}) . tokens . 0 b&quot;[CLS] spoil ##er - now knowing the ending i find it so clever that the whole movie takes place in a motel and each character has a different room . even sane people have many different aspects to their personality , but they don &#39; t let them become dominant - - they are controlled . malcolm &#39; s various personalities and needs were person ##ified in each character . the prostitute mother ( amanda pee ##t ) , the part of him who hated her for being a prostitute ( larry ) , the loving mother he wish he had , the loving father he wish he had , the selfish part of himself ( actress ) , the violent part of his personality ( ray [SEP]&quot; | . 1 b&quot;[CLS] i knew about this film long before i saw it . in fact , i had to buy the dvd in order to see it because no video store carried it . i didn &#39; t mind spending the $ 12 to buy it used because i collect off the wall movies . the new limited edition double dvd has great sound and visually not bad . i found myself laughing much more then &lt; br / &gt; &lt; br / &gt; jolt ##ing in fear , although there were a few scenes were i was startled . &lt; br / &gt; &lt; br / &gt; if you enjoy off the wall 70s sci - fi / horror movies , you probably will eat this one [SEP]&quot; | . 2 b&quot;[CLS] this movie is really really awful . it &#39; s as bad as zombie 90 well maybe not that bad but pretty close . if your a fan of the italian horror movies then you might like this movie . i thought that it was dam near un ##watch ##able of course i &#39; m not a fan of the italian movies . the only italian movie that was ok was jungle holocaust . which is one over ##rated movie . this film is way over ##rated . but let &#39; s get started with how horrible this film really is shall we . the acting is goofy and horrible . the effects suck . no plot with this movie . little gore which is the [SEP]&quot; | . 3 b&#39;[CLS] wait a minute . . . yes i do . &lt; br / &gt; &lt; br / &gt; the director of &#39; the breed &#39; has obviously seen terry gill ##iam &#39; s &#39; brazil &#39; a few too many times and asked himself the question , &quot; if &#39; brazil &#39; had been an ill - conceived tale about vampires in the near future , what would it be like ? &quot; well , i &#39; ll tell ya , it &#39; d be like 91 minutes of a swedish whore kicking you in the groin , only not as satisfying . the dialogue was laced with gr ##at ##uit ##ous curse words and tri ##te one - liner ##s , and whoever edited this [SEP]&#39; | . 4 b&quot;[CLS] this is the type of movie that &#39; s just barely involving enough for one viewing , but i don &#39; t think i could stand to watch it again . it looks and plays like a mid - seventies tv movie , only with some gr ##at ##uit ##ous sex and violence thrown in . &lt; br / &gt; &lt; br / &gt; i agree with several previous posters - - her ##ve ville ##chai ##ze is not very menacing , and at times even comes off as un ##int ##ended comedy . at least the other two villains make up for that . also , it was jolt ##ing to see jonathan fr ##id is such a pedestrian role , which definitely under - [SEP]&quot; | . 5 b&quot;[CLS] i like sci - fi movies and everything &#39; bout it and aliens , so i watched this flick . nothing new , nothing special , average acting , typical h . b . davenport &#39; story , weak and che ##es ##y fx &#39; s , bad ending of movie , but still the author idea is good . the marines on lost island find the truth about alien landing there and truth about past - experiments on them . they die one after one , some of them were killed by lonely alien , and others by human enemies . ufo effects , when it flees and crush ##es are bad , too . the voices of angry alien are funny , too . [SEP]&quot; | . 6 b&quot;[CLS] i was lucky enough to see a preview of this film tonight . this was a very cool , eerie film . well acted , especially by ska ##rs ##gard who played his role of terry glass perfectly . sob ##ies ##ki did a very good job too as it seems to me that she has a bright future ahead of her . the music was well placed but was fairly standard . the use of shadows was quite interesting as well . overall , this was quite a nice surprise considering i &#39; m not much a fan of this genre . 7 / 10 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]&quot; | . 7 b&#39;[CLS] my kids and i love this movie ! ! we think that richard pry ##or and the whole cast did a wonderful job in the movie . it means more to us now since the passing of richard ! ! we will miss his sense of humor . but his movies and shows will stay with us forever ! ! we especially love the parts of brad , frank crawford and ar ##lo pear ! ! they had some one liner ##s in the movie that were great ! ! my son and i love to quote those one liner ##s when we see each other and my daughter will join us when we discuss the movie . we thought the moving guys were terrific . [SEP]&#39; | . 8 b&quot;[CLS] somehow the an ##ima ##trix shorts with the most interesting premises have the worst outcome . mat ##ric ##ulated is the worst of the bunch ( although it &#39; s a close call with program ) , as it takes a great idea ( showing the machines the beauty of mankind by plug ##ging them in ) and turns it into the worst experience of the 9 . &lt; br / &gt; &lt; br / &gt; as i said , the story begins promising and interesting , but ends with a long , long , long sequence of &#39; weird &#39; images , a cross between the famous scenes from 2001 and v ##ga - rain ( who can remember it ) , but not as [SEP]&quot; | . 9 b&quot;[CLS] while holiday ##ing in the basque region of spain , two couples discover a child whose hands are severely miss ##ha ##pen . the child has been gravely mist ##reate ##d , and , as a result , cannot communicate . the two couples reluctantly decide to rescue her and report her circumstances to the authorities . however , severe weather and the dense ##ness of the forest surrounding their holiday home make it impossible for them to make a quick get ##away . soon , the local inhabitants become aware that the girl is missing , and they right ##ly suspect the holiday - makers of taking her . suspicions and paranoia begin to fest ##er , and it isn &#39; t long before violence [SEP]&quot; | . Run prediction on test reviews . result = model.predict(test_ds) . result.shape . (50000, 1) . result_df = pd.DataFrame({&#39;label&#39;: tf.squeeze(result[:10]).numpy(), &#39;text&#39;: test_text}) result_df.head() . label text . 0 0.464566 | b&quot;SPOILER - Now knowing the ending I find it so clever that the whole movie takes place in a motel and each character has a different room. Even sane people have many different aspects to their personality, but they don&#39;t let them become dominant -- they are controlled. Malcolm&#39;s various personalities and needs were personified in each character. The prostitute mother (Amanda Peet), the part of him who hated her for being a prostitute (Larry), the loving mother he wish he had, the loving father he wish he had, the selfish part of himself (actress), the violent part of his personality (Ray Liotta and Busey), the irrational emotions he feels and his need to be loved (Ginnie) and his attempts to control those feelings (Lou), the hurt little boy who sees far too many traumatic things in his life, and of course, John Cusack who seems to represent Malcolm himself trying to analyze and understand all the craziness in his mind, tries to follow the rules (accepting responsibility for the car accident), help others (giving Amanda Peet a ride, and stitching up the mother). Very cleverly done!&quot; | . 1 0.252326 | b&#39;I knew about this film long before I saw it. In fact, I had to buy the DVD in order to see it because no video store carried it. I didn &#39;t mind spending the $12 to buy it used because I collect off the wall movies. The new limited edition double DVD has great sound and visually not bad. I found myself laughing much more then&lt;br /&gt;&lt;br /&gt;jolting in fear, although there were a few scenes were I was startled.&lt;br /&gt;&lt;br /&gt;If you enjoy off the wall 70s sci-fi/horror movies, you probably will eat this one up. I was a little dissapointed at how abrubtly it ended. I wanted the movie to keep going, see how things pan out. The DVD revolution has brought so many&lt;br /&gt;&lt;br /&gt;lost clasics back to life, it is truly wonderful. Blue Sunshine is one of those lost &quot;missing links&quot; of the cinema. Enjoy!&#39; | . 2 0.485239 | b&quot;This movie is really really awful. It&#39;s as bad as Zombie 90 well maybe not that bad but pretty close. If your a fan of the Italian horror movies then you might like this movie. I thought that it was dam near unwatchable of course I&#39;m not a fan of the Italian movies. The only Italian movie that was OK was Jungle holocaust. Which is one overrated movie. This film is way overrated. But let&#39;s get started with how horrible this film really is shall we. The acting is goofy and horrible. The effects suck. No plot with this movie. Little gore which is the only good thing in the film isn&#39;t showed nearly enough to be worth watching this wreck. The zombies are very fake looking. It looks like it&#39;s a bunch of dudes wearing cheap dollar store masks. Please avoid this film at all costs.&quot; | . 3 0.251897 | b&#39;Wait a minute... yes I do.&lt;br /&gt;&lt;br /&gt;The director of &#39;The Breed &#39; has obviously seen Terry Gilliam &#39;s &#39;Brazil &#39; a few too many times and asked himself the question, &quot;If &#39;Brazil &#39; had been an ill-conceived tale about vampires in the near future, what would it be like?&quot; Well, I &#39;ll tell ya, it &#39;d be like 91 minutes of a Swedish whore kicking you in the groin, only not as satisfying. The dialogue was laced with gratuitous curse words and trite one-liners, and whoever edited this piece of crap should be shot. I have no real idea of exactly how the whole thing ended because I &#39;m not really sure what happened during the first part of the film. With so many subplots your head begins to hurt and so much bad acting your head wants to explode this movie should only be viewed with large quantities of beer and at least two other people you can MST3K with. The only thing that made me not stab myself in the eye with a dirty soup spoon was this line: Evil Doctor Guy: &quot;That &#39;s it, you are not James Bond, and I am not Blofeld. No more explanations!&quot; Dude From Jason &#39;s Lyric: &quot;I &#39;m getting paid scale!&quot; The cinematography was shaky at best and the acting was putrid. Also, what was with all the pseudo-1984 posters and PA announcements? The costumes were from the 50 &#39;s, the cars were from the 60 &#39;s, the music was from the 90 &#39;s and I wish I were dead. This movie sucks.&#39; | . 4 0.274131 | b&#39;This is the type of movie that &#39;s just barely involving enough for one viewing, but I don &#39;t think I could stand to watch it again. It looks and plays like a mid-Seventies TV movie, only with some gratuitous sex and violence thrown in.&lt;br /&gt;&lt;br /&gt;I agree with several previous posters -- Herve Villechaize is NOT very menacing, and at times even comes off as unintended comedy. At least the other two villains make up for that. Also, it was jolting to see Jonathan Frid is such a pedestrian role, which definitely under-utilized his enormous talents.&lt;br /&gt;&lt;br /&gt;But I think the basic problem with &quot;Seizure&quot; is in the storyline. The evil trio that are conjured up from Frid &#39;s mind are seen too early and too often. They appear to everyone at once, and announce their (murky) plans too early in the picture. In fact, Stone takes this idea and literally shoves it in the viewer &#39;s face, with a series of challenges for the guests; challenges that it doesn &#39;t seem like they have any chance of winning, anyway. How much more effective would have been keeping the evil ones in the shadows, preying on each house guest in turn, sowing confusion and doubt among the remaining house guests, who don &#39;t know who or what is causing the carnage. By having the trio appear early on, to all the &quot;assembled guests&quot;, and announcing their plan (confusing as that plan is), much potential for tension and suspense are lost.&lt;br /&gt;&lt;br /&gt;Also, a more gradual appearance of the evil ones would indicate Frid is slowing losing control of his subconscious. To have Frid subconsciously conjure up these baddies, because he &#39;s got hidden grudges against his wife and friends, would have been a far more logical plot device. Instead of having Frid play an intended victim from the get-go, it would have worked better to have him slowing becoming helpless to control the menace he &#39;s created, with mixed feelings of guilt and satisfaction as his shallow, superficial friends are killed off. The plot Stone offers up is confusing as to the origins and, most importantly, the motivations of the evil trio, and never gives any explanation why Frid, from whose mind they came from, can exercise absolutely no control over them. Confusing is the word that best sums up the whole picture, and the end feels like a total cheat. Better to have some great showdown in which Frid is finally able to banish the creations of his own tormented mind.&lt;br /&gt;&lt;br /&gt;Oliver Stone has done some notable work in his career, but sadly &quot;Seizure&quot; is not among them.&#39; | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/nlp/2020/03/15/Text_classification_with_BERT_and_TF_Text.html",
            "relUrl": "/tensorflow/nlp/2020/03/15/Text_classification_with_BERT_and_TF_Text.html",
            "date": " â€¢ Mar 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, itâ€™s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://dzlab.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}