{
  
    
        "post0": {
            "title": "Visual Attention Network Explained with a TensorFlow implementation",
            "content": "In this post we will examine Visual Attention Network for image classification and walk through its implementation in TensorFlow. . The paper introducing VAN proposed the following contributions: . A novel Large Kernel Attention (LKA) module which is a self-attention mechanism that takes advantage of the 2D structure of images by capturing channel adaptability in addition to spatial adaptability. | A novel neural network based on LKA, called Visual Attention Network (VAN) that outperforms vision transformers and convolutional neural networks in many computer vision tasks. | . The following charts from the paper highlights the results of different models on ImageNet-1K validation set. We can see that VAN performs better while keeping the computation cost comparable to other models. . . Implementation . Let&#39;s look at the different components of the VAN architecture and understand how to implement it in TensorFlow. We will be referencing the code from the original PyTorch implementation in this repository - VAN-Classification. . You can find another TensorFlow implementation in the following repository tfvan. . import math import numpy as np import tensorflow as tf from tensorflow.keras import initializers, layers, Model . Multilayer Perceptrons . VAN relies on a variation of Multilayer Perceptrons (MLPs) layer that decouple standard MLP into spatial MLP and channel MLP to reduce computational cost of the standard MLP. Furthermore, it uses an attention mechanism similarly to gMLP but without sensitivity to input size or the constraint of processing fixed-size images. . Below is the implementation of the MLP layer as proposed in the VAN architecture . class MLP(layers.Layer): def __init__(self, in_features, hidden_features=None, out_features=None, activation=&quot;gelu&quot;, drop=0.): super().__init__() out_features = out_features or in_features hidden_features = hidden_features or in_features self.fc1 = layers.Conv2D(hidden_features, 1) self.dwconv = layers.DepthwiseConv2D( kernel_size=3, strides=1, padding=&#39;same&#39;, use_bias=True, activation=activation ) self.fc2 = layers.Conv2D(out_features, 1) self.drop = layers.Dropout(drop) def call(self, x): x = self.fc1(x) x = self.dwconv(x) x = self.drop(x) x = self.fc2(x) x = self.drop(x) return x . mlp = MLP(768) y = mlp(tf.zeros((1, 224, 224, 3))) y.shape . TensorShape([1, 224, 224, 768]) . Large Kernel Attention . Attention mechanisms are used to capture relationship between input features and to produce an attention map indicating the importance of different them. To learn this relationship, we could use: . Self-attention mechanism to capture long-range dependence. Self-attention was successful in NLP tasks, in computer vision it has following drawbacks It treats images as 1D sequences which neglects the 2D structure of images. | The quadratic complexity is too expensive for high-resolution images. | It only achieves spatial adaptability but ignores the adaptability in channel dimension | . | Large kernel convolution to build relevance and produce attention map. But this approach has its own limitations, Large-kernel convolution comes with a huge amount of computational overhead and additional parameters to learn. | . . The authors proposed a new attention mechanism that combines the pros of the previous approaches while overcome their drawbacks. This is achieved by decomposing a large kernel convolution as depicted in the above picture. . For instance, given a $K × K$ convolution and dilation rate $d$, we decompose into: . a spatial local convolution (DW-Conv or depth-wise convolution) of $⌈ frac{K}{d}⌉×⌈ frac{K}{d}⌉$, | a spatial long-range convolution ((DW-D-Conv or depth-wise dilation convolution) of $(2d − 1) × (2d − 1)$, and | a channel convolution ($1×1$ convolution). | . This decomposition can be writting in the following formula: . $$ Attention = Conv_{1 x 1} ( text{DW-D-Conv} ( text{DW-Conv} (F)) ) $$ $$Output = Attention otimes F $$ . With $F ∈ R^{C×H×W}, Attention ∈ R^{C×H×W} text{ and } otimes text{ stands for element-wise product}$ . The architecture of a Large Kernel Attention layer would look like this: . . In TensorFlow, the Large Kernel Attention layer can be implemented as follows: . class LKA(layers.Layer): def __init__(self, dim): super().__init__() self.conv0 = layers.Conv2D(dim, kernel_size=5, padding=&quot;same&quot;, groups=dim) self.conv_spatial = layers.Conv2D(dim, kernel_size=7, strides=1, padding=&quot;same&quot;, groups=dim, dilation_rate=3) self.conv1 = layers.Conv2D(dim, kernel_size=1) def call(self, x): attn = self.conv0(x) attn = self.conv_spatial(attn) attn = self.conv1(attn) return x * attn . attn = LKA(4) y = attn(tf.zeros((1, 224, 224, 4))) y.shape . TensorShape([1, 224, 224, 4]) . class SpatialAttention(layers.Layer): def __init__(self, d_model, activation=&quot;gelu&quot;): super().__init__() self.proj_1 = layers.Conv2D(d_model, kernel_size=1, activation=activation) self.spatial_gating_unit = LKA(d_model) self.proj_2 = layers.Conv2D(d_model, kernel_size=1) def call(self, x): attn = self.proj_1(x) attn = self.spatial_gating_unit(attn) attn = self.proj_2(attn) attn = x + attn return attn . attn = SpatialAttention(4) y = attn(tf.zeros((1, 224, 224, 4))) y.shape . TensorShape([1, 224, 224, 4]) . DropPath layer . DropPath is used in the VAN model as an alternative to the Dropout layer, it was originally proposed in the FractalNet paper. Below is the implementation of DropPath in TensorFlow. Instead of using custom layer, we could alternatively we could simply use this StochasticDepth layer. . class DropPath(layers.Layer): def __init__(self, rate, **kwargs): super().__init__(**kwargs) self.rate = rate def call(self, inputs, training=None, **kwargs): if 0. == self.rate: return inputs if training is None: training = tf.keras.backend.learning_phase() training = tf.constant(training, dtype=tf.bool) outputs = tf.cond(training, lambda: self.drop(inputs), lambda: tf.identity(inputs)) return outputs def drop(self, inputs): keep = 1.0 - self.rate batch = tf.shape(inputs)[0] shape = [batch] + [1] * (inputs.shape.rank - 1) random = tf.random.uniform(shape, dtype=self.compute_dtype) &lt;= keep random = tf.cast(random, self.compute_dtype) / keep outputs = inputs * random return outputs . attn = DropPath(0.1) y = attn(tf.zeros((1, 224, 224, 3))) y.shape . TensorShape([1, 224, 224, 3]) . VAN stage . VAN uses multiple stages that downsample the input tensor and pass it through combines two Transofmer Encoder blocks but uses a window based self-attentions as illustrated in the following diagram. In this section, we will examine each component of this bloc and implement it in TensorFlow. . . Downsampling layer . The patch embedding layer is used to downsample a tensor using convolutional layers . class OverlapPatchEmbed(layers.Layer): def __init__(self, img_size=224, patch_size=7, patch_stride=4, in_chans=3, embed_dim=768, dilation_rate=1, **kwargs): super().__init__(**kwargs) img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1] self.num_patches = self.H * self.W # same pad layer dilation_rate = (dilation_rate, dilation_rate) total_pad = (patch_size[0] - 1) * dilation_rate[0], (patch_size[1] - 1) * dilation_rate[1] top_pad = total_pad[0] // 2 bottom_pad = total_pad[0] - top_pad left_pad = total_pad[1] // 2 right_pad = total_pad[1] - top_pad # noinspection PyAttributeOutsideInit self.pad = layers.ZeroPadding2D(((top_pad, bottom_pad), (left_pad, right_pad))) # embedding self.proj = layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_stride) self.norm = layers.BatchNormalization() def call(self, x): x = self.pad(x) x = self.proj(x) x = self.norm(x) return x . embed = OverlapPatchEmbed() y = embed(tf.zeros((1, 224, 224, 3))) y.shape . TensorShape([1, 56, 56, 768]) . Block layer . The Block layer is repeated multiple times in a stage. It is composed of the following building blocks: . . In TensorFlow, we can implement this layer as follows: . class Block(layers.Layer): def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., activation=&quot;gelu&quot;, **kwargs): super().__init__(**kwargs) self.norm1 = layers.BatchNormalization() self.attn = SpatialAttention(dim) self.drop_path = DropPath(drop_path) self.norm2 = layers.BatchNormalization() mlp_hidden_dim = int(dim * mlp_ratio) self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, activation=activation, drop=drop) layer_scale_init_value = 1e-2 self.layer_scale_1 = tf.Variable( initial_value=layer_scale_init_value * tf.ones((1, 1, 1, dim)), trainable=True) self.layer_scale_2 = tf.Variable( layer_scale_init_value * tf.ones((1, 1, 1, dim)), trainable=True) def call(self, x): x = x + self.drop_path(self.layer_scale_1 * self.attn(self.norm1(x))) x = x + self.drop_path(self.layer_scale_2 * self.mlp(self.norm2(x))) return x . block = Block(3) y = block(tf.zeros((1, 224, 224, 3))) y.shape . TensorShape([1, 224, 224, 3]) . Stage layer . The following layer implements a VAN&#39;s stage by using the OverlapPatchEmbed layer and a sequence of Block layers . class Stage(layers.Layer): def __init__(self, i, embed_dims, mlp_ratios, depths, path_drops, drop_rate=0., **kwargs): super().__init__(**kwargs) # downsample self.patch_embed = OverlapPatchEmbed( patch_size = 7 if 0 == i else 3, patch_stride = 4 if 0 == i else 2, embed_dim = embed_dims[i], name = f&#39;patch_embed{i + 1}&#39; ) # blocks self.blocks = [Block( dim=embed_dims[i], mlp_ratio=mlp_ratios[i], drop=drop_rate, drop_path=path_drops[sum(depths[:i]) + j], name=f&#39;block{i + 1}.{j}&#39; ) for j in range(depths[i])] # normalization self.norm = layers.LayerNormalization(name=f&#39;norm{i + 1}&#39;) def call(self, x): x = self.patch_embed(x) for block in self.blocks: x = block(x) x = self.norm(x) return x . Putting it together . After defining all the major components of the VAN model, we can put them together to build the model. This is fairly straightforward now as we just need to pass the input image through a sequence of Stage layers. Then, add a head to the model that consists of a pooling followed by a dense layer that outputs the result (e.g. class ID if the task is classification). . def create_VAN(embed_dims, mlp_ratios, depths, drop_rate=0., path_drop=0.1, input_shape=(224, 224, 3), pooling=None, classes=2): # stochastic depth decay rule path_drops = np.linspace(0., path_drop, sum(depths)) # input image inputs = layers.Input(shape=input_shape, name=&#39;image&#39;) x = inputs # create stages for i in range(len(depths)): stage = Stage(i, embed_dims, mlp_ratios, depths, path_drops, drop_rate, name = f&#39;stage_{i}&#39;) x = stage(x) # pooling layer if pooling in {None, &#39;avg&#39;}: x = layers.GlobalAveragePooling2D(name=&#39;avg_pool&#39;)(x) elif pooling == &#39;max&#39;: x = layers.GlobalMaxPooling2D(name=&#39;max_pool&#39;)(x) else: raise ValueError(f&#39;Expecting pooling to be one of None/avg/max. Found: {pooling}&#39;) # head classifier x = layers.Dense(classes, name=&#39;head&#39;)(x) outputs = layers.Activation(&#39;softmax&#39;, dtype=&#39;float32&#39;, name=&#39;pred&#39;)(x) # Create model. model = Model(inputs, outputs, name=&#39;van&#39;) return model . The smallest model in terms of parameters is TinyVAN, which we create as follows: . model = create_VAN(embed_dims=(32, 64, 160, 256), mlp_ratios=(8, 8, 4, 4), depths=(3, 3, 5, 2)) . tf.keras.utils.plot_model(model, rankdir=&#39;LR&#39;, show_shapes=True) . model.summary() . Model: &#34;van&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= image (InputLayer) [(None, 224, 224, 3)] 0 stage_0 (Stage) (None, 56, 56, 32) 80384 stage_1 (Stage) (None, 28, 28, 64) 286528 stage_2 (Stage) (None, 14, 14, 160) 1608480 stage_3 (Stage) (None, 7, 7, 256) 1880832 avg_pool (GlobalAveragePool (None, 256) 0 ing2D) head (Dense) (None, 2) 514 pred (Activation) (None, 2) 0 ================================================================= Total params: 3,856,738 Trainable params: 3,849,314 Non-trainable params: 7,424 _________________________________________________________________ . Training . The authors trained the VAN model for various vision tasks (e.g. classification or object detection). They trained the model during 310 epochs using AdamW optimizer with momentum=0.9, weight decay=5 × 10−2 and batch size = 1,024. . For the learning rate (LR), Cosine scheduling and warm-up strategy were used. The initial LR is set to 5 × 10−4 . They used the following data augmentation techniques: . random clipping | random horizontal flipping | label-smoothing | mixup | cutmix | random erasing. | . That&#39;s all folks . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/classification/2022/05/31/VAN_Classification.html",
            "relUrl": "/tensorflow/vision/classification/2022/05/31/VAN_Classification.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Swin Transformer Explained with a TensorFlow implementation",
            "content": "After the introduction of the ViT architecture for image classification, many similar works attempted to use a Transormer based architecture for building a classification mode. One of the top performer is proposed in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. This paper come with the following import contributions: . Building a hierarchical representation of the input image by taking patches of small sizes and gradually increasing their size. | Instead of applying Transformer&#39;s attention to the entire input as usually done in NLP tasks, attention is calculated locally from non-overlapping shifted windows. | Comprehensive experiments on various vision tasks and model architectures and strong results, e.g. 58.7 box AP and 51.1 mask AP on test-dev on COCO object detection. | . . Key differences between Swin Transformer and original Vision Transformer (ViT) model is that ViT produced feature maps of a single low resolution and because it uses a global self-attention ViT has a quadratic computation complexity to input image size. On the other hand, Swin Transformer builds hierarchical feature maps by merging image patches and it has a linear computation complexity to input image size because it uses local windows for calculating selt-attention. . In this article, we will discuss the different concepts of the Swin Transformers (the name Swin stands for Shifted window) model and implement it in TensorFlow. . Overview . As described in the paper arxiv.org and depicted in the following diagram, SWin Transformer works as follows: . Each image is split into fixed-size patches of size 4 x 4 then passed to a sequence of stages | The first stage, Calculate a Patch Embeddings for each patch and also the positional embeddings of the patch then add everything together | The second, third and forth stages starts by merging patches and then passing them through a Swin Transformer block to calculate feature maps | At eash of those later stages, the number of channels of the patches is multiplied by 2 and the width and height of the images is divided by 2. | . . Implementation . Having built a high level idea of the Swin Transformer Architecture, let&#39;s now look at the code-implementation and understand how to implement this architecture in TensorFlow. We will be referencing the code from the Swin-Transformer-Tensorflow repository to explain the implementation. For the PyTorch implementation you can refer to the original implementation of Swin-Transformer. . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.layers import ( Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention, Softmax ) from tensorflow.keras.initializers import TruncatedNormal . Patch Partition Layer . The first component of the Swin-T architecture is a Path Parition layer which is used to partition an input image into multiple small patches. . In TensorFlow, we can simply use the tf.image.extract_patches function to extract patches. We can use it inside a custom Layer to make it easy to use later when building the model . class PatchPartition(Layer): def __init__(self, window_size=4, channles=3): super(PatchPartition, self).__init__() self.window_size = window_size def call(self, images): batch_size = tf.shape(images)[0] patches = tf.image.extract_patches( images=images, sizes=[1, self.window_size, self.window_size, 1], strides=[1, self.window_size, self.window_size, 1], rates=[1, 1, 1, 1], padding=&quot;VALID&quot;, ) patch_dims = patches.shape[-1] patches = tf.reshape(patches, [batch_size, -1, patch_dims]) return patches . Note how similar is this patch extraction layer to the one used in ViT . We could also use Conv2d layer to do the same as a 2-D Convolution uses kernel size (which in our case will be the patch size) and the stride also will be equal to the patch size as we want the windows to not be overlapping. . Alternatively we can implemnt this using tf.reshape like this for instance: . batch, height, width, channels = images.shape patch_num_x = width // window_size patch_num_y = height // window_size new_shape = (-1, patch_num_y, window_size, patch_num_x, window_size, channels) tf.reshape(images, shape=new_shape) . Let&#39;s download a test image and make sure it matches the shape of images in the ImageNet dataset. . !curl -s -o flower.jpeg https://images.unsplash.com/photo-1604085572504-a392ddf0d86a?ixlib=rb-1.2.1&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=224&amp;q=224 . image = plt.imread(&#39;flower.jpeg&#39;) image = tf.image.resize(tf.convert_to_tensor(image), size=(224, 224)) plt.imshow(image.numpy().astype(&quot;uint8&quot;)) plt.axis(&quot;off&quot;); . We can apply the images through our PatchParition layer . batch = tf.expand_dims(image, axis=0) patches = PatchPartition()(batch) patches.shape . TensorShape([1, 3136, 48]) . Now we can examine the partitions that we just created . n = int(np.sqrt(patches.shape[1])) for i, patch in enumerate(patches[0]): ax = plt.subplot(n, n, i + 1) patch_img = tf.reshape(patch, (4, 4, 3)) ax.imshow(patch_img.numpy().astype(&quot;uint8&quot;)) ax.axis(&quot;off&quot;) . Linear Embedding layer . The second component in the Swin-T architecture is the Linear Embedding layer which is simply a combination of projection and embedding layers. It is used to calculate the patch embedding and the position embedding then add both as illustrated with the following diagram . . class LinearEmbedding(Layer): def __init__(self, num_patches, projection_dim, **kwargs): super(LinearEmbedding, self).__init__(**kwargs) self.num_patches = num_patches self.projection = Dense(projection_dim) self.position_embedding = Embedding(input_dim=num_patches, output_dim=projection_dim) def call(self, patch): # calculate patches embeddings patches_embed = self.projection(patch) # calcualte positional embeddings positions = tf.range(start=0, limit=self.num_patches, delta=1) positions_embed = self.position_embedding(positions) # add both embeddings encoded = patches_embed + positions_embed return encoded . We can confirm that the output of this layer is as expected 1, 3136, 96 number of patches is num_patch_x * num_patch_y = 224 / window_size . embeddings = LinearEmbedding(3136, 96)(patches) embeddings.shape . TensorShape([1, 3136, 96]) . Patch Merging layer . Patch merging as it may indicate is used to merge smaller patches into larger ones. Let&#39;s look at the illustration to understand what this layer is doing, if the input of the layer is an 8 x 8 pixel image with 4 patches of 4 x 4 then the output becomes an 8 x 8 pixels but with one large patch of 8 x 8 plus an additional channel layer as channel layers are doubled. . . This layer can be simply implemented as a linear layer so that we can easily define the size of the output and double the channels as follows: . class PatchMerging(Layer): def __init__(self, input_resolution, channels): super(PatchMerging, self).__init__() self.input_resolution = input_resolution self.channels = channels self.linear_trans = Dense(2 * channels, use_bias=False) def call(self, x): height, width = self.input_resolution _, _, C = x.get_shape().as_list() x = tf.reshape(x, shape=(-1, height, width, C)) x0 = x[:, 0::2, 0::2, :] x1 = x[:, 1::2, 0::2, :] x2 = x[:, 0::2, 1::2, :] x3 = x[:, 1::2, 1::2, :] x = tf.concat((x0, x1, x2, x3), axis=-1) x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C)) return self.linear_trans(x) . channels = 96 num_patch_x = 224 // 4 num_patch_y = 224 // 4 out_patches = PatchMerging((num_patch_x, num_patch_y), channels)(patches) print(f&#39;Input shape (B, H * W, C) = {patches.shape}&#39;) print(f&#39;Ouput shape (B, H/2*W/2, 4C) = {out_patches.shape}&#39;) . Input shape (B, H * W, C) = (1, 3136, 48) Ouput shape (B, H/2*W/2, 4C) = (1, 784, 192) . Swin Transfomer block . The Swin Transformer block combines two Transofmer Encoder blocks but uses a window based self-attentions as illustrated in the following diagram. In this section, we will examine each component of this bloc and implement it in TensorFlow . Note: W-MSA stands for Window Multi-head Self-Attention and SW-MSA stands for Shifted Window Multi-head Self-Attention. . . Multilayer Perceptron . A Multilayer Perceptron (MLP) consists basically two dense layers and a GELU activation layer. It is used in the classical Transformer architecture and it is also used in the Swin Transformer blocks. We can simply implement it as custom layer as follows: . class MLP(Layer): def __init__(self, hidden_features, out_features, dropout_rate=0.1): super(MLP, self).__init__() self.dense1 = Dense(hidden_features, activation=tf.nn.gelu) self.dense2 = Dense(out_features) self.dropout = Dropout(dropout_rate) def call(self, x): x = self.dense1(x) x = self.dropout(x) x = self.dense2(x) y = self.dropout(x) return y . mlp = MLP(768 * 2, 768) y = mlp(tf.zeros((1, 197, 768))) y.shape . TensorShape([1, 197, 768]) . Window multi-head self-attention . The following implementaiton support boths of shifted and non-shifted window attention. . class WindowAttention(Layer): def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.): super().__init__() self.dim = dim self.window_size = window_size # Wh, Ww self.num_heads = num_heads head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 # define a parameter table of relative position bias initializer = TruncatedNormal(mean=0., stddev=.02) # position table shape is: (2*Wh-1 * 2*Ww-1, nH) table_shape = ((2*self.window_size[0]-1) * (2*self.window_size[1]-1), num_heads) self.relative_position_bias_table = tf.Variable(initializer(shape=table_shape)) # get pair-wise relative position index for each token inside the window coords_h = tf.range(self.window_size[0]) coords_w = tf.range(self.window_size[1]) coords = tf.stack(tf.meshgrid(coords_h, coords_w)) # 2, Wh, Ww coords_flatten = tf.reshape(coords, [2, -1]) # 2, Wh*Ww relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :] # 2, Wh*Ww, Wh*Ww relative_coords = tf.transpose(relative_coords, perm=[1,2,0]) # Wh*Ww, Wh*Ww, 2 relative_coords = relative_coords + [self.window_size[0] - 1, self.window_size[1] - 1] # shift to start from 0 relative_coords = relative_coords * [2*self.window_size[1] - 1, 1] self.relative_position_index = tf.math.reduce_sum(relative_coords,-1) # Wh*Ww, Wh*Ww self.qkv = Dense(dim * 3, use_bias=qkv_bias, kernel_initializer=initializer) self.attn_drop = Dropout(attn_drop) self.proj = Dense(dim, kernel_initializer=initializer) self.proj_drop = Dropout(proj_drop) self.softmax = Softmax(axis=-1) def call(self, x, mask=None): _, L, N, C = x.shape qkv = tf.transpose(tf.reshape(self.qkv(x), [-1, N, 3, self.num_heads, C // self.num_heads]), perm=[2, 0, 3, 1, 4]) # [3, B_, num_head, Ww*Wh, C//num_head] q, k, v = tf.unstack(qkv) # make torchscript happy (cannot use tensor as tuple) q = q * self.scale attn = tf.einsum(&#39;...ij,...kj-&gt;...ik&#39;, q, k) relative_position_bias = tf.reshape(tf.gather(self.relative_position_bias_table, tf.reshape(self.relative_position_index, [-1])), [self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1]) # Wh*Ww,Wh*Ww,nH relative_position_bias = tf.transpose(relative_position_bias, perm=[2, 0, 1]) # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias if mask is not None: nW = mask.shape[0] # every window has different mask [nW, N, N] attn = tf.reshape(attn, [-1 // nW, nW, self.num_heads, N, N]) + mask[:, None, :, :] # add mask: make each component -inf or just leave it attn = tf.reshape(attn, [-1, self.num_heads, N, N]) attn = self.softmax(attn) else: attn = self.softmax(attn) attn = self.attn_drop(attn) x = tf.reshape(tf.transpose(attn @ v, perm=[0, 2, 1, 3]), [-1, L, N, C]) x = self.proj(x) x = self.proj_drop(x) return x . attn = WindowAttention(96, window_size=(4, 4), num_heads=8, qkv_bias=True, qk_scale=None, attn_drop=0.0, proj_drop=0.0) y = attn(tf.zeros((1, 196, 16, 96))) y.shape . TensorShape([1, 196, 16, 96]) . Helper functions . Before defining the Swin Transformer block, we need couple helper functions to create create windows and merge them . First, window_partition which as the name suggest create windows from the input tensor . def window_partition(x, window_size): _, H, W, C = x.shape num_patch_y = H // window_size num_patch_x = W // window_size x = tf.reshape(x, [-1, num_patch_y, window_size, num_patch_x, window_size, C]) x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5]) windows = tf.reshape(x, [-1, num_patch_x * num_patch_y, window_size, window_size, C]) return windows . windows = window_partition(batch, 4) print(f&#39;Input shape (B, H, W, C) = {batch.shape}&#39;) print(f&#39;Ouput shape (num_windows*B, window_size, window_size, C) = {windows.shape}&#39;) . Input shape (B, H, W, C) = (1, 224, 224, 3) Ouput shape (num_windows*B, window_size, window_size, C) = (1, 3136, 4, 4, 3) . Second, window_reverse which as the name suggest reverse the created windows . def window_reverse(windows, window_size, H, W): C = windows.shape[-1] B = int(windows.shape[1] / (H * W / window_size / window_size)) x = tf.reshape(windows, [B, H // window_size, W // window_size, window_size, window_size, C]) x = tf.reshape(tf.transpose(x, perm=[0, 1, 3, 2, 4, 5]), [-1, H, W, C]) return x . y = window_reverse(windows, 4, 224, 224) print(f&#39;Input shape (B, num_windows*B, window_size, window_size, C) = {windows.shape}&#39;) print(f&#39;Ouput shape (B, H, W, C) = {y.shape}&#39;) . Input shape (B, num_windows*B, window_size, window_size, C) = (1, 3136, 4, 4, 3) Ouput shape (B, H, W, C) = (1, 224, 224, 3) . class DropPath(Layer): def __init__(self, prob): super().__init__() self.drop_prob = prob def call(self, x, training=None): if self.drop_prob == 0. or not training: return x keep_prob = 1 - self.drop_prob shape = (x.shape[0],) + (1,) * (x.ndim - 1) random_tensor = tf.random.uniform(shape=shape) random_tensor = tf.where(random_tensor &lt; keep_prob, 1, 0) output = x / keep_prob * random_tensor return output . drop = DropPath(0.2) y = drop(tf.zeros((1, 197, 768))) y.shape . TensorShape([1, 197, 768]) . SwinTransformerBlock . With the shifted window partitioning approach, consecutive Swin Transformer blocks are computed as . class SwinTransformerBlock(Layer): def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.): super().__init__() self.dim = dim self.input_resolution = input_resolution self.num_heads = num_heads self.window_size = window_size self.shift_size = shift_size self.mlp_ratio = mlp_ratio if min(self.input_resolution) &lt;= self.window_size: # if window size is larger than input resolution, we don&#39;t partition windows self.shift_size = 0 self.window_size = min(self.input_resolution) assert 0 &lt;= self.shift_size &lt; self.window_size, &quot;shift_size must in 0-window_size&quot; self.norm1 = LayerNormalization(epsilon=1e-5) self.attn = WindowAttention( dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop) self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else tf.identity self.norm2 = LayerNormalization(epsilon=1e-5) mlp_hidden_dim = int(dim * mlp_ratio) self.mlp = MLP(mlp_hidden_dim, dim, dropout_rate=drop) if self.shift_size &gt; 0: # calculate attention mask for SW-MSA H, W = self.input_resolution img_mask = np.zeros([1, H, W, 1]) # 1 H W 1 h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) cnt = 0 for h in h_slices: for w in w_slices: img_mask[:, h, w, :] = cnt cnt += 1 img_mask = tf.constant(img_mask) mask_windows = window_partition(img_mask, self.window_size) # nW, window_size, window_size, 1 mask_windows = tf.reshape(mask_windows, [-1, self.window_size * self.window_size]) attn_mask = mask_windows[:, None, :] - mask_windows[:, :, None] self.attn_mask = tf.where(attn_mask==0, -100., 0.) else: self.attn_mask = None def call(self, x): H, W = self.input_resolution B, L, C = x.shape assert L == H * W, &quot;input feature has wrong size&quot; shortcut = x x = self.norm1(x) x = tf.reshape(x, [-1, H, W, C]) # cyclic shift if self.shift_size &gt; 0: shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=(1, 2)) else: shifted_x = x # partition windows x_windows = window_partition(shifted_x, self.window_size) # nW*B, window_size, window_size, C x_windows = tf.reshape(x_windows, [-1, x_windows.shape[1], self.window_size * self.window_size, C]) # nW*B, window_size*window_size, C # W-MSA/SW-MSA attn_windows = self.attn(x_windows, mask=self.attn_mask) # nW*B, window_size*window_size, C # merge windows attn_windows = tf.reshape(attn_windows, [-1, x_windows.shape[1], self.window_size, self.window_size, C]) shifted_x = window_reverse(attn_windows, self.window_size, H, W) # B H&#39; W&#39; C # reverse cyclic shift if self.shift_size &gt; 0: x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=(1, 2)) else: x = shifted_x x = tf.reshape(x, [-1, H * W, C]) # FFN x = shortcut + self.drop_path(x) x = x + self.drop_path(self.mlp(self.norm2(x))) return x . block = SwinTransformerBlock(96, (56, 56), 8, window_size=4) y = block(embeddings) y.shape . TensorShape([1, 3136, 96]) . Putting it together . After defining all the major components of the Swin-T architecture, we can put them together to build the model. This is fairly straightforward now as we just need to plug a window partition to an blocks of Swin Transformers separated by a merging layer. For classification, we add a pooling then a dense layer to the form the head of the model. . . def create_SwinTransformer(num_classes, input_shape=(224, 224, 3), window_size=4, embed_dim=96, num_heads=8): num_patch_x = input_shape[0] // window_size num_patch_y = input_shape[1] // window_size inputs = Input(shape=input_shape) # Patch extractor patches = PatchPartition(window_size)(inputs) patches_embed = LinearEmbedding(num_patch_x * num_patch_y, embed_dim)(patches) # first Swin Transformer block out_stage_1 = SwinTransformerBlock( dim=embed_dim, input_resolution=(num_patch_x, num_patch_y), num_heads=num_heads, window_size=window_size, shift_size=0 )(patches_embed) # second Swin Transformer block out_stage_1 = SwinTransformerBlock( dim=embed_dim, input_resolution=(num_patch_x, num_patch_y), num_heads=num_heads, window_size=window_size, shift_size=1 )(out_stage_1) # patch merging representation = PatchMerging((num_patch_x, num_patch_y), channels=embed_dim)(out_stage_1) # pooling representation = GlobalAveragePooling1D()(representation) # logits output = Dense(num_classes, activation=&quot;softmax&quot;)(representation) # Create model model = Model(inputs=inputs, outputs=output) return model . . Note: In our case we are using only one stage of Swin Transformer blocks. In the original paper, there are 4 of those stages. . model = create_SwinTransformer(2) . model.summary() . Model: &#34;model_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_21 (InputLayer) [(None, 224, 224, 3)] 0 patch_partition_21 (PatchPa (None, None, 48) 0 rtition) linear_embedding_22 (Linear (None, 3136, 96) 305760 Embedding) swin_transformer_block_65 ( (None, 3136, 96) 112232 SwinTransformerBlock) swin_transformer_block_66 ( (None, 3136, 96) 112232 SwinTransformerBlock) patch_merging_15 (PatchMerg (None, 784, 192) 73728 ing) global_average_pooling1d_3 (None, 192) 0 (GlobalAveragePooling1D) dense_344 (Dense) (None, 2) 386 ================================================================= Total params: 604,338 Trainable params: 604,338 Non-trainable params: 0 _________________________________________________________________ . That&#39;s all folks . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/classification/2022/02/27/Swin_Transfomer.html",
            "relUrl": "/tensorflow/vision/classification/2022/02/27/Swin_Transfomer.html",
            "date": " • Feb 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Bedtime stories generated by AI - the speech part",
            "content": "This article is a deed dive on how episodes for the Bedtime stories podcast are generated. Specifically, how the speech is generated and how it is composed with background audion. To learn more about the podcast, check this overview article - Bedtime stories generated by AI. . &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; For the speech generation, I use the TensorFlowTTS library and the pre-trained models. Unfortunately, this library provides only one voice but hopefully in the future there will be more voices available. . Setup . First, we need to install the speech libraries . %%capture %%bash pip install pydub pip install git+https://github.com/TensorSpeech/TensorFlowTTS.git pip install git+https://github.com/repodiac/german_transliterate.git#egg=german_transliterate . . Note: You must restart the runtime in order to use newly installed versions. . import os import re import numpy as np from pydub import AudioSegment from pydub.playback import play import soundfile import subprocess import tempfile import IPython.display as ipd from tqdm import tqdm import tensorflow as tf from tensorflow_tts.inference import TFAutoModel from tensorflow_tts.inference import AutoConfig from tensorflow_tts.inference import AutoProcessor . [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /root/nltk_data... [nltk_data] Unzipping taggers/averaged_perceptron_tagger.zip. [nltk_data] Downloading package cmudict to /root/nltk_data... [nltk_data] Unzipping corpora/cmudict.zip. . Second, we need to download the speech models Tacotron 2 and Melgan which were trained on the LJ Speech Dataset. . tacotron2 = TFAutoModel.from_pretrained(&quot;tensorspeech/tts-tacotron2-ljspeech-en&quot;, name=&quot;tacotron2&quot;) . melgan = TFAutoModel.from_pretrained(&quot;tensorspeech/tts-melgan-ljspeech-en&quot;, name=&quot;melgan&quot;) . processor = AutoProcessor.from_pretrained(&quot;tensorspeech/tts-tacotron2-ljspeech-en&quot;) . Speech to text . We need to define couple helper functions. For instance, a helper function to perform actual speech synthesis for a given text . def text2speech(input_text, text2mel_model, vocoder_model): input_ids = processor.text_to_sequence(input_text) # text2mel part _, mel_outputs, stop_token_prediction, alignment_history = text2mel_model.inference( tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0), tf.convert_to_tensor([len(input_ids)], tf.int32), tf.convert_to_tensor([0], dtype=tf.int32) ) # vocoder part audio = vocoder_model(mel_outputs)[0, :, 0] return mel_outputs.numpy(), alignment_history.numpy(), audio.numpy() . Because I could not perform speech synthesis on large text, I needed a helper function that will chunk a large text into smaller chunks. . def split_into_chunks(text: str, max_length): &quot;&quot;&quot;Split a chunk of text into chunks of max_length and return a list of them.&quot;&quot;&quot; sentences = re.split(r&quot;(?&lt;= .) s+(?=[A-Z])&quot;, text.replace(&quot; n&quot;, &quot; &quot;)) chunks = [] current_chunk = [] chunk_length = 0 for sentence in sentences: sentence_length = len(sentence) if chunk_length + sentence_length + 1 &gt; max_length: # This chunk would overflow, make a new chunk. chunks.append(&quot; &quot;.join(current_chunk)) current_chunk = [] chunk_length = 0 current_chunk.append(sentence) chunk_length += sentence_length + 1 chunks.append(&quot; &quot;.join(current_chunk)) return chunks . %%bash rm -rf *.mp3 rm -rf *.wav . Next paste the story text in placeholder variable . story = &quot;&quot;&quot; replace with actual story &quot;&quot;&quot; . Then, create chunks from the story text and place those chunks around some introductory and prelude texts. . number = 1 begining = f&quot;&quot;&quot; Welcome to Episode {number} of the Bedtime short stories podcast, the AI generated podcast with short stories to help you sleep. I am Ex Machina, and I will be narrating your story tonight. &quot;&quot;&quot; end = f&quot;&quot;&quot; I hope you didn&#39;t make it so far and you are already asleep. If not then I hope you have enjoyed this short story. See next time. &quot;&quot;&quot;.strip() . chunks = split_into_chunks(story, 1000) chunks = [begining] + chunks + [end] . # setup window for tacotron2 if you want to try tacotron2.setup_window(win_front=10, win_back=10) . Now we can generate the speech for every chunk and save it in a separate WAV file . sr = 22050 chunk_names = [] for index, chunk in tqdm(enumerate(chunks), total=len(chunks)): mels, alignment_history, audios = text2speech(chunk, tacotron2, melgan) chunk_name = f&#39;voice_{number}_part_{index}.wav&#39; soundfile.write(chunk_name, audios, sr, &#39;PCM_24&#39;) chunk_names.append(chunk_name) . 100%|██████████| 10/10 [07:45&lt;00:00, 46.54s/it] . Silence . To make the episode speech less stressfull, adding short silence sections is a good idea. . First, we generate a WAV file with 3 seconds of silence . silence_segment = AudioSegment.silent(duration=3000) silence_segment.export(&#39;silence.wav&#39;, format=&quot;wav&quot;); . Second, we place the silence audio with the rest of the episode audio . first = 1 last = len(chunk_names) - 1 chunk_names = chunk_names[:first] + [&#39;silence.wav&#39;] + chunk_names[first: last] + [&#39;silence.wav&#39;] + chunk_names[last:] . Then we concatenate different audio chunks to generate the speech file of the episode . def concatenate_tracks(chunk_names, output): &quot;&quot;&quot;Concatenate mutliple audio tracks into one.&quot;&quot;&quot; audios = np.array([]) for chunk_name in tqdm(chunk_names): audio , _ = soundfile.read(chunk_name) audios = np.concatenate([audios, audio]) soundfile.write(output, audios, sr, &#39;PCM_24&#39;) def wav2mp3(input, output): audio = AudioSegment.from_wav(input) audio.export(output, format=&quot;mp3&quot;) . concatenate_tracks(chunk_names, &#39;voice.wav&#39;) . 100%|██████████| 12/12 [00:00&lt;00:00, 45.51it/s] . For convinience when later adding background, I convert the WAV audio file into the MP3 format. . wav2mp3(&#39;voice.wav&#39;, &#39;voice.mp3&#39;) . Background sound . To make the episode more interesting I add a background sound that matches the theme of the episode. I use freesound.org which is a great resouce for loyalty free audio. For instance, some interesting audios: Ocean waves, rain with thunder. . First, download the audio that best matches the episode theme . !curl -s -o base_background.mp3 https://freesound.org/data/previews/237/237729_3839718-lq.mp3 . voice_duration = AudioSegment.from_wav(&#39;voice.wav&#39;).duration_seconds background_duration = AudioSegment.from_mp3(&#39;base_background.mp3&#39;).duration_seconds print(f&quot;Voice duration is {voice_duration} seconds vs base background in {background_duration} seconds.&quot;) . Voice duration is 397.30965986394557 seconds vs base background in 32.875083333333336 seconds. . base_background = AudioSegment.from_mp3(&quot;base_background.mp3&quot;) background = base_background for _ in range(int(voice_duration / background_duration) + 1): background = background + base_background background.export(&quot;background.mp3&quot;, format=&quot;mp3&quot;) background_duration = background.duration_seconds print(f&quot;Voice duration is {voice_duration} seconds vs background in {background_duration} seconds.&quot;) . Voice duration is 397.30965986394557 seconds vs background in 460.2511666666667 seconds. . def add_background_track(episode_file, background_file, output): tempbg = tempfile.mkstemp()[1] tempepisode = tempfile.mkstemp()[1] episode = AudioSegment.from_mp3(episode_file) background = AudioSegment.from_mp3(background_file) padded_episode = AudioSegment.silent(duration=7000) + episode + AudioSegment.silent(duration=8000) padded_episode.export(tempepisode, format=&#39;mp3&#39;) cut_bg = background[: padded_episode.duration_seconds * 1000].fade_in(3000).fade_out(5000) # Lower the background track volume. lower_volume_cut_bg = cut_bg - 10 lower_volume_cut_bg.export(tempbg, format=&#39;mp3&#39;) subprocess.run( [ &quot;ffmpeg&quot;, &quot;-y&quot;, &quot;-i&quot;, tempbg, &quot;-i&quot;, tempepisode, &quot;-filter_complex&quot;, &quot;amerge,acompressor=threshold=-21dB:ratio=12:attack=100:release=500&quot;, &quot;-ac&quot;, &quot;2&quot;, &quot;-c:a&quot;, &quot;libmp3lame&quot;, &quot;-q:a&quot;, &quot;4&quot;, output, ] ) os.unlink(tempbg) os.unlink(tempepisode) . Finally, add the background to the voice file and disply the final result . add_background_track(&#39;voice.mp3&#39;, &#39;background.mp3&#39;, &#39;episode.mp3&#39;) . AudioSegment.from_mp3(&#39;episode.mp3&#39;) . Your browser does not support the audio element. That&#39;s all folks . You can give the podcast a try, all episodes are pulished here https://anchor.fm/exmachina . I would love to hear any feedack, suggestions or ideas for improvement. So feel free to leave a comment or reach out on twitter @bachiirc . &lt;/div&gt; .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/speech/2022/02/18/bedtime-stories-speech.html",
            "relUrl": "/tensorflow/speech/2022/02/18/bedtime-stories-speech.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Exploring bias in Transformers",
            "content": "In Recent years, language models are able to achieve remarkable results, from writing stories (e.g. GPT-2 and GPT-3), turn captions into images, or write code. . Some of those models are trained by a masking technique: taking sentences, splitting them into tokens (i.e. words), and then randomly hiding some of those tokens and train the model to predict those hidden tokens. At the core of those models is a Transformer architecture, hence the name of the transfomers library. Learn more on performing Masked Language Modeling here - link. . In this post, we will try BERT one of those language models and see how its prediction for masked words different based on location or gender. This post is insipired by the followin article - What Have Language Models Learned? . . Let&#39;s install and import dependencies . %%capture %%bash pip install pyyaml==5.4.1 pip install -q transformers . from transformers import TFAutoModelForMaskedLM, AutoTokenizer import tensorflow as tf import plotly.express as px import math import pandas as pd . /usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details. defaults = yaml.load(f) . BERT has lot variation and the transformers library offer easy access many of them, we will use bert-base-uncased but you can find more models here - link . tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) model = TFAutoModelForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;) . All model checkpoint layers were used when initializing TFBertForMaskedLM. All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training. . Let&#39;s define a helper function, that will tokenize a sentence, encode its tokens with IDs in the model vocabulary, do a forward pass and retrieve the predictions at the index of the mask token. As we need probablities we will pass the predictions through a softmax function and collect the top N tokens. . def top_words(sequence, k=100): inputs = tokenizer(sequence, return_tensors=&quot;tf&quot;) mask_token_index = tf.where(inputs[&quot;input_ids&quot;] == tokenizer.mask_token_id)[0, 1] token_logits = model(**inputs).logits mask_token_logits = token_logits[0, mask_token_index, :] mask_token_probs = tf.nn.softmax(mask_token_logits) top_k = tf.math.top_k(mask_token_probs, k) probs, indices = top_k.values.numpy(), top_k.indices.numpy() tokens = [tokenizer.decode([index]) for index in indices] return tokens, probs . For plotting the predicted tokens and using the probabilities as coordiates, we create the following function . def plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title): intersection = list(set(tokens1).intersection(tokens2)) indicies1 = [tokens1.index(token) for token in intersection] indicies2 = [tokens2.index(token) for token in intersection] df = pd.DataFrame.from_dict({ &#39;token&#39;: intersection, &#39;X&#39;: [probs1[index] for index in indicies1], &#39;Y&#39;: [probs2[index] for index in indicies2] }) fig = px.scatter(df, x=&quot;X&quot;, y=&quot;Y&quot;, text=&quot;token&quot;, log_x=True, log_y=True, size_max=60, labels=dict(X=label1, Y=label2) ) fig.update_traces(textposition=&#39;top center&#39;) fig.update_layout(height=800, title_text=title) fig.show() . Let&#39;s define a sentence and try to see how the language model top prediction changes based on the location for New York vs Texas . tokens1, probs1 = top_words(f&quot;in New York, they like to buy {tokenizer.mask_token}.&quot;, 200) tokens2, probs2 = top_words(f&quot;in Texas, they like to buy {tokenizer.mask_token}.&quot;, 200) label1 = &quot;New York&quot; label2 = &#39;Texas&#39; title = &#39;Likelihood per token: New York vs Texas&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . You can see the bias for books, clothes which seem to be predicted more for New Yorkers, while buying cattle have higher likelihood for people in Texas. . Let&#39;s try another set of locations: Londeners vs Algerois . tokens1, probs1 = top_words(f&quot;in London, they like to buy {tokenizer.mask_token}.&quot;, 200) tokens2, probs2 = top_words(f&quot;in Algiers, they like to buy {tokenizer.mask_token}.&quot;, 200) label1 = &#39;London&#39; label2 = &#39;Algiers&#39; title = &#39;Likelihood per token: London vs Algiers&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . Seems that people in London tend to buy way more books than those in Algiers . Now, lets try something with gender difference . tokens1, probs1 = top_words(f&quot;Lauren was born in the year of {tokenizer.mask_token}.&quot;, 200) tokens2, probs2 = top_words(f&quot;Elsie was born in the year of {tokenizer.mask_token}.&quot;, 200) label1 = &#39;Lauren&#39; label2 = &#39;Elsie&#39; title = &#39;Likelihood per token: Lauren vs Elsie&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . tokens1, probs1 = top_words(f&quot;Jane worked as a {tokenizer.mask_token}.&quot;, 200) tokens2, probs2 = top_words(f&quot;Jim worked as a {tokenizer.mask_token}.&quot;, 200) label1 = &#39;Jane&#39; label2 = &#39;Jim&#39; title = &#39;Likelihood per token: Jane vs Jim&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . It is funny how lot of those top predictions does not even make sense, but still the model predicted maid for Jane vs salesman for Jim. . tokens1, probs1 = top_words(f&quot;The doctor performed CPR even though {tokenizer.mask_token} knew it was too late.&quot;, 200) tokens2, probs2 = top_words(f&quot;The nurse performed CPR even though {tokenizer.mask_token} knew it was too late.&quot;, 200) label1 = &#39;Doctor&#39; label2 = &#39;Nurse&#39; title = &#39;Likelihood per token: Doctor vs Nurse&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . Here while most predictions have low likelihood for both, still he seems to be highly associated with doctor and she is highly associated with nurse. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/nlp/explainability/2022/02/04/transformers_bias.html",
            "relUrl": "/tensorflow/nlp/explainability/2022/02/04/transformers_bias.html",
            "date": " • Feb 4, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Vision Transformer in TensorFlow",
            "content": "The publication of the Vision Transformer (or simply ViT) architecture in An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale had a great impact on the use of a Transformer-based architecture in computer vision problems. In fact, it was the first architecture that made good results on the ImageNet because of those two factors: . Applying Transformers in the processing of the entire image. | Also the training method used in ViT had impact on the model performance. | . Overview . As described in the paper arxiv.org and depicted in the following diagram, ViT works as follows: . Each image is split into fixed-size patches | Calculate a Patch Embeddings for each patch | Add position embeddings a class token to each of the Patch Embeddings | The sequence of embeddings are passed to a Transformer encoder | Pass the representations through a MLP Head to get final class predictions. | . As a detailed example, when trying to classify an image (with similar size as ImageNet images: 224 x 224 x 3, the process is as follows: . First, divide the image into patches of size 16 x 16. This will create 14 x 14 = 196 patches. Then, we put the patches in sequence one after the other as depicted in the diagram above. . Second, pass each of those patches through a linear layer to get an embeddings vector or Patch Embeddings of the patch of size 1 x 768 (note that 3 x 16 x 16 = 768). In the diagram those vectors are colored in pink. . Third, for each patch we calculate it&#39;s Position Embeddings (as shown in purple) then add this vector to the previous Patch Embeddings. We also append at the begining the embeddings of the special token [class]. . After this, we endup with a matrix of size 1 x 768 + 196 x 768 = 197 x 768. . Forth, the patch embeddings are passed through Transformer Encoder to get the learned representations of the [class] token. The output will be a 1 x 768 vector. . Fifth, the representation generated by the transformer is passed to the MLP Head (which is simply a Linear Layer) to generate the class predictions. . Implementation . As described in the previous section, the ViT model consits of a patch embedding, multiple Transformer blocks with self-attention layer, and a multilayer perceptron. . In the rest of this article we will go over each component of the ViT architecture and implement it in TensorFlow. . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.layers import Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention . Patch extraction . The first step in implementing the ViT model is the extraction of pathches from an input image as depicted in the following illustration . . In TensorFlow, we can simply use the tf.image.extract_patches function to extract patches. We can use it inside a custom Layer to make it easy to use later when building the model . class PatchExtractor(Layer): def __init__(self): super(PatchExtractor, self).__init__() def call(self, images): batch_size = tf.shape(images)[0] patches = tf.image.extract_patches( images=images, sizes=[1, 16, 16, 1], strides=[1, 16, 16, 1], rates=[1, 1, 1, 1], padding=&quot;VALID&quot;, ) patch_dims = patches.shape[-1] patches = tf.reshape(patches, [batch_size, -1, patch_dims]) return patches . !curl -s -o flower.jpeg https://images.unsplash.com/photo-1604085572504-a392ddf0d86a?ixlib=rb-1.2.1&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=224&amp;q=224 . image = plt.imread(&#39;flower.jpeg&#39;) image = tf.image.resize(tf.convert_to_tensor(image), size=(224, 224)) plt.imshow(image.numpy().astype(&quot;uint8&quot;)) plt.axis(&quot;off&quot;); . For an image of size (224, 224) we get 196 patches of 16x16 . batch = tf.expand_dims(image, axis=0) patches = PatchExtractor()(batch) patches.shape . TensorShape([1, 196, 768]) . We can visualize all the 196 patches . n = int(np.sqrt(patches.shape[1])) for i, patch in enumerate(patches[0]): ax = plt.subplot(n, n, i + 1) patch_img = tf.reshape(patch, (16, 16, 3)) ax.imshow(patch_img.numpy().astype(&quot;uint8&quot;)) ax.axis(&quot;off&quot;) . Patch Encoding . The Patch encoder takes as input the patches and generate their embeddings which later get passes to the Transformer. For each path, it will also create a positional embeddings vector. The following illustration describes inner working of the patch encoder. . . The PatchEncoder implementation is straightforward, we need a Dense layer to project a patch into a vector of size projection_dim, plus an Embedding layer to learn the positional embeddings. We also need a trainable tf.Variable that will learn the [class] token embeddings. . We use a custom layer to put all this together as follows: . class PatchEncoder(Layer): def __init__(self, num_patches=196, projection_dim=768): super(PatchEncoder, self).__init__() self.num_patches = num_patches self.projection_dim = projection_dim w_init = tf.random_normal_initializer() class_token = w_init(shape=(1, projection_dim), dtype=&quot;float32&quot;) self.class_token = tf.Variable(initial_value=class_token, trainable=True) self.projection = Dense(units=projection_dim) self.position_embedding = Embedding(input_dim=num_patches+1, output_dim=projection_dim) def call(self, patch): batch = tf.shape(patch)[0] # reshape the class token embedins class_token = tf.tile(self.class_token, multiples = [batch, 1]) class_token = tf.reshape(class_token, (batch, 1, self.projection_dim)) # calculate patches embeddings patches_embed = self.projection(patch) patches_embed = tf.concat([patches_embed, class_token], 1) # calcualte positional embeddings positions = tf.range(start=0, limit=self.num_patches+1, delta=1) positions_embed = self.position_embedding(positions) # add both embeddings encoded = patches_embed + positions_embed return encoded . We can confirm that the output of this layer is as expected 1, 197, 768 . embeddings = PatchEncoder()(patches) embeddings.shape . TensorShape([1, 197, 768]) . Multilayer Perceptron . A Multilayer Perceptron (MLP) consists basically two dense layers and a GELU activation layer. It is used in the Transformer encoder as well as the final output layer of the ViT model. We can simply implement it as custom layer as follows: . class MLP(Layer): def __init__(self, hidden_features, out_features, dropout_rate=0.1): super(MLP, self).__init__() self.dense1 = Dense(hidden_features, activation=tf.nn.gelu) self.dense2 = Dense(out_features) self.dropout = Dropout(dropout_rate) def call(self, x): x = self.dense1(x) x = self.dropout(x) x = self.dense2(x) y = self.dropout(x) return y . mlp = MLP(768 * 2, 768) y = mlp(tf.zeros((1, 197, 768))) y.shape . TensorShape([1, 197, 768]) . Transformer Encoder . The transformer encoder consists of a sequence of L blocs of typical Transformer architecture. So we just need to implement the bloc once and use it multiple times . . The transfomer bloc uses LayerNormalization and MultiHeadAttention layers, along with some skip connections. The following custom layer implements the Transformer bloc . class Block(Layer): def __init__(self, projection_dim, num_heads=4, dropout_rate=0.1): super(Block, self).__init__() self.norm1 = LayerNormalization(epsilon=1e-6) self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate) self.norm2 = LayerNormalization(epsilon=1e-6) self.mlp = MLP(projection_dim * 2, projection_dim, dropout_rate) def call(self, x): # Layer normalization 1. x1 = self.norm1(x) # encoded_patches # Create a multi-head attention layer. attention_output = self.attn(x1, x1) # Skip connection 1. x2 = Add()([attention_output, x]) #encoded_patches # Layer normalization 2. x3 = self.norm2(x2) # MLP. x3 = self.mlp(x3) # Skip connection 2. y = Add()([x3, x2]) return y . block = Block(768) y = block(tf.zeros((1, 197, 768))) y.shape . TensorShape([1, 197, 768]) . Now, we can simply create custom layer that implements a Transfomer using the previous Block layer. . class TransformerEncoder(Layer): def __init__(self, projection_dim, num_heads=4, num_blocks=12, dropout_rate=0.1): super(TransformerEncoder, self).__init__() self.blocks = [Block(projection_dim, num_heads, dropout_rate) for _ in range(num_blocks)] self.norm = LayerNormalization(epsilon=1e-6) self.dropout = Dropout(0.5) def call(self, x): # Create a [batch_size, projection_dim] tensor. for block in self.blocks: x = block(x) x = self.norm(x) y = self.dropout(x) return y . transformer = TransformerEncoder(768) y = transformer(embeddings) y.shape . TensorShape([1, 197, 768]) . Putting it together . After defining all the major components of the ViT architecture, we can put them together to build the model. This is fairly straightforward now as we just need to plug a patch extract to an encoder to a transformer to a multilayer perceptron as depicted in the following diagram . . def create_VisionTransformer(num_classes, num_patches=196, projection_dim=768, input_shape=(224, 224, 3)): inputs = Input(shape=input_shape) # Patch extractor patches = PatchExtractor()(inputs) # Patch encoder patches_embed = PatchEncoder(num_patches, projection_dim)(patches) # Transformer encoder representation = TransformerEncoder(projection_dim)(patches_embed) representation = GlobalAveragePooling1D()(representation) # MLP to classify outputs logits = MLP(projection_dim, num_classes, 0.5)(representation) # Create model model = Model(inputs=inputs, outputs=logits) return model . model = create_VisionTransformer(2) . model.summary() . Model: &#34;model_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_7 (InputLayer) [(None, 224, 224, 3)] 0 patch_extractor_5 (PatchExt (None, None, 768) 0 ractor) patch_encoder_13 (PatchEnco (None, 197, 768) 742656 der) transformer_encoder_11 (Tra (None, 197, 768) 141743616 nsformerEncoder) global_average_pooling1d_7 (None, 768) 0 (GlobalAveragePooling1D) mlp_151 (MLP) (None, 2) 592130 ================================================================= Total params: 143,078,402 Trainable params: 143,078,402 Non-trainable params: 0 _________________________________________________________________ . That&#39;s all folks . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/classification/2021/10/01/vision_transformer.html",
            "relUrl": "/tensorflow/vision/classification/2021/10/01/vision_transformer.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Effortless video querying with TF Hub",
            "content": "Combining vision and language usually leads to better performance whatever is the task. In this article, we will see how we can leaverage a pre-trained model to search videos by activity. . We will use a model called Multiple Instance Learning (MIL) and Noise Contrastive Estimation (NCE) or simply MIL-NCE from TensorFlow Hub. This model was trained on the HowTo100M dataset which is a large-scale dataset of narrated 136M video clips where content creators explain the tasks being performed in the video. . This model can generate embeddings for video and text, we will use this capability to perform retrieval of the best video matching a query by calculating the distance between the two embeddings. . To learn more about the model and the general task it was trained for check the original paper here - arxiv.org . . Let&#39;s start but getting the dependencies . import math import uuid import cv2 import numpy as np from pathlib import Path import tensorflow as tf import tensorflow_hub as tfhub from google.colab.patches import cv2_imshow from IPython.display import Image . We need a function to download a video for a given url, store it locally and give it a random file name, we will use this later to get the test videos . def fetch_video(video_url): extension = Path(video_url).suffix file_name = str(uuid.uuid4()) + extension path = tf.keras.utils.get_file(file_name, video_url, cache_dir=&#39;.&#39;, cache_subdir=&#39;.&#39;) return path . Next, we define a function that will crop the frames to a square selected to be in the middle of the frame: . def get_center_square_coordinates(height, width): dimension = min(width, height) x_start = (width // 2) - (dimension // 2) x_end = x_start + dimension y_start = (height // 2) - (dimension // 2) y_end = y_start + dimension return x_start, y_start, x_end, y_end def crop_center(in_frame): height, width = in_frame.shape[:2] x_start, y_start, x_end, y_end = get_center_square_coordinates(height, width) out_frame = in_frame[y_start:y_end, x_start:x_end] return out_frame . Next, we define a set of helper functions to read a video file and extract at most 32 frames (if video has fewer frames we repeat some) . def extract_frames(video_path, max_frames, resize=(224, 224)): &quot;&quot;&quot;Extract at most max_frames from the video&quot;&quot;&quot; capture = cv2.VideoCapture(video_path) frames = [] while len(frames) &lt;= max_frames: frame_read, frame = capture.read() if not frame_read: break frame = crop_center(frame) frame = cv2.resize(frame, resize) frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) frames.append(frame) capture.release() return np.array(frames) def repeat_frames(in_frames, max_frames): &quot;&quot;&quot;Repeat frames until reaching a length of max_frames&quot;&quot;&quot; if len(in_frames) &gt;= max_frames: return in_frames repetitions = math.ceil(float(max_frames) / len(in_frames)) repetitions = int(repetitions) out_frames = in_frames.repeat(repetitions, axis=0) return out_frames def read_video(video_path, max_frames=32, resize=(224, 224)): # read frame and extract its frames frames = extract_frames(video_path, max_frames, resize) # make sure we have max_frames frames = repeat_frames(frames, max_frames) # select only max_frames frames = frames[:max_frames] return frames / 255.0 . For the test, we will use random GIF files . URLS = [ &#39;https://media.giphy.com/media/Wrm9ZTb7LFMcESBA4i/giphy.gif&#39;, &#39;https://media.giphy.com/media/Bom5hTsAsI8sFnH68k/giphy.gif&#39;, &#39;https://media.giphy.com/media/2aLiVCqTZmxwXeRfMh/giphy.gif&#39;, &#39;https://media.giphy.com/media/ngzhAbaGP1ovS/giphy.gif&#39;] . Image(url=URLS[-1]) . Use the previsous helper functions to download the videos and extract the frames . VIDEOS = [read_video(fetch_video(url)) for url in URLS] . Downloading data from https://media.giphy.com/media/Wrm9ZTb7LFMcESBA4i/giphy.gif 7331840/7331587 [==============================] - 0s 0us/step 7340032/7331587 [==============================] - 0s 0us/step Downloading data from https://media.giphy.com/media/Bom5hTsAsI8sFnH68k/giphy.gif 5185536/5181685 [==============================] - 0s 0us/step 5193728/5181685 [==============================] - 0s 0us/step Downloading data from https://media.giphy.com/media/2aLiVCqTZmxwXeRfMh/giphy.gif 23699456/23697427 [==============================] - 0s 0us/step 23707648/23697427 [==============================] - 0s 0us/step Downloading data from https://media.giphy.com/media/ngzhAbaGP1ovS/giphy.gif 1130496/1126589 [==============================] - 0s 0us/step 1138688/1126589 [==============================] - 0s 0us/step . For the search test we define those random queries . QUERIES = [&#39;biking&#39;, &#39;launching&#39;, &#39;skiing&#39;, &#39;skateboarding&#39;] . We load the pre-trained model and its weights from TF Hub which is available at https://tfhub.dev/deepmind/mil-nce/s3d/1 . model = tfhub.load(&#39;https://tfhub.dev/deepmind/mil-nce/s3d/1&#39;) . Next we define a helper function that will use the model to calcualte the embeddings of the input video . def get_video_embeddings(model, input_frames): frames = tf.cast(input_frames, dtype=tf.float32) frames = tf.constant(frames) video_model = model.signatures[&#39;video&#39;] video_embedding = video_model(frames) video_embedding = video_embedding[&#39;video_embedding&#39;] return video_embedding . Similarly, we also define a helper function that will use the model to calcualte the embeddings of the input text . def get_text_embeddings(model, input_words): words = tf.constant(input_words) text_model = model.signatures[&#39;text&#39;] text_embedding = text_model(words) text_embedding = text_embedding[&#39;text_embedding&#39;] return text_embedding . Now we calculate the embeddings for the text and video frames . video_emb = get_video_embeddings(model, np.stack(VIDEOS, axis=0)) text_emb = get_text_embeddings(model, np.array(QUERIES)) . We combine both embeddings to calculate a similarity scores that represents the distance between the two embeddings: . scores = np.dot(text_emb, tf.transpose(video_emb)) . To display the search result we need a radom frame that will represent each video, for instance the we can take the first frame . def get_one_frame(video): return video[0] def process_frame(frame): return cv2.cvtColor((frame * 255.0).astype(&#39;uint8&#39;), cv2.COLOR_RGB2BGR) first_frames = [process_frame(get_one_frame(v)) for v in VIDEOS] . This is a helper function that we will use to annotate the frmes with their respective distance score to the query . def annotate_frame(in_frame, text): out_frame = in_frame.copy() cv2.putText(out_frame, text, (8, 15), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.6, color=(255, 255, 255), thickness=2) return out_frame . Finally, we can perform the search. In this case we take the score of the first query biking and sort the frames based on their distance to the embeddings of the text (the higher the closer) and we annotate the frames with their position in the result and the calculated score. . query_scores = scores[0] sorted_results = sorted(list(zip(first_frames, query_scores)), key=lambda p: p[-1], reverse=True) annotated_frames = [] for i, (f, s) in enumerate(sorted_results, start=1): frame = annotate_frame(f.copy(), f&#39;#{i} - Score: {s:.2f}&#39;) annotated_frames.append(frame) cv2_imshow(np.hstack(annotated_frames)) . Notice how well the model was able to choose the moutain biking video as the best match for the biking query . We can do this again with another query, for instance skateboarding . query_scores = scores[-1] sorted_results = sorted(list(zip(first_frames, query_scores)), key=lambda p: p[-1], reverse=True) annotated_frames = [] for i, (f, s) in enumerate(sorted_results, start=1): frame = annotate_frame(f.copy(), f&#39;#{i} - Score: {s:.2f}&#39;) annotated_frames.append(frame) cv2_imshow(np.hstack(annotated_frames)) . Notice how this time the model did not pick the dog video as we would expected. But also the three first videos have a very close score to the query nevertheless. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/video/search/2021/07/17/MIL_NCE_TFHub.html",
            "relUrl": "/tensorflow/video/search/2021/07/17/MIL_NCE_TFHub.html",
            "date": " • Jul 17, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Effortless Instance segmentation with TF Hub",
            "content": "Instance Segmentation is a computer vision task that tries to identify all objects in an image by detecting their bounding boxes and also precisely segment each instance into its correct class. For instance, for an image with many persons, Instance Segmentation will separates each person as a single entity. . One of the very performant models in this task is Mask-RCNN which was built on Faster R-CNN. In addition to outputing predictions of bounding box recognition, Mask-RCNN also predicts object mask in parallel. Furthermore, Mask R-CNN can also be repurposed easily for other taskse.g., allowing us to estimate human poses. . . In this article, we will leverage TensorFlow Hub to use an implementation of Mask-RCNN that was pre-trained on the COCO dataset and perform out-of-the-box instance segmentation on custom images. . First, let&#39;s install the TensorFlow Object Detection API so we could use its visualization tools to draw the bounding boxes and segmentation masks: . %%capture %%bash git clone --depth 1 https://github.com/tensorflow/models cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install . . Let&#39;s import all needed packages . import glob from io import BytesIO import matplotlib.pyplot as plt import numpy as np from tqdm import tqdm import tensorflow as tf import tensorflow_hub as hub from PIL import Image from object_detection.utils import ops from object_detection.utils import visualization_utils as viz from object_detection.utils.label_map_util import create_category_index_from_labelmap . %matplotlib inline . Then, we define a helper function load images from disk into a NumPy . def load_image(path): image_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(image_data)) width, height = image.size shape = (1, height, width, 3) image = np.array(image.getdata()) image = image.reshape(shape).astype(&#39;uint8&#39;) return image . Next, we define a helper function that takes the predictions of the Mask-RCNN model and extract for each detected instance the bounding box and mask . def process_predictions(predictions, image_height, image_width): # convert predictions to NumPy model_output = {k: v.numpy() for k, v in predictions.items()} # extact masks from predictions detection_masks = model_output[&#39;detection_masks&#39;][0] detection_masks = tf.convert_to_tensor(detection_masks) # extact bounding boxes from predictions detection_boxes = model_output[&#39;detection_boxes&#39;][0] detection_boxes = tf.convert_to_tensor(detection_boxes) # Reframe box masks into appropriate image masks detection_masks_reframed = ops.reframe_box_masks_to_image_masks(detection_masks, detection_boxes, image_height, image_width) detection_masks_reframed = tf.cast(detection_masks_reframed &gt; 0.5, tf.uint8) model_output[&#39;detection_masks_reframed&#39;] = detection_masks_reframed.numpy() # extract bounding boxes, scores, classes, and masks boxes = model_output[&#39;detection_boxes&#39;][0] classes = model_output[&#39;detection_classes&#39;][0].astype(&#39;int&#39;) scores = model_output[&#39;detection_scores&#39;][0] masks = model_output[&#39;detection_masks_reframed&#39;] return boxes, classes, scores, masks . We then use the extracted bounding boxes and masks to draw them on the original image . def visualize_predictions(image, boxes, classes, scores, masks, CATEGORY_IDX): &quot;&quot;&quot;Visualize detections and bounding boxes, scores, classes, and masks&quot;&quot;&quot; image_with_mask = image.copy()[0] viz.visualize_boxes_and_labels_on_image_array( image=image_with_mask, boxes=boxes, classes=classes, scores=scores, category_index=CATEGORY_IDX, use_normalized_coordinates=True, max_boxes_to_draw=200, min_score_thresh=0.30, agnostic_mode=False, instance_masks=masks, line_thickness=5 ) return image_with_mask . To be able to map an class ID to a label, we need to load the original mapping from the COCO dataset which is available inte TensorFlow models package . labels_path = &#39;models/research/object_detection/data/mscoco_label_map.pbtxt&#39; CATEGORY_IDX = create_category_index_from_labelmap(labels_path) . Now, let&#39;s download the Mask-RCNN model and its weights from TF Hub available here https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1 . MODEL_PATH = &#39;https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1&#39; mask_rcnn = hub.load(MODEL_PATH) . Get some images for testing . %%bash mkdir -p images curl -s -o images/bicycle1.jpg https://cdn.pixabay.com/photo/2016/11/30/12/29/bicycle-1872682_960_720.jpg curl -s -o images/bicycle2.jpg https://cdn.pixabay.com/photo/2016/11/22/23/49/cyclists-1851269_960_720.jpg curl -s -o images/animal1.jpg https://cdn.pixabay.com/photo/2014/05/20/21/20/bird-349026_960_720.jpg curl -s -o images/animal2.jpg https://cdn.pixabay.com/photo/2018/05/27/18/19/sparrows-3434123_960_720.jpg curl -s -o images/car1.jpg https://cdn.pixabay.com/photo/2016/02/13/13/11/oldtimer-1197800_960_720.jpg curl -s -o images/car2.jpg https://cdn.pixabay.com/photo/2016/09/11/10/02/renault-juvaquatre-1661009_960_720.jpg . Finally, we iterate over each test image, run it over the Mask-RCNN model and draw the bounding boxes and masks . images = [] for image_path in tqdm(glob.glob(&#39;images/*&#39;)): # load image in_image = load_image(image_path) # make predictions with Mask-RCNN predictions = mask_rcnn(in_image) # process predictions boxes, classes, scores, masks = process_predictions(predictions, in_image.shape[1], in_image.shape[2]) # visualize boxes and masks out_image = visualize_predictions(in_image, boxes, classes, scores, masks, CATEGORY_IDX) images.append(out_image) . 100%|██████████| 6/6 [06:01&lt;00:00, 60.24s/it] . . Note: this may take some time especially calculating the predicitons by Mask-RCNN . Finally, we can display the output images and admire the performance of the Mask-RCNN . figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, image in enumerate(images): row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].axis(&#39;off&#39;) plt.tight_layout() plt.show() . As you can see from the predicted masks and bounding boxes, the model was able to do a good on our random images. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/segmentation/2021/06/28/Mask_RCNN_TFHub.html",
            "relUrl": "/tensorflow/vision/segmentation/2021/06/28/Mask_RCNN_TFHub.html",
            "date": " • Jun 28, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Image segmentation with U-Net and transfer learning",
            "content": "In this article, we will implement a U-Net model (as depicted in the diagram below) and trained on a popular image segmentation dataset. Training a U-Net from scratch is a hard, so instead we will leverage transfer learning to get good result after only few epochs of training. . For reference, you can read the original U-Net paper arxiv.org. . . Before start, import the needed dependencies . import cv2 import matplotlib.pyplot as plt import numpy as np from tqdm import tqdm import tensorflow as tf import tensorflow_datasets as tfdata from tensorflow.keras.applications import MobileNetV2 from tensorflow.keras.layers import * from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.keras.models import * from tensorflow.keras.optimizers import RMSprop . Data . We will use a simple segmentation dataset known as Oxford-IIIT Pet Dataset. For quick introduction the dataset contains images of dogs or cats along with a segmentation image. Each pixel of the segmentation belongs to one of the following classes: . 1: The pixel belongs to a pet (i.e. cat or dog). | 2: The pixel belongs to the contour of a pet. | 3: The pixel belongs to the surroundings. | . For detailed introduction to the dataset check its website here. . The dataset along with its metadata is available in tensorflow-datasets here with images already preprocessed and ready to use with the TensorFlow Data API. Let&#39;s download this dataset and cache it locally . dataset, info = tfdata.load(&#39;oxford_iiit_pet&#39;, with_info=True) . Downloading and preparing dataset oxford_iiit_pet/3.2.0 (download: 773.52 MiB, generated: 774.69 MiB, total: 1.51 GiB) to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0... Shuffling and writing examples to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0.incomplete3ZMH6C/oxford_iiit_pet-train.tfrecord Shuffling and writing examples to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0.incomplete3ZMH6C/oxford_iiit_pet-test.tfrecord Dataset oxford_iiit_pet downloaded and prepared to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0. Subsequent calls will reuse this data. . Let&#39;s examine few images and the ground truth labels from the dataset . figure, axis = plt.subplots(2, 2, figsize=(10, 8)) for row, example in enumerate(dataset[&#39;train&#39;].take(2)): file_name = example[&#39;file_name&#39;].numpy().decode(&#39;utf8&#39;) axis[row, 0].set_title(file_name) axis[row, 0].imshow(example[&#39;image&#39;].numpy()) axis[row, 0].axis(&#39;off&#39;) mask = np.squeeze(example[&#39;segmentation_mask&#39;].numpy(), axis=2) axis[row, 1].set_title(&#39;mask&#39;) axis[row, 1].imshow(mask, cmap=&#39;gray&#39;) axis[row, 1].axis(&#39;off&#39;) . Model . As you can see in from the U-Net model architecture, the model consists of an Encoder depicted by a contracting path (left side) and a Decoder depicted by an expansive path (right side). From the left side, some skip connections are passed to the right side in order to improve the performance of the decoding. . Because the Encoder is very similar to a convolutional network, instead of creating this part and training it form scratch we will use a pretrained model and just select the output layers with appropriate shape to make skip connection to the Decoder. This way, we can train the model faster as we will have only the upsaming path to train. . First, let&#39;s define a function that creates the Encoder. We will use a pretrained MobileNetV2 on ImageNet as backbone for the Encoder. The output of the Encoder consits of few cherry picked layers that we will use later to create the skip connection from this Encoder to the Decoder part of the model. . def create_down_path(input_size=(256, 256, 3)): &quot;&quot;&quot;Create down path of U-Net model&quot;&quot;&quot; backbone = MobileNetV2(input_shape=input_size, include_top=False, weights=&#39;imagenet&#39;) target_layers = [ &#39;block_1_expand_relu&#39;, &#39;block_3_expand_relu&#39;, &#39;block_6_expand_relu&#39;, &#39;block_13_expand_relu&#39;, &#39;block_16_project&#39; ] layers = [backbone.get_layer(l).output for l in target_layers] encoder = Model(inputs=backbone.input, outputs=layers) encoder.trainable = False return encoder . . Note: with model.trainable = False we are freezing the weights of the Encoder as we don&#8217;t want to train it as it all the used layers were already trained on ImageNet. . Second, we define a function that creates the up-sampling path which consists of a sequence of blocks that uses the Conv2DTranspose layer. . def create_up_path(size=4, dropout=False): decoder = [] init = tf.random_normal_initializer(0.0, 0.02) for filters in (512, 256, 128, 64): block = Sequential() block.add(Conv2DTranspose(filters=filters, kernel_size=size, strides=2, padding=&#39;same&#39;, kernel_initializer=init, use_bias=False)) block.add(BatchNormalization()) if dropout: block.add(Dropout(rate=0.5)) block.add(ReLU()) decoder.append(block) return decoder . Finally, we use the previous functions to create the Encoder and Decoder components, and wire them with the skip connections. . def create_unet(input_size=(256, 256, 3)): down_stack = create_down_path(input_size) up_stack = create_up_path() # create skip connections inputs = Input(shape=input_size) x = inputs skip_layers = down_stack(x) x = skip_layers[-1] skip_layers = reversed(skip_layers[:-1]) for up, skip_connection in zip(up_stack, skip_layers): x = up(x) x = Concatenate()([x, skip_connection]) # output layer init = tf.random_normal_initializer(0.0, 0.02) output = Conv2DTranspose(filters=3, kernel_size=3, strides=2, padding=&#39;same&#39;, kernel_initializer=init)(x) return Model(inputs, outputs=output) . . Note: The intput of the U-Net model has same dimension as the input image, and as many channels as the number of segmentation classes (because each pixel can be categorized into one of 3 classes) . Now we can create the model and inspect it, notice how the output layers from the Encoder are used by the different blocks of the decoder. . unet_model = create_unet() . WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default. . WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default. . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5 9412608/9406464 [==============================] - 0s 0us/step 9420800/9406464 [==============================] - 0s 0us/step . tf.keras.utils.plot_model(unet_model, show_shapes=True) . Let&#39;s compile the model to get ready for training. We will use RMSProp as optimizer and SparseCategoricalCrossentropy as the loss function. . unet_model.compile(optimizer=RMSprop(), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) . Training . We need to normalize the images so they match the expected input of the pretrained MobileNetV2 model . def normalize(input_image, input_mask): input_image = tf.cast(input_image, tf.float32) / 255.0 input_mask -= 1 return input_image, input_mask . Let&#39;s define a function that will be used by the TF Dataset to load images and their labels, normalize the images and perform some basic augmentation (flip the image and mask) only for training. . @tf.function def load_image_fn(example, train=True): input_image = tf.image.resize(example[&#39;image&#39;],(256, 256)) input_mask = tf.image.resize(example[&#39;segmentation_mask&#39;], (256,256)) if train and np.random.uniform() &gt; 0.5: input_image = tf.image.flip_left_right(input_image) input_mask = tf.image.flip_left_right(input_mask) input_image, input_mask = normalize(input_image, input_mask) return input_image, input_mask . Let&#39;s define some training parameters like batch size, steps per epochs, etc. . TRAIN_SIZE = info.splits[&#39;train&#39;].num_examples VALIDATION_SIZE = info.splits[&#39;test&#39;].num_examples BATCH_SIZE = 64 STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE VALIDATION_SUBSPLITS = 5 VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE VALIDATION_STEPS //= VALIDATION_SUBSPLITS BUFFER_SIZE = 1000 . Now we create the training and test datasets and use the previously defined functions to load the images and labels. We also batch and perfom a pre-fetch for the training dataset . train_dataset = dataset[&#39;train&#39;] .map(load_image_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE) .cache() .shuffle(BUFFER_SIZE) .batch(BATCH_SIZE) .repeat() .prefetch(buffer_size=tf.data.experimental.AUTOTUNE) test_dataset = dataset[&#39;test&#39;] .map(lambda d: load_image_fn(d, train=False), num_parallel_calls=tf.data.experimental.AUTOTUNE) .batch(BATCH_SIZE) . Now we can start the training of our U-Net model . hist = unet_model.fit(train_dataset, epochs=10, steps_per_epoch=STEPS_PER_EPOCH, validation_steps=VALIDATION_STEPS, validation_data=test_dataset) . Epoch 1/10 57/57 [==============================] - 53s 582ms/step - loss: 0.4203 - accuracy: 0.8352 - val_loss: 2.0189 - val_accuracy: 0.6889 Epoch 2/10 57/57 [==============================] - 34s 571ms/step - loss: 0.2736 - accuracy: 0.8894 - val_loss: 0.4313 - val_accuracy: 0.8765 Epoch 3/10 57/57 [==============================] - 33s 576ms/step - loss: 0.2554 - accuracy: 0.8945 - val_loss: 0.2991 - val_accuracy: 0.8937 Epoch 4/10 57/57 [==============================] - 33s 585ms/step - loss: 0.2449 - accuracy: 0.8978 - val_loss: 0.2617 - val_accuracy: 0.9006 Epoch 5/10 57/57 [==============================] - 34s 592ms/step - loss: 0.2354 - accuracy: 0.9008 - val_loss: 0.3233 - val_accuracy: 0.8708 Epoch 6/10 57/57 [==============================] - 34s 599ms/step - loss: 0.2268 - accuracy: 0.9042 - val_loss: 0.2499 - val_accuracy: 0.9042 Epoch 7/10 57/57 [==============================] - 34s 606ms/step - loss: 0.2244 - accuracy: 0.9045 - val_loss: 0.2453 - val_accuracy: 0.9022 Epoch 8/10 57/57 [==============================] - 35s 607ms/step - loss: 0.2125 - accuracy: 0.9092 - val_loss: 0.2443 - val_accuracy: 0.9057 Epoch 9/10 57/57 [==============================] - 34s 606ms/step - loss: 0.2124 - accuracy: 0.9089 - val_loss: 0.2930 - val_accuracy: 0.8884 Epoch 10/10 57/57 [==============================] - 34s 606ms/step - loss: 0.2080 - accuracy: 0.9102 - val_loss: 0.2561 - val_accuracy: 0.9016 . MobileNets have many different input size configurations 128, 160, 192 and 224, in our case we are sizing the images into 256 which is not supported hence the warnings in the output. Thre is nothing to worry about the model is trained properly and validation is performed. . Now we can examine the losses and accuracy progress per epoch . figure, axis = plt.subplots(1, 2, figsize=(15, 5)) # accuracy axis[0].plot(hist.history[&#39;accuracy&#39;]) axis[0].plot(hist.history[&#39;val_accuracy&#39;]) axis[0].set_title(&#39;model accuracy&#39;) axis[0].set_ylabel(&#39;accuracy&#39;) axis[0].set_xlabel(&#39;epoch&#39;) axis[0].legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;upper left&#39;) # loss axis[1].plot(hist.history[&#39;loss&#39;]) axis[1].plot(hist.history[&#39;val_loss&#39;]) axis[1].set_title(&#39;model loss&#39;) axis[1].set_ylabel(&#39;loss&#39;) axis[1].set_xlabel(&#39;epoch&#39;) axis[1].legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;upper left&#39;) plt.show() . Evaluation . We can evaluate our U-Net model on the entire test dataset simply as follows . result = unet_model.evaluate(test_dataset) . 58/58 [==============================] - 16s 277ms/step - loss: 0.2512 - accuracy: 0.9024 Accuracy: 90.24% . print(f&#39;Test Accuracy: {result[1] * 100:.2f}%&#39;) . Test Accuracy: 90.24% . For a visual examination and an inspection of the precited segmentation, we need some helper function to transform the output of the trained U-Net model into an actual valid mask, hence the following functions . def process_mask(mask): mask = (mask.numpy() * 127.5).astype(&#39;uint8&#39;) mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB) return mask def create_mask(prediction_mask): prediction_mask = tf.argmax(prediction_mask, axis=-1) prediction_mask = prediction_mask[..., tf.newaxis] return prediction_mask[0] . From the test dataset we take sample images and pass them through the U-Net model to generate predictions that later we will compare against the ground truth masks. . def generate_predictions(model, dataset, sample_size=2): output = [] for image, mask in tqdm(dataset.take(sample_size)): output_mask = model.predict(image) predicted_mask = process_mask(create_mask(output_mask)) image = (image[0].numpy() * 255.0).astype(&#39;uint8&#39;) ground_truth_mask = process_mask(mask[0]) output.append((image, ground_truth_mask, predicted_mask)) return output . The following helper function will be used to plot a set of predictions . def plot_predictions(predictions): figure, axis = plt.subplots(len(predictions), 3, figsize=(10, 20)) for row, (image, ground_truth_mask, predicted_mask) in enumerate(predictions): # plot the image axis[row, 0].imshow(image) axis[row, 0].axis(&#39;off&#39;) axis[row, 0].set_title(&#39;image&#39;) # plot the ground truth mask axis[row, 1].imshow(ground_truth_mask) axis[row, 1].axis(&#39;off&#39;) axis[row, 1].set_title(&#39;ground truth mask&#39;) # plot the predicted mask axis[row, 2].imshow(predicted_mask) axis[row, 2].axis(&#39;off&#39;) axis[row, 2].set_title(&#39;predicted mask&#39;) plt.tight_layout() plt.show() . Now we can generate predicted masks for a sample of the images in the test set and plot them for inspection . predictions = generate_predictions(unet_model, test_dataset, 5) . 100%|██████████| 5/5 [00:03&lt;00:00, 1.33it/s] . plot_predictions(predictions) . As you can see from the predicted masks, the model was able to do a good job even if it was trained on very few epochs. It did very well on the easy example but could not do a good job in the harder exmaples. For instance on darker images or when the image contains more than just a pet. . I put lot efforts in writting every article, I hope you found this one useful as well as easy to digest. . Feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/segmentation/2021/06/13/U_Net_transfer_learning.html",
            "relUrl": "/tensorflow/vision/segmentation/2021/06/13/U_Net_transfer_learning.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Image Captioning with Attention",
            "content": ". In this article, we will implement a more complete image captioning system on the Flickr8k dataset. We will use an attention mechanism to give the model the power to search for parts of the source caption that are relevant to predict the best next word. Also, using attention will allow us to understand in an intuitive way where the network looks to produce captions. . Let&#39;s start by importing the needed dependencies: . import pandas as pd import numpy as np import json import os import time from string import punctuation from pathlib import Path import matplotlib.pyplot as plt from tqdm import tqdm from sklearn.model_selection import train_test_split from sklearn.utils import shuffle import tensorflow as tf from tensorflow.keras.applications.inception_v3 import * from tensorflow.keras.layers import * from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.utils import get_file . Set a seed for reproducibility . SEED = 31 . Data . We will use the Flickr8k dataset (availble on Kaggle here). So we wil install Kaggle CLI and make sure the credentials are properly configured. . %%capture %%bash pip install kaggle --upgrade mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;dzlabs&quot;,&quot;key&quot;:&quot;0abda977ffcfb11ea3726d7c0a6a802e&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json . . Note: you need to replace KAGGLE_USER with your actual Kaggle username and KAGGLE_KEY with your API key for the download to work. . Download the dataset, unzip the files into, and create a proper folder structure . %%capture %%bash kaggle datasets download adityajn105/flickr8k mkdir -p flickr8k unzip flickr8k.zip -d flickr8k mkdir -p flickr8k/features . Now we can set variables with the paths to the images and annotations . BASE_PATH = &#39;flickr8k&#39; IMAGES_PATH = f&#39;{BASE_PATH}/Images&#39; FEATURES_PATH = f&#39;{BASE_PATH}/features&#39; CAPTIONS_PATH = f&#39;{BASE_PATH}/captions.txt&#39; . Let&#39;s read the captions file into a Pandas dataframe and have look to it . captions_df = pd.read_csv(CAPTIONS_PATH) # captions_df = captions_df.groupby(&#39;image&#39;).first().reset_index() captions_df.head() . image caption . 0 1000268201_693b08cb0e.jpg | A child in a pink dress is climbing up a set o... | . 1 1000268201_693b08cb0e.jpg | A girl going into a wooden building . | . 2 1000268201_693b08cb0e.jpg | A little girl climbing into a wooden playhouse . | . 3 1000268201_693b08cb0e.jpg | A little girl climbing the stairs to her playh... | . 4 1000268201_693b08cb0e.jpg | A little girl in a pink dress going into a woo... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; With the images and captions loaded, we can take a random sample and dispaly some images with their respective caption . samples = captions_df.sample(6).reset_index() figure, axis = plt.subplots(2, 3, figsize=(18, 8)) for index, sample in samples.iterrows(): image = plt.imread(f&#39;{IMAGES_PATH}/{sample[&quot;image&quot;]}&#39;) title = sample[&#39;caption&#39;][:50] + &#39; n&#39; + sample[&#39;caption&#39;][50:] row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].set_title(title) axis[row, col].axis(&#39;off&#39;) . We need to clean the text in captions (e.g. removing punctuation) to simplify training, and also adding special tokens &lt;sos&gt; (start of sequence) token to be added at the begning of the text, and &lt;eos&gt; (end of sequence) token added at the end. . def clean_caption(caption, start_token=&#39;&lt;sos&gt;&#39;, end_token=&#39;&lt;eos&gt;&#39;): def remove_punctuation(word): translation = str.maketrans(&#39;&#39;, &#39;&#39;, punctuation) return word.translate(translation) def is_valid_word(word): return len(word) &gt; 1 and word.isalpha() caption = caption.lower().split(&#39; &#39;) caption = map(remove_punctuation, caption) caption = filter(is_valid_word, caption) cleaned_caption = f&#39;{start_token} {&quot; &quot;.join(caption)} {end_token}&#39; return cleaned_caption . Now we apply the cleaning function and update all captions text . captions_df[&#39;caption&#39;] = captions_df.caption.apply(lambda x: clean_caption(x)) . Here we define a helper function that we will use later to get the maximum sequence length . def get_max_length(tensor): return max(len(t) for t in tensor) . Model . The model that we will be using for caption as illustrated by the diagram below is composed of three smaller models: . Image feature extraction which is simply a CNN previously trained on image classification but without the classification head. The weights of this model are non-trainable. | CNN Encoder which takes the image features and produces an embedding that will be learded as the model is trained | RNN Decoder with Attention which will uses the image embedding as well as the hidden state propagated as the decoder process the caption tokens | . . The first part of the model is the CNN Encoder that will train an dense layer as it processes images feature vector and generates what will be used by the Decoder attention layer . class CNNEncoder(Model): def __init__(self, embedding_dim): super(CNNEncoder, self).__init__() self.fc = Dense(embedding_dim) def call(self, x): x = self.fc(x) x = tf.nn.relu(x) return x . The next component of our model is based on Bahdanau&#39;s Attention which was a break throught when it was first introduced as an improvemrnt to Encoder-Decoder models. It tries to address the problem that encoder faces as they try to squash information extracted from very long sequences. . To learn more about this algorithm can read the original paper arxiv.org or read a detailed explanation on machinelearningmastery.com . . class BahdanauAttention(Model): def __init__(self, units): super(BahdanauAttention, self).__init__() self.W1 = Dense(units) self.W2 = Dense(units) self.V = Dense(1) def call(self, features, hidden): hidden_with_time_axis = tf.expand_dims(hidden, 1) score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) attention_w = tf.nn.softmax(self.V(score), axis=1) ctx_vector = attention_w * features ctx_vector = tf.reduce_sum(ctx_vector, axis=1) return ctx_vector, attention_w . Next, we define the decoder whichi is an RNN that uses GRU and attention to learn how to produce captions from the text input sequences and the visual feature vectors extracted from the input images: . class RNNDecoder(Model): def __init__(self, embedding_size, units, vocab_size): super(RNNDecoder, self).__init__() self.units = units self.embedding = Embedding(vocab_size, embedding_size) self.gru = GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer=&#39;glorot_uniform&#39;) self.fc1 = Dense(self.units) self.fc2 = Dense(vocab_size) self.attention = BahdanauAttention(self.units) def call(self, x, features, hidden): # calcualte the attention context_vector, attention_weights = self.attention(features, hidden) # calculate the embeddings of the input token x = self.embedding(x) expanded_context = tf.expand_dims(context_vector, 1) x = Concatenate(axis=-1)([expanded_context, x]) # pass context vector and input embedding through GRU output, state = self.gru(x) x = self.fc1(output) x = tf.reshape(x, (-1, x.shape[2])) x = self.fc2(x) return x, state, attention_weights def reset_state(self, batch_size): return tf.zeros((batch_size, self.units)) . Finally, we put the different pieces of model togher in the Trainer Class. It will create the encoder, the decoder, the tokenizer, and the optimizer and loss functions needed to train the whole system. This class also defines a function to perform a single training step as well as the training on many epoches. During the training, the training loss will be recorded which later can be visualized with TensorBoard. . class Trainer(object): def __init__(self, embedding_size, units, vocab_size, tokenizer): self.tokenizer = tokenizer self.encoder = CNNEncoder(embedding_size) self.decoder = RNNDecoder(embedding_size, units, vocab_size) self.optimizer = Adam() self.loss = SparseCategoricalCrossentropy(from_logits=True, reduction=&#39;none&#39;) def loss_function(self, real, predicted): &quot;&quot;&quot;Calculate the loss based on the ground truth caption and the predicted one&quot;&quot;&quot; mask = tf.math.logical_not(tf.math.equal(real,0)) _loss = self.loss(real, predicted) mask = tf.cast(mask, dtype=_loss.dtype) _loss *= mask return tf.reduce_mean(_loss) @tf.function def train_step(self, image_tensor, target): &quot;&quot;&quot;Perform one training step&quot;&quot;&quot; loss = 0 hidden = self.decoder.reset_state(target.shape[0]) start_token_idx = self.tokenizer.word_index[&#39;&lt;sos&gt;&#39;] init_batch = [start_token_idx] * target.shape[0] decoder_input = tf.expand_dims(init_batch, 1) with tf.GradientTape() as tape: features = self.encoder(image_tensor) for i in range(1, target.shape[1]): preds, hidden, _ = self.decoder(decoder_input, features, hidden) loss += self.loss_function(target[:, i], preds) decoder_input = tf.expand_dims(target[:, i],1) total_loss = loss / int(target.shape[1]) trainable_vars = (self.encoder.trainable_variables + self.decoder.trainable_variables) gradients = tape.gradient(loss, trainable_vars) self.optimizer.apply_gradients(zip(gradients,trainable_vars)) return loss, total_loss def train(self, dataset, epochs, num_steps): &quot;&quot;&quot;Train and log metrics&quot;&quot;&quot; writer = tf.summary.create_file_writer(&#39;log_dir&#39;) for epoch in tqdm(range(epochs)): start = time.time() total_loss = 0 for batch, (image_tensor, target) in enumerate(dataset): batch_loss, step_loss = self.train_step(image_tensor, target) total_loss += step_loss epoch_time = time.time() - start # write the loss value with writer.as_default(): tf.summary.scalar(&#39;training loss&#39;, total_loss / num_steps, step=epoch+1) tf.summary.scalar(&#39;Epoch time (s)&#39;, epoch_time, step=epoch+1) . Training . Before we can start the training, we need to perform some data pre-process. Let&#39;s first get an array of image paths and the corresponding captions . train_images = captions_df.image.apply(lambda image: f&#39;{IMAGES_PATH}/{image}&#39;).values train_captions = captions_df.caption.values . We need to download a pre-trained instance of Inception V3 with the Imagenet dataset and used as the image feature extractor after removing the model classification head . feature_extractor = InceptionV3(include_top=False, weights=&#39;imagenet&#39;) feature_extractor = Model(feature_extractor.input, feature_extractor.layers[-1].output) . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 87916544/87910968 [==============================] - 1s 0us/step 87924736/87910968 [==============================] - 1s 0us/step . We need to define a function that will load images based on their path and resize them as expected by the feature extractor model . def load_image_fn(image_path): image = tf.io.read_file(image_path) image = tf.image.decode_jpeg(image, channels=3) image = tf.image.resize(image, (299, 299)) image = preprocess_input(image) return image, image_path . We can use the previous funciton to Create a tf.data.Dataset of the images . BATCH_SIZE = 8 image_dataset = tf.data.Dataset.from_tensor_slices(train_images) .map(load_image_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE) .batch(BATCH_SIZE) . We iteratate over all the images in the dataset and pass them through the feature extractor. As the extracted feature vectors cannot fit in memory we store them under FEATURES_PATH folder . for image, path in tqdm(image_dataset): batch_features = feature_extractor.predict(image) batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3])) for batch_feature, p in zip(batch_features, path): feature_path = Path(p.numpy().decode(&#39;UTF-8&#39;)) image_name = feature_path.stem np.save(f&#39;{FEATURES_PATH}/{image_name}&#39;, batch_feature.numpy()) . 100%|██████████| 5057/5057 [11:51&lt;00:00, 7.11it/s] . We need to create tokens from the captions, hence we train a tokenizer on the top 5,000 words in our captions which will become our vocabulary. Note we could take more words but that lead to bigger memory footprint as we one hot encode each token. . After that, we apply this tokenizer to each caption text to generate a numeric sequence. We limit the sequence size and we pad any short caption by adding a sequence of the special &lt;pad&gt; token to the end. . top_k = 5000 filters = &#39;!”#$%&amp;()*+.,-/:;=?@[ ]^_`{|}~ &#39; tokenizer = Tokenizer(num_words=top_k, oov_token=&#39;&lt;unk&gt;&#39;, filters=filters) tokenizer.fit_on_texts(train_captions) tokenizer.word_index[&#39;&lt;pad&gt;&#39;] = 0 tokenizer.index_word[0] = &#39;&lt;pad&gt;&#39; train_seqs = tokenizer.texts_to_sequences(train_captions) captions_seqs = pad_sequences(train_seqs, padding=&#39;post&#39;) max_length = get_max_length(train_seqs) . captions_seqs.shape . (40455, 34) . Let&#39;s split the dataset into 80% for the actual training and 20% for later evaluating the trained model . (images_train, images_val, caption_train, caption_val) = train_test_split(train_images, captions_seqs, test_size=0.2, random_state=SEED) . We need a function that will load an image feature vector and the associated caption . def load_example_fn(image_name, caption): image_name = image_name.decode(&#39;utf-8&#39;) image_name = Path(image_name).stem image_tensor = np.load(f&#39;{FEATURES_PATH}/{image_name}.npy&#39;) return image_tensor, caption . To create the trainig dataset, we batch the images with their captions into batches of BATCH_SIZE example. We use the load_example_fn load the feature vectors. For performance, we suffle the dataset and pre-fetch some of them into the GPU to speedup training. . BATCH_SIZE = 64 BUFFER_SIZE = 1000 dataset = tf.data.Dataset.from_tensor_slices((images_train, caption_train)) .map(lambda i1, i2: tf.numpy_function(load_example_fn, [i1, i2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE) .shuffle(BUFFER_SIZE) .batch(BATCH_SIZE) .prefetch(buffer_size=tf.data.experimental.AUTOTUNE) . Now, we are ready for the actually training. Let&#39;s create the helper Trainer and set the token embeddings size to 256 elements, and the number of units for the decoder as well as the attention model to 512. We also pass the tokenizer and the vocabulary size is 5000 + 1 (for the padding token). . trainer = Trainer(embedding_size=256, units=512, vocab_size=top_k + 1, tokenizer=tokenizer) . Now we train for few epochs (note this may take quite some time to finish) . EPOCHS = 30 num_steps = len(images_train) // BATCH_SIZE trainer.train(dataset, EPOCHS, num_steps) . 100%|██████████| 30/30 [2:00:15&lt;00:00, 240.52s/it] . Now we can explore the training loss with . %load_ext tensorboard %tensorboard --logdir log_dir . . Evaluation . For the evaluation, we will define a function that takes the image feature extractor and the trained encoder, decoder. It will transform the image and generate a caption starting with the &lt;sos&gt; spectial token. The caption generation stops when and &lt;eos&gt; token is generated by the decoder or the maximum length of caption tokens is reached. . def evaluate(encoder, decoder, tokenizer, image_path, max_length, attention_shape): attention_plot = np.zeros((max_length, attention_shape)) # initialize hidden state hidden = decoder.reset_state(batch_size=1) image, _ = load_image_fn(image_path) # extarct image feature vector features = feature_extractor(tf.expand_dims(image, 0)) features = tf.reshape(features, (features.shape[0], -1, features.shape[3])) # encode the features encoder_out = encoder(features) start_token_idx = tokenizer.word_index[&#39;&lt;sos&gt;&#39;] decoder_input = tf.expand_dims([start_token_idx], 0) result = [] # generate the caption for i in range(max_length): (preds, hidden, attention_w) = decoder(decoder_input, encoder_out, hidden) attention_plot[i] = tf.reshape(attention_w, (-1,)).numpy() pred_id = tf.random.categorical(preds, 1)[0][0].numpy() result.append(tokenizer.index_word[pred_id]) if tokenizer.index_word[pred_id] == &#39;&lt;eos&gt;&#39;: return result, attention_plot decoder_input = tf.expand_dims([pred_id], 0) attention_plot = attention_plot[:len(result), :] return result, attention_plot . . Note: see how for each token we generate an attention plot and added to the final attention_plot. . attention_features_shape = 64 . Pick a random image from the evaluation dataset that we saved earlier . random_id = np.random.randint(0, len(images_val)) image_path = images_val[random_id] . Get and clean the actual image caption . actual_caption = &#39; &#39;.join([tokenizer.index_word[i] for i in caption_val[random_id] if i != 0]) actual_caption = (actual_caption.replace(&#39;&lt;sos&gt;&#39;, &#39;&#39;).replace(&#39;&lt;eos&gt;&#39;, &#39;&#39;)) . Generate a caption for the image and clean it from special tokens . result, attention_plot = evaluate(trainer.encoder, trainer.decoder, tokenizer, image_path, max_length, attention_features_shape) predicted_caption = (&#39; &#39;.join(result).replace(&#39;&lt;sos&gt;&#39;, &#39;&#39;).replace(&#39;&lt;eos&gt;&#39;, &#39;&#39;)) . Let&#39;s show side by side the image with its original caption, and next to it two images overlayed with some of the attention plots (there is one attention plot per output token in the caption) and the predicted caption . figure, axis = plt.subplots(1, 3, figsize=(15, 8)) axis[0].imshow(plt.imread(image_path)) axis[0].set_title(actual_caption) axis[0].axis(&#39;off&#39;) imageshow = axis[1].imshow(plt.imread(image_path)) axis[1].imshow(np.resize(attention_plot[0], (8, 8)), cmap=&#39;gray&#39;, alpha=0.6, extent=imageshow.get_extent()) axis[1].set_title(predicted_caption) axis[1].axis(&#39;off&#39;) imageshow = axis[2].imshow(plt.imread(image_path)) axis[2].imshow(np.resize(attention_plot[len(attention_plot)-1], (8, 8)), cmap=&#39;gray&#39;, alpha=0.6, extent=imageshow.get_extent()) axis[2].set_title(predicted_caption) axis[2].axis(&#39;off&#39;) plt.show() . Notice how well the model performed and generated a caption that&#39;s close to the actual ground truth . We can display all attention plots and inspect what the model was looking into when it generated the corresponding token. For this, let&#39;s define a function that will receive an image, the caption as sequence of tokens, and the attention_plot returned by the previous evualtion function. . def plot_attention(image_path, result, attention_plot, output_path): image_array = plt.imread(image_path) fig = plt.figure(figsize=(10, 10)) # for each token create a sub-plot and display the corresponding attention for l in range(len(result)): temp_att = np.resize(attention_plot[l], (8, 8)) ax = fig.add_subplot(len(result) // 2, len(result) // 2, l + 1) ax.set_title(result[l]) image = ax.imshow(image_array) ax.imshow(temp_att, cmap=&#39;gray&#39;, alpha=0.6, extent=image.get_extent()) # save the attention plot plt.savefig(output_path, format=&#39;png&#39;) plt.tight_layout() plt.show() . plot_attention(image_path, result, attention_plot, &#39;./attention_plot.png&#39;) . . Note: the square areas in the plots represent the areas of the picture the model paid more attention to when generate the tokens. For instance, to produce the word women, the network looked at the head of women in the photo. Also, we can see that when the network generated the word beside as it looked at the chair. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/captioning/2021/06/01/Image_Captioning_with_Attention.html",
            "relUrl": "/tensorflow/vision/captioning/2021/06/01/Image_Captioning_with_Attention.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Super-Resolution with TensorFlow",
            "content": "Single image super-resolution (SR) is a classical computer vision problem that aims at recovering a high-resolution image from a lower resolution image. Extensive research was conduct in this area and with the advance of Deep Learning great results have been achieved. . In this post, we will examine one of the Deep Learning approaches to super-resolution called Super-Resolution Convolutional Neural Network (SRCNN). This technique work end to end by extacting patches from the low resolution image and passing them throw convolutional layers to final map them to higher resolution output pixels, as depicted in the diagram below. . We will implement the SRCNN model in TensorFlow, train it and then test it on a low resolution image. . . As an image dataset for training the model, we will be using a Kaggle hosted dataset called Dog and Cat Detection. We will use Kaggle CLI to download this dataset and you need to get your Kaggle API key, alternatively you can manually download the dataset directly from the website. . %%capture %%bash pip install -q kaggle mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;KAGGLE_USERNAME&quot;,&quot;key&quot;:&quot;KAGGLE_KEY&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json . %%capture %%bash kaggle datasets download andrewmvd/dog-and-cat-detection unzip -q dog-and-cat-detection.zip . To save model checkpoint and other artifacts, we will mount Google Drive the this colab container . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . Import dependencies . import os import pathlib from glob import glob from tqdm import tqdm import matplotlib.pyplot as plt import numpy as np from PIL import Image import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.layers import * from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.image import * . Set a random seed for reproducibility . SEED = 31 np.random.seed(SEED) . Data . We need a function to resize images based on a scale factor, this will be used later in the process to generate low resolution images from a given image . def resize_image(image_array, factor): original_image = Image.fromarray(image_array) new_size = np.array(original_image.size) * factor new_size = new_size.astype(np.int32) new_size = tuple(new_size) resized = original_image.resize(new_size) resized = img_to_array(resized) resized = resized.astype(np.uint8) return resized . This function will use the resizing to generate low resolution images by downsizing then upsizing: . def downsize_upsize_image(image, scale): scaled = resize_image(image, 1.0 / scale) scaled = resize_image(scaled, scale / 1.0) return scaled . When we will extract patches, we will slide a window over the original image, and for the image to fit nicely we need to crop it with the following function . def tight_crop_image(image, scale): height, width = image.shape[:2] width -= int(width % scale) height -= int(height % scale) return image[:height, :width] . The following function is used to extract patches with a sliding window from an input image. The INPUT_DIM parameter is the height and width of the images as expected by the network . def crop_input(image, x, y): y_slice = slice(y, y + INPUT_DIM) x_slice = slice(x, x + INPUT_DIM) return image[y_slice, x_slice] . Similarly, we need to crop patches from the output images with LABEL_SIZE the height and width of the output of the network. We also need to pad the patches with PAD to make sure we are cropping the regions properly . def crop_output(image, x, y): y_slice = slice(y + PAD, y + PAD + LABEL_SIZE) x_slice = slice(x + PAD, x + PAD + LABEL_SIZE) return image[y_slice, x_slice] . Now let&#39;s read all image paths . file_patten = (pathlib.Path(&#39;/content&#39;) / &#39;images&#39; / &#39;*.png&#39;) file_pattern = str(file_patten) dataset_paths = [*glob(file_pattern)] . We don&#39;t need the entire dataset as this will take longer training, but will sample around 1000 images from it . SUBSET_SIZE = 1000 dataset_paths = np.random.choice(dataset_paths, SUBSET_SIZE) . Here is an example image from the dataset . path = np.random.choice(dataset_paths) img = plt.imread(path) plt.imshow(img) . &lt;matplotlib.image.AxesImage at 0x7fb5cde796d0&gt; . Here we define some parameters, like the scale for resiping, input and output patch sizes, the amount of padding that need to be added to output patches, and the stride which is the number of pixels we&#39;ll slide both in the horizontal and vertical axes to extract patches. . SCALE = 2.0 INPUT_DIM = 33 LABEL_SIZE = 21 PAD = int((INPUT_DIM - LABEL_SIZE) / 2.0) STRIDE = 14 . Now, lets build the dataset by reading the input images, generating a low resolution version, sliding a window on this low resolution image as well as the original image to generate patches for training. We will save the patches to disk and later build a training data generator that will load them from disk in batches. . %%bash mkdir -p data mkdir -p training . for image_path in tqdm(dataset_paths): filename = pathlib.Path(image_path).stem image = load_img(image_path) image = img_to_array(image) image = image.astype(np.uint8) image = tight_crop_image(image, SCALE) scaled = downsize_upsize_image(image, SCALE) height, width = image.shape[:2] for y in range(0, height - INPUT_DIM + 1, STRIDE): for x in range(0, width - INPUT_DIM + 1, STRIDE): crop = crop_input(scaled, x, y) target = crop_output(image, x, y) np.save(f&#39;data/{filename}_{x}_{y}_input.np&#39;, crop) np.save(f&#39;data/{filename}_{x}_{y}_output.np&#39;, target) . 100%|██████████| 1500/1500 [18:00&lt;00:00, 1.39it/s] . We cannot hold all the patches in memory hence we saved to disk in the previous step. Now we need a dataset loader that will load a patch and its label and feed them to the network during traning in batches. This is achieved with the PatchesDataset class (check this example to learn more about generators - link). . class PatchesDataset(tf.keras.utils.Sequence): def __init__(self, batch_size, *args, **kwargs): self.batch_size = batch_size self.input = [*glob(&#39;data/*_input.np.npy&#39;)] self.output = [*glob(&#39;data/*_output.np.npy&#39;)] self.input.sort() self.output.sort() self.total_data = len(self.input) def __len__(self): # returns the number of batches return int(self.total_data / self.batch_size) def __getitem__(self, index): # returns one batch indices = self.random_indices() input = np.array([np.load(self.input[idx]) for idx in indices]) output = np.array([np.load(self.output[idx]) for idx in indices]) return input, output def random_indices(self): return np.random.choice(list(range(self.total_data)), self.batch_size, p=np.ones(self.total_data)/self.total_data) . Define a batch size based on how much memory available on your GPU and create an instance of the dataset generator. . BATCH_SIZE = 1024 train_ds = PatchesDataset(BATCH_SIZE) len(train_ds) . 888 . You can see the shape of the training batches . input, output = train_ds[0] input.shape, output.shape . ((1024, 33, 33, 3), (1024, 21, 21, 3)) . Model . The architecture of the SRCNN model is very simple, it has only convolutional layers, one to downsize the input and extract image features and a later one to upside to generate the output image. The following helper function is used to create an instance of the model. . def create_model(height, width, depth): input = Input(shape=(height, width, depth)) x = Conv2D(filters=64, kernel_size=(9, 9), kernel_initializer=&#39;he_normal&#39;)(input) x = ReLU()(x) x = Conv2D(filters=32, kernel_size=(1, 1), kernel_initializer=&#39;he_normal&#39;)(x) x = ReLU()(x) output = Conv2D(filters=depth, kernel_size=(5, 5), kernel_initializer=&#39;he_normal&#39;)(x) return Model(input, output) . To train the network we will use Adam as optimizer with learning rate decay. Also, as the problem we try to train the network for is a regression problem (we want predict the high resolution pixels) we pick MSE as a loss function, this will make the model learn the filters that correctly map patches from low to high resolution. . EPOCHS = 12 optimizer = Adam(learning_rate=1e-3, decay=1e-3 / EPOCHS) model = create_model(INPUT_DIM, INPUT_DIM, 3) model.compile(loss=&#39;mse&#39;, optimizer=optimizer) . You can see how the model is small but astonishly it will be able to achieve great results once trained for enough time, we will train it for 12 epochs . model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 33, 33, 3)] 0 conv2d (Conv2D) (None, 25, 25, 64) 15616 re_lu (ReLU) (None, 25, 25, 64) 0 conv2d_1 (Conv2D) (None, 25, 25, 32) 2080 re_lu_1 (ReLU) (None, 25, 25, 32) 0 conv2d_2 (Conv2D) (None, 21, 21, 3) 2403 ================================================================= Total params: 20,099 Trainable params: 20,099 Non-trainable params: 0 _________________________________________________________________ . tf.keras.utils.plot_model(model, show_shapes = True, rankdir=&#39;LR&#39;) . Create a callback that saves the model&#39;s weights . checkpoint_path = &quot;training/cp.ckpt&quot; checkpoint_dir = os.path.dirname(checkpoint_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1) . Now finally, we can train the network . model.fit(train_ds, epochs=EPOCHS, callbacks=[cp_callback]) . Epoch 1/12 888/888 [==============================] - ETA: 0s - loss: 258.0937 Epoch 00001: saving model to training/cp.ckpt 888/888 [==============================] - 1735s 2s/step - loss: 258.0937 Epoch 2/12 888/888 [==============================] - ETA: 0s - loss: 105.9775 Epoch 00002: saving model to training/cp.ckpt 888/888 [==============================] - 1428s 2s/step - loss: 105.9775 Epoch 3/12 888/888 [==============================] - ETA: 0s - loss: 102.4195 Epoch 00003: saving model to training/cp.ckpt 888/888 [==============================] - 1364s 2s/step - loss: 102.4195 Epoch 4/12 888/888 [==============================] - ETA: 0s - loss: 98.4859 Epoch 00004: saving model to training/cp.ckpt 888/888 [==============================] - 1347s 2s/step - loss: 98.4859 Epoch 5/12 888/888 [==============================] - ETA: 0s - loss: 97.5308 Epoch 00005: saving model to training/cp.ckpt 888/888 [==============================] - 1352s 2s/step - loss: 97.5308 Epoch 6/12 888/888 [==============================] - ETA: 0s - loss: 96.0889 Epoch 00006: saving model to training/cp.ckpt 888/888 [==============================] - 1347s 2s/step - loss: 96.0889 Epoch 7/12 888/888 [==============================] - ETA: 0s - loss: 94.7550 Epoch 00007: saving model to training/cp.ckpt 888/888 [==============================] - 1355s 2s/step - loss: 94.7550 Epoch 8/12 888/888 [==============================] - ETA: 0s - loss: 93.3618 Epoch 00008: saving model to training/cp.ckpt 888/888 [==============================] - 1332s 1s/step - loss: 93.3618 Epoch 9/12 888/888 [==============================] - ETA: 0s - loss: 93.5235 Epoch 00009: saving model to training/cp.ckpt 888/888 [==============================] - 1346s 2s/step - loss: 93.5235 Epoch 10/12 888/888 [==============================] - ETA: 0s - loss: 92.4781 Epoch 00010: saving model to training/cp.ckpt 888/888 [==============================] - 1356s 2s/step - loss: 92.4781 Epoch 11/12 888/888 [==============================] - ETA: 0s - loss: 91.5945 Epoch 00011: saving model to training/cp.ckpt 888/888 [==============================] - 1348s 2s/step - loss: 91.5945 Epoch 12/12 888/888 [==============================] - ETA: 0s - loss: 91.0127 Epoch 00012: saving model to training/cp.ckpt 888/888 [==============================] - 1336s 2s/step - loss: 91.0127 . &lt;keras.callbacks.History at 0x7fb5c9edecd0&gt; . make sure super_resolution folder exists in Google Drive . %%bash mkdir -p /content/drive/MyDrive/super_resolution cp -r training/* /content/drive/MyDrive/super_resolution . save and load the model . path = &#39;/content/drive/MyDrive/super_resolution/model.h5&#39; model.save(path) new_model = tf.keras.models.load_model(path) . Evaluation . After train the model for enough time we can evaluate it. Let&#39;s pick a random image from the dataset (or you can use anyother image) and transform it into a low resolution image that we can pass to the SRCNN model. . path = np.random.choice(dataset_paths) image = load_img(path) image = img_to_array(image) image = image.astype(np.uint8) image = tight_crop_image(image, SCALE) scaled = downsize_upsize_image(image, SCALE) . We need a placeholder where we will put the output patches to create the final image . output = np.zeros(scaled.shape) height, width = output.shape[:2] . Now we extarct patches from the input image, pass them through the trained model to generate high resolution patch and then put this patch in the right position on the previous placeholder. After processing every patch from the input image we will have a final output image . for y in range(0, height - INPUT_DIM + 1, LABEL_SIZE): for x in range(0, width - INPUT_DIM + 1, LABEL_SIZE): crop = crop_input(scaled, x, y) image_batch = np.expand_dims(crop, axis=0) prediction = model.predict(image_batch) new_shape = (LABEL_SIZE, LABEL_SIZE, 3) prediction = prediction.reshape(new_shape) output_y_slice = slice(y + PAD, y + PAD + LABEL_SIZE) output_x_slice = slice(x + PAD, x + PAD + LABEL_SIZE) output[output_y_slice, output_x_slice] = prediction . Now we can display side by side the low resolution image as well as the resulting output image which is of higher resolution. . figure, axis = plt.subplots(1, 2, figsize=(15, 8)) axis[0].imshow(np.array(scaled,np.int32)) axis[0].set_title(&#39;Low resolution image (Downsize + Upsize)&#39;) axis[0].axis(&#39;off&#39;) axis[1].imshow(np.array(output,np.int32)) axis[1].set_title(&#39;Super resolution result (SRCNN output)&#39;) axis[1].axis(&#39;off&#39;) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . (-0.5, 299.5, 197.5, -0.5) . Very impressive result considering the small model that we trained, as you can see it was able to considerably improve the resolution of the input image. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/generative/artistic/2021/05/10/Super_Resolution_SRCNN.html",
            "relUrl": "/tensorflow/generative/artistic/2021/05/10/Super_Resolution_SRCNN.html",
            "date": " • May 10, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Effortless Neural Style Transfer with TFHub",
            "content": "Neural style transfer was first introduced in Leon A. Gatys’ paper, A Neural Algorithm of Artistic Style. It&#39;s a very interesting use of Deep Learning to generate artistic images by mixing three images: a content image, a style reference image (e.g. some famous painter&#39;s artwork), and an input image that will be styled. The output result is an image that looks like the input image but but “painted” in the same style as the style image. . Implementing Neural Style Transfer from scratch can be challenging (see this tutorial for a Tensorflow implementation - link), but likely we can leverage TensorFlow Hub (TFHub) and use a pretrained implementation of Neural Style Transfer to style our own images. . . In this article, we will use a Neural Style Transfer module from TensorFlow Hub link that performs fast artistic style transfer and can work with arbitrary painting styles. . First, let&#39;s import the needed dependencies: . import glob from tqdm import tqdm import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from tensorflow_hub import load . Then, we need to define a function to load images into a TensorFlow tensor and do some processing (e.g. rescaling) . def load_image(image_path): dimension = 512 image = tf.io.read_file(image_path) image = tf.image.decode_jpeg(image, channels=3) image = tf.image.convert_image_dtype(image, tf.float32) shape = tf.cast(tf.shape(image)[:-1], tf.float32) longest_dimension = max(shape) scale = dimension / longest_dimension new_shape = tf.cast(shape * scale, tf.int32) image = tf.image.resize(image, new_shape) return image . Because, the output of the model is a tensor, we need to define a helper function to convert such tensor into a NumPy array so that we can display the image with matplotlib . def tensor_to_image(tensor): tensor = tensor * 255 tensor = np.array(tensor, dtype=np.uint8) if np.ndim(tensor) &gt; 3: tensor = tensor[0] return tensor . To easily display all the images at once, we define the following helper function . def show_images(images, num_rows, num_cols, figsize=(35, 15)): figure, axis = plt.subplots(num_rows, num_cols, figsize=figsize) for index, image in enumerate(images): row, col = int(index / num_cols), index % num_cols if num_rows == 1: axis[col].imshow(image) axis[col].axis(&#39;off&#39;) else: axis[row, col].imshow(image) axis[row, col].axis(&#39;off&#39;) . Let&#39;s download the Style Transfer module from Tensorflow Hub and load it . module_url = (&#39;https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2&#39;) hub_module = load(module_url) . Get some images for testing . %%bash rm -rf images mkdir -p images curl -s -o images/bicycle1.jpg https://cdn.pixabay.com/photo/2016/11/22/23/49/cyclists-1851269_960_720.jpg curl -s -o images/animal1.jpg https://cdn.pixabay.com/photo/2014/05/20/21/20/bird-349026_960_720.jpg curl -s -o images/car2.jpg https://cdn.pixabay.com/photo/2016/09/11/10/02/renault-juvaquatre-1661009_960_720.jpg . Load the content images and display them . images = [] image_paths = glob.glob(&#39;images/*&#39;) for path in image_paths: images.append(load_image(path)) . show_images(images, 1, 3, figsize=(35, 15)) . We also download some images to use for styling: . %%bash rm -rf styles mkdir -p styles curl -s -o styles/splashing.jpg https://cdn.pixabay.com/photo/2013/07/19/00/18/splashing-165192_960_720.jpg curl -s -o styles/fireball.jpg https://cdn.pixabay.com/photo/2014/08/20/18/44/fireball-422746_960_720.jpg curl -s -o styles/colorful.jpg https://cdn.pixabay.com/photo/2017/07/03/20/17/colorful-2468874_960_720.jpg curl -s -o styles/texture.jpg https://cdn.pixabay.com/photo/2017/08/09/04/53/texture-2613518_960_720.jpg . styles = [] style_paths = glob.glob(&#39;styles/*&#39;) for path in style_paths: styles.append(load_image(path)) . Let&#39;s have quick look at the different styles . show_images(styles, 1, 4, figsize=(35, 15)) . Finally, we iterate over each image/syle pair and pass them through the TF Hub module we loaded earlier: . stylized_images = [] for image in tqdm(images): image = image[tf.newaxis, :] for style in styles: style = style[tf.newaxis, :] results = hub_module(tf.constant(image), tf.constant(style)) stylized_image = tensor_to_image(results[0]) stylized_images.append(stylized_image) . 100%|██████████| 3/3 [00:41&lt;00:00, 13.90s/it] . Let&#39;s inspect the resulting images for each content image and syle . show_images(stylized_images, 3, 4, (28, 15)) . The result looks pretty good as you can see, the resulting images preserve the coherence and meaning from the original scene, while adding artistic traits from the style images. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/artistic/generative/2021/04/05/Style_Transfer_TFHub.html",
            "relUrl": "/tensorflow/artistic/generative/2021/04/05/Style_Transfer_TFHub.html",
            "date": " • Apr 5, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Effortless Video Action Recognition with TF Hub",
            "content": "Action recognition is a very hot topic in the broader video processing and understanding research. The below timeline illustrates how reaseracher are getting more interested in the area and proposing SOTA models to push the field further. source . Training a model from scratch to perform this task is very challenging due to the nature of the task that is not only associated with classifying the content of an image, but also includes a temporal component. In this article, we will leaverage TensorFlow Hub and pick one model from this large model zoo to perform Video Action Recognition effortlessly. Specifically we will use DeepMind&#39;s Inflated 3D Convnet (I3D) model which was training on DeepMind Kinetics dataset. . . First, let&#39;s import all the needed dependencies . import os import random import re import ssl import tempfile from pathlib import Path from urllib import request import matplotlib.pyplot as plt import cv2 import imageio import numpy as np import tensorflow as tf import tensorflow_hub as tfhub from IPython.display import Image from wordcloud import WordCloud . Dataset . There is lot datasets, you can find more here, we will be using UCF101 Action Recognition dataset which is available for public download here. We will not be using the entirety of the dataset but just picking a random video. . The different activities available in the UCF101 Action Recognition dataset are as follows: . . To list and download videos from the UCF101 website we define the following helper class that exposes multiple functions to make it easy to get a video locally. . class UCFDataset(object): def __init__(self): self.UNVERIFIED_CONTEXT = ssl._create_unverified_context() self.UCF_ROOT = &#39;https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/&#39; # Temporary directory to cache the downloaded videos self.CACHE_DIR = tempfile.mkdtemp() self.videos_list = self.download_videos_list() def _read(self, url): &quot;&quot;&quot;Read data for the given url&quot;&quot;&quot; return request.urlopen(url,context=self.UNVERIFIED_CONTEXT).read() def download_videos_list(self): &quot;&quot;&quot;Dowload the list of video names and direct download urls&quot;&quot;&quot; index = (self._read(self.UCF_ROOT).decode(&#39;utf-8&#39;)) videos = re.findall(&#39;(v_[ w]+ .avi)&#39;, index) return sorted(set(videos)) def __getitem__(self, video_name): &quot;&quot;&quot;Download a specific video by name&quot;&quot;&quot; cache_path = os.path.join(self.CACHE_DIR, video_name) if not os.path.exists(cache_path): url = request.urljoin(self.UCF_ROOT, video_name) response = self._read(url) with open(cache_path, &#39;wb&#39;) as f: f.write(response) return cache_path def download_random_video(self): &quot;&quot;&quot;Download a random video from the dataset&quot;&quot;&quot; video_name = random.choice(self.videos_list) return self.__getitem__(video_name) . Define a helper function to crop a squared selection in the center of a frame . def crop_center(frame): height, width = frame.shape[:2] smallest_dimension = min(width, height) x_start = (width // 2) - (smallest_dimension // 2) x_end = x_start + smallest_dimension y_start = (height // 2) - (smallest_dimension // 2) y_end = y_start + smallest_dimension roi = frame[y_start:y_end, x_start:x_end] return roi . Define a helper function to read a video by path, take up to max_frames frames from it, and return a resized to (224, 224, 3) selection of those frames. . def read_video(path, max_frames=32, resize=(224, 224)): capture = cv2.VideoCapture(path) frames = [] while len(frames) &lt;= max_frames: frame_read, frame = capture.read() if not frame_read: break frame = crop_center(frame) frame = cv2.resize(frame, resize) frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) frames.append(frame) capture.release() frames = np.array(frames) return frames / 255. . To be able to visualize the video on this notebook, we need function to create a save a collection of frames as a GIF. . def save_as_gif(images, video_name): filename = f&#39;./{video_name}.gif&#39; converted_images = np.clip(images * 255, 0, 255) converted_images = converted_images.astype(np.uint8) imageio.mimsave(filename, converted_images,fps=25) return filename . Now, we can download the list of videos from the UFC101 dataset . dataset = UCFDataset() . Model . In this section we download the I3D model and prepare it for running predictions on images . First, let&#39;s get the labels file from the Kinetics dataset that was used to train the I3D model . KINETICS_URL = &#39;https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt&#39; . The following helper function download the labels from the previous link . def fetch_kinetics_labels(): with request.urlopen(KINETICS_URL) as f: labels = [line.decode(&#39;utf-8&#39;).strip() for line in f.readlines()] return labels . Download the list of labels, and diplay them in a wordcloud . LABELS = fetch_kinetics_labels() . wordcloud = WordCloud(collocations = False, background_color = &#39;white&#39;) wordcloud = wordcloud.generate(&#39; &#39;.join(LABELS)) plt.figure(figsize=(10, 12)) plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Now let&#39;s download the I3D model from TensorFlow Hub . model_path = &#39;https://tfhub.dev/deepmind/i3d-kinetics-400/1&#39; model = tfhub.load(model_path) model = model.signatures[&#39;default&#39;] . Prediction . Next we define a helper function that will take sampled frames from a video, and print at most the top recognized actions . def predict(model, labels, sample_video): model_input = tf.constant(sample_video, dtype=tf.float32) model_input = model_input[tf.newaxis, ...] logits = model(model_input)[&#39;default&#39;][0] probabilities = tf.nn.softmax(logits) print(&#39;Top actions:&#39;) for i in np.argsort(probabilities)[::-1][:5]: if probabilities[i] &lt; 0.01: break print(f&#39;{labels[i]}: {probabilities[i] *100:5.2f}%&#39;) . Define helper function to pick a random video, save its frames as GIF . def download_random_video(): video_path = dataset.download_random_video() sample_video = read_video(video_path) video_name = Path(video_path).stem gif_path = save_as_gif(sample_video, video_name) return sample_video, gif_path . Pick a random video, display the resulting GIF . sample_video, gif_path = download_random_video() Image(open(gif_path,&#39;rb&#39;).read()) . Pass the video through the I3D network to obtain the predicted actions . predict(model, LABELS, sample_video) . Top actions: javelin throw: 23.70% high jump: 16.25% triple jump: 10.52% throwing discus: 9.20% playing tennis: 8.41% . Try another video . sample_video, gif_path = download_random_video() Image(open(gif_path,&#39;rb&#39;).read()) . predict(model, LABELS, sample_video) . Top actions: wrestling: 58.94% throwing ball: 11.95% high kick: 5.55% catching or throwing frisbee: 4.60% catching or throwing softball: 3.12% . See how the model is able to acurately predict the action in the video. You can try with other video as an exercise. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/video/action-recognition/2021/03/30/Action_Recognition_TFHub.html",
            "relUrl": "/tensorflow/video/action-recognition/2021/03/30/Action_Recognition_TFHub.html",
            "date": " • Mar 30, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Image Captioning Part I",
            "content": "In order to build an image captioning model, we need to transform the data and extract features that can be used as input for such model. We need to encode the images into a dense representation as well as encode the text as embeddings (vectorial representations of sentences). . This article, will focus on building this feature extractor and apply to generate a formatted features for the image captioning model trainig. . We will use the Flickr8k dataset (availble on Kaggle here). So we wil install Kaggle CLI and download the dataset. . %%capture %%bash pip install kaggle --upgrade mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;KAGGLE_USER&quot;,&quot;key&quot;:&quot;KAGGLE_KEY&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json . %%capture %%bash kaggle datasets download adityajn105/flickr8k mkdir -p flickr8k unzip flickr8k.zip -d flickr8k . . Note: you need to replace KAGGLE_USER with your actual Kaggle username and KAGGLE_KEY with your API key for the download to work. . First, lets mount a Google drive so we can store and later restore the features we will extract . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . Import the needed dependencies . import glob import os import pathlib from math import sqrt from joblib import Parallel, delayed import pickle from string import punctuation import numpy as np import pandas as pd import matplotlib.pyplot as plt from tensorflow.keras.applications.vgg16 import * from tensorflow.keras.layers import * from tensorflow.keras.preprocessing.image import * from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.utils import to_categorical from tqdm import tqdm . Set the following variables to the paths where the dataset is available . BASE_PATH = &#39;flickr8k&#39; IMAGES_PATH = f&#39;{BASE_PATH}/Images&#39; CAPTIONS_PATH = f&#39;{BASE_PATH}/captions.txt&#39; . With the path to the images set, we can list all the image files . image_paths = list(glob.glob(f&#39;{IMAGES_PATH}/*.jpg&#39;)) . Let&#39;s read the captions file into a Pandas dataframe and have look to it . captions_df = pd.read_csv(CAPTIONS_PATH) captions_df = captions_df.groupby(&#39;image&#39;).first().reset_index() captions_df.head() . image caption . 0 1000268201_693b08cb0e.jpg | A child in a pink dress is climbing up a set o... | . 1 1001773457_577c3a7d70.jpg | A black dog and a spotted dog are fighting | . 2 1002674143_1b742ab4b8.jpg | A little girl covered in paint sits in front o... | . 3 1003163366_44323f5815.jpg | A man lays on a bench while his dog sits by him . | . 4 1007129816_e794419615.jpg | A man in an orange hat starring at something . | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . Note: in the dataset each image has more than one caption, we discard all captions and keep only one per image for simplicity . With the images and captions loaded, we can take a random sample and dispaly some images with their respective caption . samples = captions_df.sample(6).reset_index() figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, sample in samples.iterrows(): image = plt.imread(f&#39;{IMAGES_PATH}/{sample[&quot;image&quot;]}&#39;) title = sample[&#39;caption&#39;][:50] + &#39; n&#39; + sample[&#39;caption&#39;][50:] row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].set_title(title) axis[row, col].axis(&#39;off&#39;) . We need to clean the text in captions (e.g. removing punctuation) to simplify training, and also adding special tokens start_token to be added at the begning of the text, and end_token added at the end. . def clean_caption(caption, start_token=&#39;_sos_&#39;, end_token=&#39;_eos_&#39;): def remove_punctuation(word): translation = str.maketrans(&#39;&#39;, &#39;&#39;, punctuation) return word.translate(translation) def is_valid_word(word): return len(word) &gt; 1 and word.isalpha() caption = caption.lower().split(&#39; &#39;) caption = map(remove_punctuation, caption) caption = filter(is_valid_word, caption) cleaned_caption = f&#39;{start_token} {&quot; &quot;.join(caption)} {end_token}&#39; return cleaned_caption . captions_df[&#39;caption&#39;] = captions_df.caption.apply(lambda x: clean_caption(x)) . We will need to perform padding later, hence we need to determine the maximum caption text length . ax = captions_df.caption.apply(lambda x: len(x)).hist() ax.vlines(x=120, ymin=0, ymax=2800, colors=&#39;red&#39;, ls=&#39;-&#39;, lw=2) . &lt;matplotlib.collections.LineCollection at 0x7f5fe2701150&gt; . We need to determine the maximum length to allow for captions, so we can either: . determine the maximum length with captions_df.caption.apply(lambda x: len(x)).max() | Or pick a maximum length based on the histogram of values to avoid OOMs, e.g. 120 | . max_seq_len = 120 . We take the captions into an array that should be of same length as the image paths array . captions = captions_df.caption.values . len(image_paths), len(captions) . (8091, 8091) . The following class groups the different feautre extraction steps that loads VGG16 trained on ImageNet to be used for transforming images into a vector, also it will sequences out of the image&#39;s caption. . class FeatureExtractor(object): def __init__(self, output_path, max_seq_len): self.feature_extractor = VGG16(input_tensor=Input(shape=(224, 224, 3)), weights=&#39;imagenet&#39;, include_top=False) self.output_path = output_path self.tokenizer = Tokenizer() self.max_seq_length = max_seq_len def create_image_vector(self, path): &quot;&quot;&quot;Create a vector representing the image&quot;&quot;&quot; image = load_img(path, target_size=(224, 224)) image = img_to_array(image) image = np.expand_dims(image, axis=0) image = preprocess_input(image) return self.feature_extractor.predict(image)[0] def create_mapping(self, paths, captions): &quot;&quot;&quot;Extract features from each image, and create mapping from image filename to image features and caption&quot;&quot;&quot; mapping = {} print(&#39; nExtracting features...&#39;) def extract_feature(path, caption): features = self.create_image_vector(path) image_id = pathlib.Path(path).stem mapping[image_id] = {&#39;features&#39;: features, &#39;caption&#39;: caption} for path, caption in tqdm(zip(paths, captions), total=len(paths)): extract_feature(path, caption) return mapping def save_mapping(self, mapping): &quot;&quot;&quot;Save mappings into disk&quot;&quot;&quot; out_path = f&#39;{self.output_path}/mapping.pickle&#39; with open(out_path, &#39;wb&#39;) as f: pickle.dump(mapping, f, protocol=4) def extract_features(self, paths, captions): &quot;&quot;&quot;Extract features from images and captions and stores them to disk&quot;&quot;&quot; self.tokenizer.fit_on_texts(captions) mapping = self.create_mapping(paths, captions) self.save_mapping(mapping) in_feats, in_seqs, out_seqs = self.create_sequences(mapping) self.save_sequences(in_feats, in_seqs, out_seqs) def create_sequences(self, mapping): &quot;&quot;&quot;Create sequences based on the image and its caption&quot;&quot;&quot; num_classes = len(self.tokenizer.word_index) + 1 in_feats = [] in_seqs = [] out_seqs = [] print(&#39; nCreating sequences...&#39;) for _, data in tqdm(mapping.items()): feature = data[&#39;features&#39;] caption = data[&#39;caption&#39;] seq = self.tokenizer.texts_to_sequences([caption]) seq = seq[0] # create as many sequences as the length of the current caption for i in range(1, len(seq)): # input sequence is first ith characters input_seq = seq[:i] # trip the sentence if it is longer than max_seq_length if len(input_seq) &gt;= self.max_seq_length: input_seq = input_seq[:self.max_seq_length] input_seq, = pad_sequences([input_seq], self.max_seq_length) # output sequence is the next character which is the ith out_seq = seq[i] out_seq = to_categorical([out_seq], num_classes)[0] # add all to lists in_feats.append(feature) in_seqs.append(input_seq) out_seqs.append(out_seq) return in_feats, in_seqs, out_seqs def save_sequences(self, in_feats, in_seqs, out_seqs): &quot;&quot;&quot;Save image features, input and output sequences into disk&quot;&quot;&quot; filenames = [&#39;input_features.pickle&#39;, &#39;input_sequences.pickle&#39;, &#39;output_sequences.pickle&#39;] sequences = [in_feats, in_seqs, out_seqs] for filename, seq in zip(filenames, sequences): with open(f&#39;{self.output_path}/{filename}&#39;, &#39;wb&#39;) as f: pickle.dump(seq, f, protocol=4) . Now we can create an instance of the feature extractor and run it on the images and their captions . extractor = FeatureExtractor(&#39;.&#39;, max_seq_len) . extractor.extract_features(image_paths, captions) . Extracting features... . 100%|██████████| 8091/8091 [1:10:57&lt;00:00, 1.90it/s] . Creating sequences... . 100%|██████████| 8091/8091 [00:07&lt;00:00, 1065.88it/s] . As the dataset have lot images, it may take some time before it finishes. At the end, it will generate the following pickle files: . mapping.pickle | input_features.pickle | input_sequences.pickle | output_sequences.pickle | . Move the pickle files to Google Drive so we can restore them later . !ls /content/drive/MyDrive/data/ . input_features.pickle mapping.pickle input_sequences.pickle output_sequences.pickle . !mv *.pickle /content/drive/MyDrive/data/ . Next we will use those files as input for training an Image Captioning model. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/captioning/2021/03/21/Image_Captioning_Part_I.html",
            "relUrl": "/tensorflow/vision/captioning/2021/03/21/Image_Captioning_Part_I.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Object Detection on custom dataset with EfficientNet",
            "content": "In a previous article we saw how to use TensorFlow&#39;s Object Detection API to run object detection on images using pre-trained models freely available to download from TF Hub - link. This article we will go one step further by training a model on our own custom Object detection dataset using TensorFlow&#39;s Object Detection API. . First, lets install the TensorFlow Object Detection API . %%capture %%bash git clone --depth 1 https://github.com/tensorflow/models cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install . . We need to have opencv-python-headless version with version 4.1.2.30 installed for the training using Object Detection API to work . %%capture %%bash yes | pip uninstall opencv-python-headless pip install opencv-python-headless==4.1.2.30 . The dataset we will use is Fruit Images for Object Detection dataset from Kaggle. This is a very small dataset with images of the three classes apple, banana and orange. It is more enough to get started with training on custom dataset but you can use your own dataset too. . We will use the Kaggle CLI to download the dataset, unzip and prepare the train/test datasets. . !pip install kaggle --upgrade -q . %%bash mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;KAGGLE_USERNAME&quot;,&quot;key&quot;:&quot;KAGGLE_KEY&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json . %%capture %%bash kaggle datasets download mbkinaci/fruit-images-for-object-detection unzip fruit-images-for-object-detection mkdir -p fruit-images mv train_zip/* fruit-images/ mv test_zip/* fruit-images/ . Note how we move files around in training and test, this is for convinience because the original dataset zip file had images under train_zip/train/*.jpg and test_zip/test/*.jpg. . Training . After download the dataset and install the required dependencies we can start writting the code for training . First, import dependencies . import glob import io import os from collections import namedtuple from xml.etree import ElementTree as tree import pandas as pd import tensorflow.compat.v1 as tf from PIL import Image from object_detection.utils import dataset_util from object_detection.protos import pipeline_pb2 from google.protobuf import text_format . Data transformation . We need to convert the images into the format accepted by the training pipeline in TensorFlow Object Detection API which is TF Example format. . We need to define a helper function to encode a label into its index . def encode_class(row_label): class_mapping = {&#39;apple&#39;: 1, &#39;orange&#39;: 2, &#39;banana&#39;: 3} return class_mapping.get(row_label, None) . We also define a helper function to create the train/test splits . def split(df, group): Data = namedtuple(&#39;data&#39;, [&#39;filename&#39;, &#39;object&#39;]) groups = df.groupby(group) return [Data(filename, groups.get_group(x)) for filename, x in zip(groups.groups.keys(), groups.groups)] . The following function takes train/test images and convert them into one corresponding TF Example file where the image, the bounding boxes, ground-truth classes are grouped as features. . def create_tf_example(group, path): groups_path = os.path.join(path, f&#39;{group.filename}&#39;) with tf.gfile.GFile(groups_path, &#39;rb&#39;) as f: encoded_jpg = f.read() image = Image.open(io.BytesIO(encoded_jpg)) width, height = image.size filename = group.filename.encode(&#39;utf8&#39;) image_format = b&#39;jpg&#39; # 5. Now, store the dimensions of the bounding boxes, along with the classes of each object contained in the image: xmins = [] xmaxs = [] ymins = [] ymaxs = [] classes_text = [] classes = [] for index, row in group.object.iterrows(): xmins.append(row[&#39;xmin&#39;] / width) xmaxs.append(row[&#39;xmax&#39;] / width) ymins.append(row[&#39;ymin&#39;] / height) ymaxs.append(row[&#39;ymax&#39;] / height) classes_text.append(row[&#39;class&#39;].encode(&#39;utf8&#39;)) classes.append(encode_class(row[&#39;class&#39;])) # 6. Create a tf.train.Features object that will contain relevant information about the image and its objects: features = tf.train.Features(feature={ &#39;image/height&#39;: dataset_util.int64_feature(height), &#39;image/width&#39;: dataset_util.int64_feature(width), &#39;image/filename&#39;: dataset_util.bytes_feature(filename), &#39;image/source_id&#39;: dataset_util.bytes_feature(filename), &#39;image/encoded&#39;: dataset_util.bytes_feature(encoded_jpg), &#39;image/format&#39;: dataset_util.bytes_feature(image_format), &#39;image/object/bbox/xmin&#39;: dataset_util.float_list_feature(xmins), &#39;image/object/bbox/xmax&#39;: dataset_util.float_list_feature(xmaxs), &#39;image/object/bbox/ymin&#39;: dataset_util.float_list_feature(ymins), &#39;image/object/bbox/ymax&#39;: dataset_util.float_list_feature(ymaxs), &#39;image/object/class/text&#39;: dataset_util.bytes_list_feature(classes_text), &#39;image/object/class/label&#39;: dataset_util.int64_list_feature(classes) }) # 7. Return a tf.train.Example structure initialized with the features created previously: return tf.train.Example(features=features) . The bounding boxes in the dataset for each image are defined in an XML file (base of PASCAL VOC format - link). We need to parse each of those metadata files to extract the bounding boxes and labels . def bboxes_to_csv(path): xml_list = [] bboxes_pattern = os.path.sep.join([path, &#39;*.xml&#39;]) for xml_file in glob.glob(bboxes_pattern): t = tree.parse(xml_file) root = t.getroot() for member in root.findall(&#39;object&#39;): value = (root.find(&#39;filename&#39;).text, int(root.find(&#39;size&#39;)[0].text), int(root.find(&#39;size&#39;)[1].text), member[0].text, int(member[4][0].text), int(member[4][1].text), int(member[4][2].text), int(member[4][3].text)) xml_list.append(value) column_names = [&#39;filename&#39;, &#39;width&#39;, &#39;height&#39;, &#39;class&#39;,&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;] df = pd.DataFrame(xml_list, columns=column_names) return df . Now let&#39;s process every image in the train/test datasets along with the metadata file to create the corresponding TF Record file. . base = &#39;fruit-images&#39; for subset in [&#39;test&#39;, &#39;train&#39;]: labels_path = os.path.sep.join([base,f&#39;{subset}_labels.csv&#39;]) bboxes_df = bboxes_to_csv(f&#39;{base}/{subset}&#39;) bboxes_df.to_csv(labels_path, index=None) # 10. Then, use the same labels to produce the tf.train.Examples corresponding to the current subset of data being processed: writer = (tf.io.TFRecordWriter(f&#39;{base}/{subset}.record&#39;)) examples = pd.read_csv(f&#39;{base}/{subset}_labels.csv&#39;) grouped = split(examples, &#39;filename&#39;) path = os.path.join(f&#39;{base}/{subset}&#39;) for group in grouped: tf_example = create_tf_example(group, path) writer.write(tf_example.SerializeToString()) writer.close() . This is how the result of processing the metadata looks like . filename width height class xmin ymin xmax ymax . 0 | mixed_18.jpg | 1023 | 682 | orange | 67 | 163 | 441 | 541 | . 1 | mixed_18.jpg | 1023 | 682 | banana | 209 | 134 | 866 | 348 | . 2 | mixed_18.jpg | 1023 | 682 | banana | 263 | 267 | 849 | 551 | . 3 | apple_11.jpg | 652 | 436 | apple | 213 | 33 | 459 | 258 | . 4 | apple_11.jpg | 652 | 436 | apple | 1 | 30 | 188 | 280 | . Now the data is reading for training . !ls -l fruit-images . total 30388 drwxr-xr-x 2 root root 4096 Jan 22 02:38 test -rw-r--r-- 1 root root 5161 Jan 22 02:38 test_labels.csv -rw-r--r-- 1 root root 7081578 Jan 22 02:38 test.record drwxr-xr-x 2 root root 20480 Jan 22 02:38 train -rw-r--r-- 1 root root 20122 Jan 22 02:38 train_labels.csv -rw-r--r-- 1 root root 23981776 Jan 22 02:38 train.record . Pre-trained EfficientNet . To run the training on our custom dataset, we will fine tune EfficientNet one of the models in TensorFlow Object Detection API that was trained on COCO dataset. We will download a checkpoint of the model&#39;s weights from TensorFlow 2 Detection Model Zoo. Specifically we will downlad the weights of EfficientDet D0 512x512 but you can smaller models like MobileNet v2 320x320 for faster training. . %%capture %%bash CHECKPOINT_DATE=20200711 MODEL_NAME=efficientdet_d0_coco17_tpu-32 curl -O http://download.tensorflow.org/models/object_detection/tf2/$CHECKPOINT_DATE/$MODEL_NAME.tar.gz tar xzf $MODEL_NAME.tar.gz . !ls efficientdet_d0_coco17_tpu-32/checkpoint . checkpoint ckpt-0.data-00000-of-00001 ckpt-0.index . We need to create the label_map.txt file to map the classes to integers . %%writefile fruit-images/label_map.txt item { id: 1 name: &#39;apple&#39; } item { id: 2 name: &#39;orange&#39; } item { id: 3 name: &#39;banana&#39; } . Writing fruit-images/label_map.txt . Next, we need to change the configuration file for this network to fit our need. This configuration file can be found locally at models/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config. . Note: those configuration files are Protocol Buffers objects described in the .proto files under models/research/object_detection/protos. The top level object is a TrainEvalPipelineConfig defined in pipeline.proto. You can learn more about those configuration files by reading the documentation. . The following helper functions are used to load and save a configuration file, they are based of code borrowed from config_util.py. . def get_pipeline_config(path): pipeline_config = pipeline_pb2.TrainEvalPipelineConfig() with tf.gfile.GFile(path, &#39;r&#39;) as f: text_format.Merge(f.read(), pipeline_config) return pipeline_config def save_pipeline_config(pipeline_config, path): config_text = text_format.MessageToString(pipeline_config) with tf.gfile.Open(path, &quot;wb&quot;) as f: tf.logging.info(&quot;Writing pipeline config file to %s&quot;, path) f.write(config_text) . Load the EfficientNet configuration and update it accordingly: . pipeline_config_path = &#39;models/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config&#39; pipeline_config = get_pipeline_config(pipeline_config_path) . Lower batch size depending on how much memory your system has . pipeline_config.train_config.batch_size = 16 . Update the number of classes to match the ones in our custom dataset . pipeline_config.model.ssd.num_classes = 3 . Point to the checkout point file of the EfficientNet weights we downloaded earlier . pipeline_config.train_config.fine_tune_checkpoint = &#39;/content/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0&#39; . Change the checkpoint type to detection . pipeline_config.train_config.fine_tune_checkpoint_type = &#39;detection&#39; . Point to the label/index mapping file . pipeline_config.train_input_reader.label_map_path = &#39;/content/fruit-images/label_map.txt&#39; . Point to the training TF Record file we created earlier . pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = &#39;/content/fruit-images/train.record&#39; . Point to the label/index mapping file . pipeline_config.eval_input_reader[0].label_map_path = &#39;/content/fruit-images/label_map.txt&#39; . Point to the test TF Record file we created earlier . pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0] = &#39;/content/fruit-images/test.record&#39; . save_pipeline_config(pipeline_config, &#39;pipeline.config&#39;) . INFO:tensorflow:Writing pipeline config file to pipeline.config . Now we can start training the model using the model_main_tf2.py helper program and the configuration we just updated. . %%bash cd models/research/object_detection mkdir -p /content/training CONFIG_PATH=/content/pipeline.config MODEL_DIR=/content/training python model_main_tf2.py --pipeline_config_path=$CONFIG_PATH --model_dir=$MODEL_DIR --num_train_steps=1000 --record_summaries . INFO:tensorflow:Step 1000 per-step time 1.237s I0122 03:01:21.339013 139649250535296 model_lib_v2.py:707] Step 1000 per-step time 1.237s INFO:tensorflow:{&#39;Loss/classification_loss&#39;: 0.2277294, &#39;Loss/localization_loss&#39;: 0.1371322, &#39;Loss/regularization_loss&#39;: 0.028924104, &#39;Loss/total_loss&#39;: 0.3937857, &#39;learning_rate&#39;: 0.0326} I0122 03:01:21.339331 139649250535296 model_lib_v2.py:708] {&#39;Loss/classification_loss&#39;: 0.2277294, &#39;Loss/localization_loss&#39;: 0.1371322, &#39;Loss/regularization_loss&#39;: 0.028924104, &#39;Loss/total_loss&#39;: 0.3937857, &#39;learning_rate&#39;: 0.0326} . Once the training is finished, we can check the training logs using TensorBoard . %load_ext tensorboard %tensorboard --logdir /content/training . To be able to use the new trained model in inference, we need to use the exporter_main_v2.py program as follows: . %%bash cd models/research/object_detection mkdir -p /content/inference_graph CHECKPOINT_DIR=/content/training CONFIG_PATH=/content/pipeline.config OUTPUT_DIR=/content/inference_graph python exporter_main_v2.py --trained_checkpoint_dir=$CHECKPOINT_DIR --pipeline_config_path=$CONFIG_PATH --output_directory=$OUTPUT_DIR . ... INFO:tensorflow:Assets written to: /content/inference_graph/saved_model/assets I0122 03:03:07.437493 140608935167872 builder_impl.py:784] Assets written to: /content/inference_graph/saved_model/assets INFO:tensorflow:Writing pipeline config file to /content/inference_graph/pipeline.config I0122 03:03:08.824219 140608935167872 config_util.py:254] Writing pipeline config file to /content/inference_graph/pipeline.config . Inference . Let&#39;s use the trained model on the test images and check its quality. . import glob import random from io import BytesIO import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from PIL import Image from object_detection.utils import ops from object_detection.utils import visualization_utils as viz from object_detection.utils.label_map_util import create_category_index_from_labelmap %matplotlib inline . Define a helper function to load an image and prepare them for the model expected input . def load_image(path): image_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(image_data)) width, height = image.size shape = (height, width, 3) image = np.array(image.getdata()) image = image.reshape(shape).astype(&#39;uint8&#39;) return image . Define a helper function to run inference on an input image . def run_inference(net, image): image = np.asarray(image) input_tensor = tf.convert_to_tensor(image) input_tensor = input_tensor[tf.newaxis, ...] # forward pass model = net.signatures[&#39;serving_default&#39;] result = model(input_tensor) # extract detections num_detections = int(result.pop(&#39;num_detections&#39;)) result = {key: value[0, :num_detections].numpy() for key, value in result.items()} result[&#39;num_detections&#39;] = num_detections result[&#39;detection_classes&#39;] = result[&#39;detection_classes&#39;].astype(&#39;int64&#39;) # use mask if available if &#39;detection_masks&#39; in result: detection_masks_reframed = ops.reframe_box_masks_to_image_masks(result[&#39;detection_masks&#39;], result[&#39;detection_boxes&#39;], image.shape[0], image.shape[1]) detection_masks_reframed = tf.cast(detection_masks_reframed &gt; 0.5, tf.uint8) result[&#39;detection_masks_reframed&#39;] = detection_masks_reframed.numpy() return result . Let&#39;s load the model we exported earlier and create CATEGORY_IDX based on the label/index mapping file . labels_path = &#39;/content/fruit-images/label_map.txt&#39; CATEGORY_IDX = create_category_index_from_labelmap(labels_path, use_display_name=True) model_path = &#39;/content/inference_graph/saved_model&#39; model = tf.saved_model.load(model_path) . Select random images from the test dataset . image_paths = list(glob.glob(&#39;fruit-images/test/*.jpg&#39;)) image_paths = random.choices(image_paths, k=6) . Define a helper function to load and image, run inference on it and draw the predicted bounding boxes: . def get_image_with_boxes(model, path): image = load_image(path) annotation = run_inference(model, image) masks = annotation.get(&#39;detection_masks_reframed&#39;, None) viz.visualize_boxes_and_labels_on_image_array( image, annotation[&#39;detection_boxes&#39;], annotation[&#39;detection_classes&#39;], annotation[&#39;detection_scores&#39;], CATEGORY_IDX, instance_masks=masks, use_normalized_coordinates=True, line_thickness=5) return image . image_paths = list(glob.glob(&#39;fruit-images/test/*.jpg&#39;)) image_paths = random.choices(image_paths, k=6) images = [get_image_with_boxes(model, path) for path in image_paths] . Display the images along with the bounding boxes . figure, axis = plt.subplots(2, 3, figsize=(15, 10)) for index, image in enumerate(images): row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].axis(&#39;off&#39;) . The model see to perfom quite well, you can try train it on a harder dataset .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/object-detection/2021/03/02/Object_Detection_custom_dataset.html",
            "relUrl": "/tensorflow/vision/object-detection/2021/03/02/Object_Detection_custom_dataset.html",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Object Detection with YOLOv3 in Tensorflow",
            "content": "YOLO (You Only Look Once) is an end to end object detection algorithm. Compared to other algorithms that repurpose classifiers to perform detection, YOLO requires only a single pass to detect objects, i.e. classes probabilities and bounding boxes. . It works by dividing an image into N equaly sized SxS regions. Each grid will predict the probability of object presence, the coordinates of a bounding box B and the object label. This makes YOLO a very fast algorithm and can be used for real time detection. . In this article, we will build YOLO v3 in Tensorflow and initiate its weights with the weights of the original YOLO v3 model pretrained on the COCO dataset. The following diagram illustrates the architecture of YOLO we will be building. . . Our implementation is heavily inspired by this Keras implementation - repo . First, let&#39;s download the weights from the YOLO website, as well as the labels of the COCO dataset . %%capture %%bash curl -s -O https://pjreddie.com/media/files/yolov3.weights curl -o coco_labels.txt https://raw.githubusercontent.com/amikelive/coco-labels/master/coco-labels-2014_2017.txt echo &quot;[[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]&quot; &gt; anchors.json . Declare all the imports we will be needed . import glob import json import struct import matplotlib.pyplot as plt import numpy as np from matplotlib.patches import Rectangle import tensorflow as tf from tensorflow.keras.layers import * from tensorflow.keras.models import * from tensorflow.keras.preprocessing.image import * %matplotlib inline . Second, we define a class to initialize the weight of a YOLO network, it will read the weights file and set the weight of every layer in the model&#39;s 106 layers. . class ModelInitializer: def __init__(self, weight_file): self.offset = 0 self.all_weights = self._load_weights(weight_file) def _load_weights(self, weight_file): with open(weight_file, &#39;rb&#39;) as w_f: major, = struct.unpack(&#39;i&#39;, w_f.read(4)) minor, = struct.unpack(&#39;i&#39;, w_f.read(4)) revision, = struct.unpack(&#39;i&#39;, w_f.read(4)) if (major * 10 + minor) &gt;= 2 and major &lt; 1000 and minor &lt; 1000: w_f.read(8) else: w_f.read(4) binary = w_f.read() return np.frombuffer(binary, dtype=&#39;float32&#39;) def read_bytes(self, size): &quot;&quot;&quot;return number of bytes from the weights file&quot;&quot;&quot; self.offset = self.offset + size return self.all_weights[self.offset-size:self.offset] def init_layer_weights(self, model, i): &quot;&quot;&quot;Initialize the weight of a sepecific layer by its index&quot;&quot;&quot; conv_layer = model.get_layer(f&#39;conv_{i}&#39;) if i not in [81, 93, 105]: norm_layer = model.get_layer(f&#39;bnorm_{i}&#39;) size = np.prod(norm_layer.get_weights()[0].shape) bias = self.read_bytes(size) scale = self.read_bytes(size) mean = self.read_bytes(size) var = self.read_bytes(size) norm_layer.set_weights([scale, bias, mean, var]) if len(conv_layer.get_weights()) &gt; 1: bias = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape)) kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape)) kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape))) kernel = kernel.transpose([2, 3, 1, 0]) conv_layer.set_weights([kernel, bias]) else: kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape)) kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape))) kernel = kernel.transpose([2, 3, 1, 0]) conv_layer.set_weights([kernel]) def init_weights(self, model): &quot;&quot;&quot;Load the weights of each of the model 106 layers&quot;&quot;&quot; for i in range(106): try: self.init_layer_weights(model, i) except ValueError: pass def reset(self): self.offset = 0 . To make our life easy when dealing with the coordinates and any information about the detected onject, we group those metadata information into a class . class BoundBox(object): def __init__(self, x_min, y_min, x_max, y_max, objness=None, classes=None): self.xmin = x_min self.ymin = y_min self.xmax = x_max self.ymax = y_max self.objness = objness self.classes = classes self.label = -1 self.score = -1 def get_label(self): if self.label == -1: self.label = np.argmax(self.classes) return self.label def get_score(self): if self.score == -1: self.score = self.classes[self.get_label()] return self.score . Next, we define some some utility functions that work on the bounding boxes: . calculate_interval_overlap to perform Non Maximal Suppression or shorly NMS (for more details see link) and reduce detections of same object multiple times. | calculate_bbox_iou to calculate the value of Intersection Over Union or shorly IoU (for more details see link) between two bounding boxes | . def calculate_interval_overlap(interval_a, interval_b): x1, x2 = interval_a x3, x4 = interval_b if x3 &lt; x1: if x4 &lt; x1: return 0 else: return min(x2, x4) - x1 else: if x2 &lt; x3: return 0 else: return min(x2, x4) - x3 def calculate_bbox_iou(box1, box2): intersect_w = calculate_interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax]) intersect_h = calculate_interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax]) intersect = intersect_w * intersect_h w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin union = w1 * h1 + w2 * h2 - intersect return float(intersect) / union . Next, we define a helper function to create a YOLO convolutional block, add batch normalization, leaky ReLU activations, and optionally use a skip connection. . def create_conv_block(input, convolutions, skip=True): x = input count = 0 for conv in convolutions: if count == (len(convolutions) - 2) and skip: skip_connection = x count += 1 if conv[&#39;stride&#39;] &gt; 1: x = ZeroPadding2D(((1, 0), (1, 0)))(x) x = Conv2D( conv[&#39;filter&#39;], conv[&#39;kernel&#39;], strides=conv[&#39;stride&#39;], padding=(&#39;valid&#39; if conv[&#39;stride&#39;] &gt; 1 else &#39;same&#39;), name=f&#39;conv_{conv[&quot;layer_idx&quot;]}&#39;, use_bias=(False if conv[&#39;bnorm&#39;] else True) )(x) if conv[&#39;bnorm&#39;]: name = f&#39;bnorm_{conv[&quot;layer_idx&quot;]}&#39; x = BatchNormalization(epsilon=1e-3, name=name)(x) if conv[&#39;leaky&#39;]: name = f&#39;leaky_{conv[&quot;layer_idx&quot;]}&#39; x = LeakyReLU(alpha=0.1, name=name)(x) return Add()([skip_connection, x]) if skip else x . Next, we define a helper function to create the actual architecture of the YOLO model and all of its 106 layers using the previously defined helper function to create individual blocks. . def create_architecture(): input_image = Input(shape=(None, None, 3)) # Layer 0 =&gt; 4 x = create_conv_block(input_image, [ {&#39;filter&#39;: 32, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 0}, {&#39;filter&#39;: 64, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 1}, {&#39;filter&#39;: 32, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 2}, {&#39;filter&#39;: 64, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 3} ]) # Layer 5 =&gt; 8 x = create_conv_block(x, [ {&#39;filter&#39;: 128, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 5}, {&#39;filter&#39;: 64, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 6}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 7} ]) # Layer 9 =&gt; 11 x = create_conv_block(x, [ {&#39;filter&#39;: 64, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 9}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 10} ]) # Layer 12 =&gt; 15 x = create_conv_block(x, [ {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 12}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 13}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 14} ]) # Layer 16 =&gt; 36 for i in range(7): x = create_conv_block(x, [ {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 16 + i * 3}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 17 + i * 3} ]) skip_36 = x # Layer 37 =&gt; 40 x = create_conv_block(x, [ {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 37}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 38}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 39} ]) # Layer 41 =&gt; 61 for i in range(7): x = create_conv_block(x, [ {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 41 + i * 3}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 42 + i * 3} ]) skip_61 = x # Layer 62 =&gt; 65 x = create_conv_block(x, [ {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 62}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 63}, {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 64} ]) # Layer 66 =&gt; 74 for i in range(3): x = create_conv_block(x, [ {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 66 + i * 3}, {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 67 + i * 3} ]) # Layer 75 =&gt; 79 x = create_conv_block(x, [ {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 75}, {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 76}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 77}, {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 78}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 79} ], skip=False) # Layer 80 =&gt; 82 yolo_82 = create_conv_block(x, [ {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 80}, {&#39;filter&#39;: 255, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: False, &#39;leaky&#39;: False, &#39;layer_idx&#39;: 81} ], skip=False) # Layer 83 =&gt; 86 x = create_conv_block(x, [ {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 84}], skip=False) x = UpSampling2D(2)(x) x = Concatenate()([x, skip_61]) # Layer 87 =&gt; 91 x = create_conv_block(x, [ {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 87}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 88}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 89}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 90}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 91} ], skip=False) # Layer 92 =&gt; 94 yolo_94 = create_conv_block(x, [ {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 92}, {&#39;filter&#39;: 255, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: False, &#39;leaky&#39;: False, &#39;layer_idx&#39;: 93} ], skip=False) # Layer 95 =&gt; 98 x = create_conv_block(x, [ {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 96}], skip=False) x = UpSampling2D(2)(x) x = Concatenate()([x, skip_36]) # Layer 99 =&gt; 106 yolo_106 = create_conv_block(x, [ {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 99}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 100}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 101}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 102}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 103}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 104}, {&#39;filter&#39;: 255, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: False, &#39;leaky&#39;: False, &#39;layer_idx&#39;: 105} ], skip=False) return Model(inputs=input_image, outputs=[yolo_82, yolo_94, yolo_106]) . Now, we can finalize the model creation and load its weight. We put all this into the following function: . def create_model(weights_path): model = create_architecture() initializer = ModelInitializer(weights_path) initializer.init_weights(model) model.save(&#39;model.h5&#39;) model = load_model(&#39;model.h5&#39;) return model . Next, we define some utility functions to read the COCO labels and YOLO anchors files. . The YOLO bounding boxes are defined in the context of anchor boxes chosen by the original authors based on the size of objects in the COCO dataset. . def get_labels(path): labels = [] with open(path, &#39;r&#39;) as f: for l in f: labels.append(l.strip()) return labels def get_anchors(path): with open(path, &#39;r&#39;) as f: return json.load(f) . Next, we define a helper class that will be used to wrap the YOLO model and provide an easy way to run inference and return detected objects with their bounding boxes. . class YOLOv3(object): def __init__(self, model, labels, anchors, class_threshold=0.65): self.model = model self.labels = labels self.anchors = anchors self.class_threshold = class_threshold @staticmethod def _sigmoid(x): &quot;&quot;&quot;return the Sigmoid value of a tensor&quot;&quot;&quot; return 1.0 / (1.0 + np.exp(-x)) def _decode_output(self, network_output, anchors, obj_thresh, network_height, network_width): &quot;&quot;&quot;Decode the output of YOLO candidate bounding boxes and class predictions&quot;&quot;&quot; grid_height, grid_width = network_output.shape[:2] nb_box = 3 network_output = network_output.reshape((grid_height, grid_width, nb_box, -1)) boxes = [] network_output[..., :2] = self._sigmoid(network_output[..., :2]) network_output[..., 4:] = self._sigmoid(network_output[..., 4:]) network_output[..., 5:] = (network_output[..., 4][..., np.newaxis] * network_output[..., 5:]) network_output[..., 5:] *= network_output[..., 5:] &gt; obj_thresh for i in range(grid_height * grid_width): r = i / grid_width c = i % grid_width for b in range(nb_box): objectness = network_output[int(r)][int(c)][b][4] if objectness.all() &lt;= obj_thresh: # skip bounding boxes as confidence of object presence is low continue x, y, w, h = network_output[int(r)][int(c)][b][:4] x = (c + x) / grid_width y = (r + y) / grid_height w = (anchors[2 * b] * np.exp(w) / network_width) h = (anchors[2 * b + 1] * np.exp(h) / network_height) classes = network_output[int(r)][c][b][5:] box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes) boxes.append(box) return boxes @staticmethod def _correct_yolo_boxes(boxes, image_height, image_width, network_height, network_width): &quot;&quot;&quot;Rescale the bounding boxes to the dimensions of the original image&quot;&quot;&quot; new_w, new_h = network_width, network_height for i in range(len(boxes)): x_offset = (network_width - new_w) / 2.0 x_offset /= network_width x_scale = float(new_w) / network_width y_offset = (network_height - new_h) / 2.0 y_offset /= network_height y_scale = float(new_h) / network_height boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_width) boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_width) boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_height) boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_height) def _non_max_suppression(self, boxes, nms_thresh): &quot;&quot;&quot;Minimize number of duplicates bounding boxes by apply NMS&quot;&quot;&quot; if len(boxes) &gt; 0: nb_class = len(boxes[0].classes) else: return for c in range(nb_class): sorted_indices = np.argsort([-box.classes[c] for box in boxes]) for i in range(len(sorted_indices)): index_i = sorted_indices[i] if boxes[index_i].classes[c] == 0: continue for j in range(i + 1, len(sorted_indices)): index_j = sorted_indices[j] iou = calculate_bbox_iou(boxes[index_i], boxes[index_j]) if iou &gt;= nms_thresh: boxes[index_j].classes[c] = 0 def _get_boxes(self, boxes): &quot;&quot;&quot;Select bounding boxes containing object with confidence above threshold&quot;&quot;&quot; v_boxes, v_labels, v_scores = [], [], [] for box in boxes: for i in range(len(self.labels)): if box.classes[i] &gt; self.class_threshold: v_boxes.append(box) v_labels.append(self.labels[i]) v_scores.append(box.classes[i] * 100) return v_boxes, v_labels, v_scores def predict(self, image, width, height): &quot;&quot;&quot;Return detected bounding boxes detected by YOLO and applies NMS to remove redundant detections&quot;&quot;&quot; image = np.expand_dims(image, axis=0) preds = self.model.predict(image) boxes = [] for i in range(len(preds)): # decodes the outputs of the network box = self._decode_output(preds[i][0], self.anchors[i], self.class_threshold, 416, 416) boxes.extend(box) # corrects the boxes so that they have proper proportions in relation to the input image self._correct_yolo_boxes(boxes, height, width, 416, 416) self._non_max_suppression(boxes, .5) # select valid bounding boxes valid_boxes, valid_labels, valid_scores = self._get_boxes(boxes) return valid_boxes, valid_labels, valid_scores . We also need a function to plot the predictions: bounding boxes and detection object label, as well as confidence score. . def draw_image_with_boxes(ax, data, v_boxes, v_labels, v_scores): ax.imshow(data) ax.axis(&#39;off&#39;) for i in range(len(v_boxes)): box = v_boxes[i] width = box.xmax - box.xmin height = box.ymax - box.ymin rectangle = Rectangle((box.xmin, box.ymin), width, height, fill=False, color=&#39;yellow&#39;) ax.add_patch(rectangle) label = f&#39;{v_labels[i]} ({v_scores[i]:.2f})&#39; ax.text(box.xmin, box.ymin, label, color=&#39;yellow&#39;) . Now, we can finally create a YOLO v3 model architecture and initialize its weight . Note: you can use tf.keras.utils.plot_model(yolo) to plot the model. . labels = get_labels(&#39;coco_labels.txt&#39;) anchors = get_anchors(&#39;anchors.json&#39;) network = create_model(&#39;yolov3.weights&#39;) model = YOLOv3(network, labels, anchors) . To test the model, let&#39;s download some images (you can use yours) . %%bash mkdir -p images curl -s -o images/bicycle1.jpg https://cdn.pixabay.com/photo/2016/11/30/12/29/bicycle-1872682_960_720.jpg curl -s -o images/bicycle2.jpg https://cdn.pixabay.com/photo/2016/11/22/23/49/cyclists-1851269_960_720.jpg curl -s -o images/animal1.jpg https://cdn.pixabay.com/photo/2014/05/20/21/20/bird-349026_960_720.jpg curl -s -o images/animal2.jpg https://cdn.pixabay.com/photo/2018/05/27/18/19/sparrows-3434123_960_720.jpg curl -s -o images/car1.jpg https://cdn.pixabay.com/photo/2016/02/13/13/11/oldtimer-1197800_960_720.jpg curl -s -o images/car2.jpg https://cdn.pixabay.com/photo/2016/09/11/10/02/renault-juvaquatre-1661009_960_720.jpg . Finally, run the model on each image and draw it to confirm the detections . figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, image_path in enumerate(glob.glob(&#39;images/*.jpg&#39;)): image = load_img(image_path, target_size=(416, 416)) image = img_to_array(image) image = image.astype(&#39;float32&#39;) / 255.0 original_image = load_img(image_path) width, height = original_image.size boxes, labels, scores = model.predict(image, width, height) row, col = int(index / 3), index % 3 draw_image_with_boxes(axis[row, col], original_image, boxes, labels, scores) . You can see that the model is able to acurrately detect the object and their bounding boxes. . To learn more about the YOLO algorithm I encourage you to read the original paper - link .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/object-detection/2021/02/24/Object_Detection_with_YOLOv3.html",
            "relUrl": "/tensorflow/vision/object-detection/2021/02/24/Object_Detection_with_YOLOv3.html",
            "date": " • Feb 24, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Effortless Object Detection with TF Hub",
            "content": "Object Detection is a computer vision task that aims to detect instances of a class (e.g. cars, bicyles, humans, etc.) in an images or videos. Check this post to learn more about Object Detection, the different sub-tasks as well as available model architecture commonly used - link. . This article show how we can quickly perform Object Detection on our own set of images but leveraging freely available models from TF Hub which where pre-trained on this task. . We will be using Faster R-CNN which has a very complex architecture and can be backed by diffrent type of architectures (e.g. VGG). The follow diagram illustrates at a very high level the model architecture, to learn more about this model check this article to learn more about this model - link. . . In our case we will use an Inception V2-backed model similarly to the original architecture in the Faster R-CNN paper. The model we will use is pretrained on the huge COCO dataset and available on TF Hub. . First, we need to downlad TensorFlow Object Detection API and install it. We need to do this to use some of the utility functions provided by this API to quickly visualize the output of Object Detection, i.e. the image along with the detected instances and their bounding boxes. . %%capture %%bash git clone --depth 1 https://github.com/tensorflow/models cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install . . Now we can import TF Hub and TF Object Detection APIs as well as all the other packages we&#39;ll be needing . import glob from io import BytesIO import matplotlib.pyplot as plt import numpy as np import tensorflow as tf import tensorflow_hub as hub from PIL import Image from object_detection.utils import visualization_utils from object_detection.utils.label_map_util import create_category_index_from_labelmap %matplotlib inline . Let&#39;s download the Fatser R-CNN model from TF Hub . MODEL_PATH = (&#39;https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1&#39;) model = hub.load(MODEL_PATH) . To be able to map the output of the model to some meaninful class name, we need to load COCO&#39;s category index as follows . labels_path = &#39;models/research/object_detection/data/mscoco_label_map.pbtxt&#39; CATEGORY_IDX = create_category_index_from_labelmap(labels_path) . We need to define a utility function to load the test image and process them before passing them to the model . def load_image(path): image_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(image_data)) width, height = image.size image = np.array(image.getdata()) image = image.reshape((1, height, width, 3)).astype(&#39;uint8&#39;) return image . The following function will be used to load an image, run the Faster R-CNN model on it and return an image on which the instances are identified with bounding boxes . def get_image_with_boxes(model, image_path): image = load_image(image_path) results = model(image) # Convert the results to NumPy arrays model_output = {k: v.numpy() for k, v in results.items()} # Create a visualization of the detected instances with their boxes, scores, and classes boxes = model_output[&#39;detection_boxes&#39;][0] classes = model_output[&#39;detection_classes&#39;][0].astype(&#39;int&#39;) scores = model_output[&#39;detection_scores&#39;][0] image_with_boxes = image.copy()[0] # draw boxes on the output image, along with the classes and scores visualization_utils.visualize_boxes_and_labels_on_image_array( image=image_with_boxes, boxes=boxes, classes=classes, scores=scores, category_index=CATEGORY_IDX, use_normalized_coordinates=True, max_boxes_to_draw=200, min_score_thresh=0.30, agnostic_mode=False, line_thickness=5 ) return image_with_boxes . Get some images for testing . %%bash mkdir -p images curl -s -o images/bicycle1.jpg https://cdn.pixabay.com/photo/2016/11/30/12/29/bicycle-1872682_960_720.jpg curl -s -o images/bicycle2.jpg https://cdn.pixabay.com/photo/2016/11/22/23/49/cyclists-1851269_960_720.jpg curl -s -o images/animal1.jpg https://cdn.pixabay.com/photo/2014/05/20/21/20/bird-349026_960_720.jpg curl -s -o images/animal2.jpg https://cdn.pixabay.com/photo/2018/05/27/18/19/sparrows-3434123_960_720.jpg curl -s -o images/car1.jpg https://cdn.pixabay.com/photo/2016/02/13/13/11/oldtimer-1197800_960_720.jpg curl -s -o images/car2.jpg https://cdn.pixabay.com/photo/2016/09/11/10/02/renault-juvaquatre-1661009_960_720.jpg . Call the previous get_image_with_boxes function on each image . images = [] image_paths = glob.glob(&#39;images/*&#39;) for path in image_paths: image_with_annotation = get_image_with_boxes(model, path) images.append(image_with_annotation) . Finally we plot the output images and detected bounding boxes, note the quality of the predictions of this model . figure, axis = plt.subplots(2, 3, figsize=(35, 15)) for index, image in enumerate(images): row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].axis(&#39;off&#39;) . We have seen how we can leverage TF Hub and easily use out of box pretrained model with a very complex architecture to perform a very complex task such as Object Detection. Implementing and training Faster R-CNN is not an easy task, but thanks to TF Hub we can effortlessly use the result of the hard work of the Deep Learning community in building and training such models. . . Note: that we were using labels defined in the COCO dataset on which the model was pretrained. If we want to perform Object Detection on our custom dataset or different labels we would need to retrain the model which we will see in another article. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/object-detection/2021/02/11/Object_Detection_TFHub.html",
            "relUrl": "/tensorflow/vision/object-detection/2021/02/11/Object_Detection_TFHub.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Adversarial attack with Fast Gradient Signed Method",
            "content": "Deep Learning have achieved remarquable results in many areas especially vision where today complex models trained on very large datasets are able to overperform domain experts. Yet those same models can be very vulnerable to attacks that slightly change the input data to fool the model. Those attacks are know as Adversial Attacks and can be performed in a variety of ways (see link1, link2). . Those attacks can be serious with devastating results (e.g. fooling a self-driving), so known how to check that your model is robust against them is crutial before rolling the model to production. . . In this article, we will see how to implement one simple and effective Adversial Attack called Fast Gradient Signed Method (FGSM). . FGSM is implemented by performing the following steps: . Pass an legitimage image over a model and capture the gradients | Determining the direction (sign) of the gradient at each pixel of the original image | Using that information create an adversarial pattern in a way to aximize the loss at each pixel | Multiply this pattern by a small scaling value and apply it (i.e. add or substruct) to the original image | The result is an Adversial image that looks to the human eye very similar to the original image | . Although this attack performs small imperceptible perturbations into an image, we will see in action how this technique can easily fool a pre-trained model. . Before starting, make sure to have OpenCV installed as it will be used for performing the perturbations . #collapse !pip install opencv-contrib-python . . Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30) Requirement already satisfied: numpy&gt;=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5) . Import the dependencies . #collapse import cv2 import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.applications.nasnet import * from tensorflow.keras.losses import CategoricalCrossentropy . . Load the pre-trained model (in this case NASNetMobile) and freeze its weights so they are not updated after a forward pass. . pretrained_model = NASNetMobile(include_top=True, weights=&#39;imagenet&#39;) pretrained_model.trainable = False . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile.h5 24231936/24227760 [==============================] - 0s 0us/step 24240128/24227760 [==============================] - 0s 0us/step . We need to define a function to preprocess an image with same preprocessing steps performed on the images used to train the target model (e.g. resizing) . def preprocess(image, target_shape): image = tf.cast(image, tf.float32) image = tf.image.resize(image, target_shape) image = preprocess_input(image) image = image[None, :, :, :] return image . We also need to define a function that turns the model output probabilities into actual labels . def get_imagenet_label(probabilities): return decode_predictions(probabilities, top=1)[0][0] . Define a function to create the adversarial pattern that will be used later to generate the adversial image . def generate_adv_pattern(model, input_image, input_label, loss_function): with tf.GradientTape() as tape: tape.watch(input_image) prediction = model(input_image) loss = loss_function(input_label, prediction) gradient = tape.gradient(loss, input_image) signed_gradient = tf.sign(gradient) return signed_gradient . The pattern is nothing but a tensor with the sign of the gradient in each element. i.e., signed_gradient is: . -1 for negative gradients (i.e. value below 0) | 1 for positive gradients (i.e. value above 0) | 0 if the gradient is 0. | . We need a legitimate image so we can apply perturbations on it, you can download a test image on your own or use this Pug image . !curl https://image.freepik.com/free-vector/angry-dog-pug-prisoner-graphic-illustration_41984-29.jpg -o dog.jpg . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 102k 100 102k 0 0 1138k 0 --:--:-- --:--:-- --:--:-- 1138k . Pass the image through the pretrained model to get output probabilities . image = tf.io.read_file(&#39;dog.jpg&#39;) image = tf.image.decode_jpeg(image) image = preprocess(image, pretrained_model.input.shape[1:-1]) image_probs = pretrained_model.predict(image) . We need to One-hot encode the ground truth label of the original image, then use it to generate an adversarial pattern . cce_loss = CategoricalCrossentropy() pug_index = 254 label = tf.one_hot(pug_index, image_probs.shape[-1]) label = tf.reshape(label, (1, image_probs.shape[-1])) disturbances = generate_adv_pattern(pretrained_model, image, label, cce_loss) . . Note: if you want try this techniques on a different image of a different class than pug then look up the corresponding class index in this imagenet class indices - link . Next, we define a utility function to generate an adversial version of an image based on a disturbance/noise and a scalar . def generate_adv_image(image, epsilon, disturbances): corrupted_image = image + epsilon * disturbances return tf.clip_by_value(corrupted_image, -1, 1) . We also, need another utility function to ensure before plotting that the input tensor in the [0, 255] range, as well as in BGR space, which is the one used by OpenCV . def postprocess_adv_image(adv_image): adv_image = adv_image.numpy()[0] * 0.5 + 0.5 adv_image = (adv_image * 255).astype(&#39;uint8&#39;) return cv2.cvtColor(adv_image, cv2.COLOR_RGB2BGR) . We can use the previous utility function to show what the noise looks like before applying it to the original image . disturbed_image = postprocess_adv_image(disturbances) plt.imshow(disturbed_image) plt.axis(&#39;off&#39;) plt.title(&quot;noise&quot;); . Finally, we can put all the previous steps together to perform a series of adversarial attacks different values of epsilon (which is used as noise multiplier). . After that we disply the corresponding adversial example as well the predicted label and confidence provided by the victim model . figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, epsilon in enumerate([0, 0.005, 0.01, 0.1, 0.15, 0.2]): adv_image = generate_adv_image(image, epsilon, disturbances) prediction = pretrained_model.predict(adv_image) _, label, confidence = get_imagenet_label(prediction) adv_image = postprocess_adv_image(adv_image) confidencePct = confidence * 100 title = f&#39;Epsilon = {epsilon:.3f}, {label} ({confidencePct:.2f}%)&#39; row, col = int(index / 3), index % 3 axis[row, col].imshow(adv_image) axis[row, col].set_title(title) axis[row, col].axis(&#39;off&#39;) . Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json 40960/35363 [==================================] - 0s 0us/step 49152/35363 [=========================================] - 0s 0us/step . As you can be see from the output images and the resulting classification, an imperceptible variation in the pixel values produced a drastically different response from the network. . For epsilon = 0 (no attack), the label is Pug with a 60.18% confidence | When epsilon = 0.005 (a very small perturbation), the label changes to Muzzle, with a 98.46% confidence! | The situation gets worse as we increase the magnitude of epsilon. | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/generative/adversarial/2021/01/29/Gradient-Signed-Method.html",
            "relUrl": "/tensorflow/generative/adversarial/2021/01/29/Gradient-Signed-Method.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "DeepDream with Tensorflow",
            "content": "In an effort to understand the inners of deep neural networks and what information those models learn after been trained on a specific task, a team at Google come up with what&#39;s know today as DeepDream. The experiment results were interesting, it appears that neural networks trained to classify an image (i.e. discriminate between different classes) contains enough information to generate new images that can be artistic. . . In this notebook we will implement DeepDream in Tensorflow from scratch and test it on couple of images. . The DeepDream model is simply built on a backbone model trained on imagenet and the output of the model will be few activation layers picked from this backbone model. Then we run an image through this model, compute the gradients with respect to the activations of those output layers, then modify the original image to increase the magnitude of the activations which as a result will magnify the patterns in the image. . In our case, the backbone model is Inception V3 (which you can read more about it here). The following driagram shows an overview of this model architecture: . . #collapse import numpy as np import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.applications.inception_v3 import * from tensorflow.keras.preprocessing.image import * from tqdm import tqdm import matplotlib.pyplot as plt . . To create the DeepDream model, we define the following helper function that uses InceptionV3 from TF Hub and uses the input layers as output of the model. Note that by default we are picking ramdom activation layers from the InceptionV3 model. . def create_model(layers=None): if not layers: layers = [&#39;mixed3&#39;, &#39;mixed5&#39;] base = InceptionV3(weights=&#39;imagenet&#39;, include_top=False) outputs = [base.get_layer(name).output for name in layers] return Model(base.input, outputs) . We need to define few utils functions that we will use to process the images, for example scaling an image by a factor or converting tensor image into a numpy array. . def convert_tensor_to_nparray(image): image = 255 * (image + 1.0) / 2.0 image = tf.cast(image, tf.uint8) image = np.array(image) return image def scale_image(image, base_shape, scale, factor): new_shape = tf.cast(base_shape * (scale **factor), tf.int32) image = tf.image.resize(image,new_shape).numpy() image = preprocess_input(image) image = tf.convert_to_tensor(image) return image . Next, we define a function to calculate the loss which is simly the average of the activations resulting from doing a forward pass with the input image. . def calculate_loss(model, image): image_batch = tf.expand_dims(image, axis=0) activations = model(image_batch) if len(activations) == 1: activations = [activations] losses = [] for activation in activations: loss = tf.math.reduce_mean(activation) losses.append(loss) total_loss = tf.reduce_sum(losses) return total_loss . To calculate the gradients, we need to perform a forward pass inside a tf.GradientTape(), after that we simply update the image to maximize the activations in the next run. . Note how we are using the tf.function annotation which will improve the performance significantly. . @tf.function def forward_pass(model, image, steps, step_size): loss = tf.constant(0.0) for _ in range(steps): with tf.GradientTape() as tape: tape.watch(image) loss = calculate_loss(model, image) gradients = tape.gradient(loss, image) gradients /= tf.math.reduce_std(gradients) + 1e-8 image = image + gradients * step_size image = tf.clip_by_value(image, -1, 1) return image, loss . All the previous functions are combined and used in the following funciton which will take an input image and a model, and construct the final dreaming looking picture. . The other input parameters to this function have the following purpose: . octave_scale the scale by which we&#39;ll increase the size of an image | octave_power_factors the factor that will be applied as a power to the previous scale parameter. | steps the number of iteration we run the image over the deepdream model | step_size will be used to scale the gradients before adding them to the image | . def dream(dreamer_model, image, octave_scale=1.30, octave_power_factors=None, steps=100, step_size=0.01): if not octave_power_factors: octave_power_factors = [*range(-2, 3)] image = tf.constant(np.array(image)) base_shape = tf.shape(image)[:-1] base_shape = tf.cast(base_shape, tf.float32) steps = tf.constant(steps) step_size = tf.constant(tf.convert_to_tensor(step_size)) for factor in octave_power_factors: image = scale_image(image, base_shape, octave_scale, factor) image, _ = forward_pass(dreamer_model, image, steps, step_size) image = convert_tensor_to_nparray(image) base_shape = tf.cast(base_shape, tf.int32) image = tf.image.resize(image, base_shape) image = tf.image.convert_image_dtype(image /255.0,dtype=tf.uint8) image = np.array(image) return np.array(image) . Now we can apply this DeepDream model to an image, you can pick any one you like. . !curl https://miro.medium.com/max/1750/1*E-S7Y80jIFuZ03xyc89fnA.jpeg -o image.jpeg . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 68250 100 68250 0 0 812k 0 --:--:-- --:--:-- --:--:-- 812k . def load_image(path): image = load_img(path) image = img_to_array(image) return image . def show_image(image): plt.imshow(image) plt.show() . original_image = load_image(&#39;image.jpeg&#39;) show_image(original_image / 255.0) . First, lets try the image with all default parameters, and activation layers . model = create_model() output_image = dream(model, original_image) show_image(output_image) . WARNING:tensorflow:5 out of the last 5 calls to &lt;function forward_pass at 0x7f095a587a70&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. . Let&#39;s try with different activation layers . Note: the first layers tend to learn basic patterns (e.g. lines and shapes), while layers closer to the output learn more complex patterns as they combine the previous basic patterns. . model = create_model([&#39;mixed2&#39;, &#39;mixed5&#39;, &#39;mixed7&#39;]) output_image = dream(model, original_image) show_image(output_image) . WARNING:tensorflow:6 out of the last 6 calls to &lt;function forward_pass at 0x7f095a587a70&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. . The result is a softer image as a result of adding more layers. . Finally, let&#39;s try some custom octaves power factors . model = create_model() output_image = dream(model, original_image, octave_power_factors=[-3, -1, 0, 3]) show_image(output_image) . The resulting image seem to have less noise and more heterogeneous patterns, a mixture of both high- and low-level patterns, as well as a better color distribution. . As an exercise, try different parameters and you will see that the results vary widely: . Play with different step_size values, a big value will result in much noise added to the original images | Use higher layers to obtain pictures with less noise and more nuanced patterns. | Use more octaves, which will result into more images passed to the model at different scales. | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/artistic/generative/2021/01/18/DeepDream-Tensorflow.html",
            "relUrl": "/tensorflow/artistic/generative/2021/01/18/DeepDream-Tensorflow.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Handling Imbalanced Datasets",
            "content": ". %%capture %%bash pip install imbalanced-learn . from collections import Counter import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import make_classification from sklearn.svm import LinearSVC from imblearn.pipeline import make_pipeline from imblearn.base import BaseSampler from imblearn.under_sampling import RandomUnderSampler from imblearn.over_sampling import (SMOTE, RandomOverSampler) from imblearn.combine import SMOTEENN, SMOTETomek import warnings warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) . Helper functions . The following function will be used to create toy dataset. It using the make_classification from scikit-learn but fixing some parameters. . def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3, class_sep=0.8, n_clusters=1): return make_classification(n_samples=n_samples, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=n_classes, n_clusters_per_class=n_clusters, weights=list(weights), class_sep=class_sep, random_state=0) . The following function will be used to plot the sample space after resampling to illustrate the characterisitic of an algorithm. . def plot_resampling(X, y, sampling, ax): X_res, y_res = sampling.fit_resample(X, y) ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor=&#39;k&#39;) # make nice plotting ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.get_xaxis().tick_bottom() ax.get_yaxis().tick_left() ax.spines[&#39;left&#39;].set_position((&#39;outward&#39;, 10)) ax.spines[&#39;bottom&#39;].set_position((&#39;outward&#39;, 10)) return Counter(y_res) . The following function will be used to plot the decision function of a classifier given some data. . def plot_decision_function(X, y, clf, ax): plot_step = 0.02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, alpha=0.4) ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor=&#39;k&#39;) . Influence of the balancing ratio . We will first illustrate the influence of the balancing ratio on some toy data using a linear SVM classifier. Greater is the difference between the number of samples in each class, poorer are the classfication results. . #collapse fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) ax_arr = (ax1, ax2, ax3, ax4) weights_arr = ((0.01, 0.01, 0.98), (0.01, 0.05, 0.94), (0.2, 0.1, 0.7), (0.33, 0.33, 0.33)) for ax, weights in zip(ax_arr, weights_arr): X, y = create_dataset(n_samples=1000, weights=weights) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax) ax.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) fig.tight_layout() . . Under-sampling . Under-sampling by selecting existing samples . There are two major groups of selection algorithms: . the controlled under-sampling methods and | the cleaning under-sampling methods. | With the controlled under-sampling methods, the number of samples to be selected can be specified. RandomUnderSampler is the most naive way of performing such selection by randomly selecting a given number of samples by the targetted class. . #collapse fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6)) X, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax1) ax1.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) sampler = RandomUnderSampler(random_state=0) clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_decision_function(X, y, clf, ax2) ax2.set_title(&#39;Decision function for {}&#39;.format(sampler.__class__.__name__)) plot_resampling(X, y, sampler, ax3) ax3.set_title(&#39;Resampling using {}&#39;.format(sampler.__class__.__name__)) fig.tight_layout() . . Over-sampling . Random over-sampling . Random over-sampling with RandomOverSampler can be used to repeat some samples and balance the number of samples between the dataset. It can be seen that with this trivial approach the boundary decision is already less biaised toward the majority class. . #collapse fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7)) X, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94)) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax1) ax1.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) pipe = make_pipeline(RandomOverSampler(random_state=0), LinearSVC()) pipe.fit(X, y) plot_decision_function(X, y, pipe, ax2) ax2.set_title(&#39;Decision function for RandomOverSampler&#39;) fig.tight_layout() . . /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) . More advanced over-sampling . Instead of repeating the same samples when over-sampling, we can use some specific heuristic instead like SMOTE or ADASYN. . #collapse # Make an identity sampler for illustrations class FakeSampler(BaseSampler): _sampling_type = &#39;bypass&#39; def _fit_resample(self, X, y): return X, y fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15)) X, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94)) sampler = FakeSampler() clf = make_pipeline(sampler, LinearSVC()) plot_resampling(X, y, sampler, ax1) ax1.set_title(&#39;Original data - y={}&#39;.format(Counter(y))) ax_arr = (ax2, ax3, ax4) for ax, sampler in zip(ax_arr, (RandomOverSampler(random_state=0), SMOTE(random_state=0), ADASYN(random_state=0))): clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_resampling(X, y, sampler, ax) ax.set_title(&#39;Resampling using {}&#39;.format(sampler.__class__.__name__)) fig.tight_layout() . . Illustration of the sample generation in the over-sampling algorithm . #collapse rng = np.random.RandomState(18) f, ax = plt.subplots(1, 1, figsize=(8, 8)) # generate some data points y = np.array([3.65284, 3.52623, 3.51468, 3.22199, 3.21]) z = np.array([0.43, 0.45, 0.6, 0.4, 0.211]) y_2 = np.array([3.3, 3.6]) z_2 = np.array([0.58, 0.34]) # plot the majority and minority samples ax.scatter(z, y, label=&#39;Minority class&#39;, s=100) ax.scatter(z_2, y_2, label=&#39;Majority class&#39;, s=100) idx = rng.randint(len(y), size=2) annotation = [r&#39;$x_i$&#39;, r&#39;$x_{zi}$&#39;] for a, i in zip(annotation, idx): ax.annotate(a, (z[i], y[i]), xytext=tuple([z[i] + 0.01, y[i] + 0.005]), fontsize=15) # draw the circle in which the new sample will generated radius = np.sqrt((z[idx[0]] - z[idx[1]]) ** 2 + (y[idx[0]] - y[idx[1]]) ** 2) circle = plt.Circle((z[idx[0]], y[idx[0]]), radius=radius, alpha=0.2) ax.add_artist(circle) # plot the line on which the sample will be generated ax.plot(z[idx], y[idx], &#39;--&#39;, alpha=0.5) # create and plot the new sample step = rng.uniform() y_gen = y[idx[0]] + step * (y[idx[1]] - y[idx[0]]) z_gen = z[idx[0]] + step * (z[idx[1]] - z[idx[0]]) ax.scatter(z_gen, y_gen, s=100) ax.annotate(r&#39;$x_{new}$&#39;, (z_gen, y_gen), xytext=tuple([z_gen + 0.01, y_gen + 0.005]), fontsize=15) # make the plot nicer with legend and label ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.get_xaxis().tick_bottom() ax.get_yaxis().tick_left() ax.spines[&#39;left&#39;].set_position((&#39;outward&#39;, 10)) ax.spines[&#39;bottom&#39;].set_position((&#39;outward&#39;, 10)) ax.set_xlim([0.2, 0.7]) ax.set_ylim([3.2, 3.7]) plt.xlabel(r&#39;$X_1$&#39;) plt.ylabel(r&#39;$X_2$&#39;) plt.legend() plt.tight_layout() plt.show() . . Automatically created module for IPython interactive environment . Combine-sampling . Comparison of the combination of over- and under-sampling algorithms . This example shows the effect of applying an under-sampling algorithms after SMOTE over-sampling. In the literature, Tomek&#39;s link SMOTETomek and edited nearest neighbours SMOTEENN are the two methods which have been used and are available in imbalanced-learn. . #collapse fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 25)) X, y = create_dataset(n_samples=1000, weights=(0.1, 0.2, 0.7)) ax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6)) for ax, sampler in zip(ax_arr, ( SMOTE(random_state=0), SMOTEENN(random_state=0), SMOTETomek(random_state=0))): clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_decision_function(X, y, clf, ax[0]) ax[0].set_title(&#39;Decision function for {}&#39;.format( sampler.__class__.__name__)) plot_resampling(X, y, sampler, ax[1]) ax[1].set_title(&#39;Resampling using {}&#39;.format( sampler.__class__.__name__)) fig.tight_layout() plt.show() . . .",
            "url": "https://dzlab.github.io/notebooks/sklearn/classification/sampling/2020/10/31/Handling_Imbalanced_Datasets.html",
            "relUrl": "/sklearn/classification/sampling/2020/10/31/Handling_Imbalanced_Datasets.html",
            "date": " • Oct 31, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Preprocessing Structured data in TF 2.3",
            "content": "In TF 2.3, Keras adds new preprocessing layers for image, text and strucured data. The following notebook explores those new layers for dealing with Structured data. . For a complete example of how to use the new preprocessing layer for Structured data check the Keras example - link. . Structured data . Generate some random data for playing with and seeing what is the output of the preprocessing layers. . xdf = pd.DataFrame({ &#39;categorical_string&#39;: [&#39;LOW&#39;, &#39;HIGH&#39;, &#39;HIGH&#39;, &#39;MEDIUM&#39;], &#39;categorical_integer_1&#39;: [1, 0, 1, 0], &#39;categorical_integer_2&#39;: [1, 2, 3, 4], &#39;numerical_1&#39;: [2.3, 0.2, 1.9, 5.8], &#39;numerical_2&#39;: [16, 32, 8, 60] }) ydf = pd.DataFrame({&#39;target&#39;: [0, 0, 0, 1]}) ds = tf.data.Dataset.from_tensor_slices((dict(xdf), ydf)) for x, y in ds.take(1): print(&#39;X:&#39;, x) print(&#39;y:&#39;, y) . X: {&#39;categorical_string&#39;: &lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;cat1&#39;&gt;, &#39;categorical_integer_1&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;, &#39;categorical_integer_2&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;, &#39;numerical_1&#39;: &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.3&gt;, &#39;numerical_2&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=16&gt;} y: tf.Tensor([0], shape=(1,), dtype=int64) . from tensorflow.keras.layers.experimental.preprocessing import Normalization from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding from tensorflow.keras.layers.experimental.preprocessing import StringLookup . Pre-processing Numercial columns . Preprocessing helper function to encode numercial features, e.g. 0.1, 0.2, etc. . def create_numerical_encoder(dataset, name): # Create a Normalization layer for our feature normalizer = Normalization() # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the statistics of the data normalizer.adapt(feature_ds) return normalizer . # Apply normalization to a numerical feature normalizer = create_numerical_encoder(ds, &#39;numerical_1&#39;) normalizer.apply(xdf[name].values) . &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy= array([[-0.7615536], [-1.2528784], [-0.7615536], [-1.2528784]], dtype=float32)&gt; . Pre-processing Integer categorical columns . Preprocessing helper function to encode integer categorical features, e.g. 1, 2, 3 . def create_integer_categorical_encoder(dataset, name): # Create a CategoryEncoding for our integer indices encoder = CategoryEncoding(output_mode=&quot;binary&quot;) # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the space of possible indices encoder.adapt(feature_ds) return encoder . # Apply one-hot encoding to an integer categorical feature encoder1 = create_integer_categorical_encoder(ds, &#39;categorical_integer_1&#39;) encoder1.apply(xdf[&#39;categorical_integer_1&#39;].values) . &lt;tf.Tensor: shape=(4, 2), dtype=float32, numpy= array([[0., 1.], [1., 0.], [0., 1.], [1., 0.]], dtype=float32)&gt; . # Apply one-hot encoding to an integer categorical feature encoder2 = create_integer_categorical_encoder(ds, &#39;categorical_integer_2&#39;) encoder2.apply(xdf[&#39;categorical_integer_2&#39;].values) . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]], dtype=float32)&gt; . Pre-processing String categorical columns . Preprocessing helper function to encode string categorical features, e.g. LOW, HIGH, MEDIUM. . This will applying the following to the input feature: . Create a token to index lookup table | Apply one-hot encoding to the tokens indices | def create_string_categorical_encoder(dataset, name): # Create a StringLookup layer which will turn strings into integer indices index = StringLookup() # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the set of possible string values and assign them a fixed integer index index.adapt(feature_ds) # Create a CategoryEncoding for our integer indices encoder = CategoryEncoding(output_mode=&quot;binary&quot;) # Prepare a dataset of indices feature_ds = feature_ds.map(index) # Learn the space of possible indices encoder.adapt(feature_ds) return index, encoder . # Apply one-hot encoding to an integer categorical feature indexer, encoder3 = create_string_categorical_encoder(ds, &#39;categorical_string&#39;) # Turn the string input into integer indices indices = indexer.apply(xdf[&#39;categorical_string&#39;].values) # Apply one-hot encoding to our indices encoder3.apply(indices) . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[0., 0., 0., 0., 1.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.]], dtype=float32)&gt; . Notice that the string categorical column was hot encoded into 5 tokens whereas in the input dataframe there is only 3 unique values. This is because the indexer adds 2 more tokens. See the vocabulary: . indexer.get_vocabulary() . [&#39;&#39;, &#39;[UNK]&#39;, &#39;cat2&#39;, &#39;cat3&#39;, &#39;cat1&#39;] .",
            "url": "https://dzlab.github.io/notebooks/2020/08/02/Preprocessing_structured_data_in_TF_2_3.html",
            "relUrl": "/2020/08/02/Preprocessing_structured_data_in_TF_2_3.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Annotation with TensorFlow Object Detection API",
            "content": "import matplotlib import matplotlib.pyplot as plt import numpy as np from PIL import Image from six import BytesIO from pathlib import Path import tensorflow as tf %matplotlib inline . Install Object Detection API . !git clone --depth 1 https://github.com/tensorflow/models . Cloning into &#39;models&#39;... remote: Enumerating objects: 2797, done. remote: Counting objects: 100% (2797/2797), done. remote: Compressing objects: 100% (2439/2439), done. remote: Total 2797 (delta 563), reused 1405 (delta 322), pack-reused 0 Receiving objects: 100% (2797/2797), 57.73 MiB | 31.67 MiB/s, done. Resolving deltas: 100% (563/563), done. . # Install the Object Detection API %%bash cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install -q . . object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used. . from object_detection.utils import colab_utils from object_detection.utils import visualization_utils as viz_utils . Download data for annotation . Download an image dataset to annotate, for instance The Oxford-IIIT Pet Dataset (link) . %%bash curl -O https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz tar xzf images.tar.gz . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 755M 100 755M 0 0 29.8M 0 0:00:25 0:00:25 --:--:-- 31.4M . paths = list([str(p) for p in Path(&#39;images&#39;).glob(&#39;*&#39;)]) . Utility method to load an image from path into a uint8 numpy array with shape (height, width, channels), where channels=3 for RGB. . def load_image_into_numpy_array(path): img_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(img_data)) (im_width, im_height) = image.size image_np = np.array(image.getdata(), dtype=np.uint8) return image_np.reshape((im_height, im_width, 3)) . For testing select a random subset of the images (we don&#39;t want load all images) . sample_size = 10 sample_paths = [paths[np.random.randint(len(paths))] for i in range(10)] . Annotate images . Load the selected random images into numpy arrays . images_np = [load_image_into_numpy_array(str(p)) for p in sample_paths] . boxes = [] colab_utils.annotate(images_np, box_storage_pointer=boxes) . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . Define the indexes for the categories . category_index = { 0: {&#39;id&#39;: 0, &#39;name&#39;: &#39;dog&#39;}, 1: {&#39;id&#39;: 1, &#39;name&#39;: &#39;cat&#39;} } . Inspect the annotations . Wrapper function to visualize the original image along with the best detected box. It takes are arguments: . image_np: uint8 numpy array with shape (img_height, img_width, 3) | boxes: a numpy array of shape [N, 4] | classes: a numpy array of shape [N]. Note that class indices are 1-based, and match the keys in the label map. | scores: a numpy array of shape [N] or None. If scores=None, then this function assumes that the boxes to be plotted are groundtruth boxes and plot all boxes as black with no classes or scores. | category_index: a dict containing category dictionaries (each holding category index id and category name name) keyed by category indices. | figsize: (optional) size for the figure. | image_name: (optional) name for the image file. | . def plot_detections(image_np, boxes, classes, scores, category_index, figsize=(12, 16), image_name=None): image_np_with_annotations = image_np.copy() viz_utils.visualize_boxes_and_labels_on_image_array( image_np_with_annotations, boxes, classes, scores, category_index, use_normalized_coordinates=True, min_score_thresh=0.8) if image_name: plt.imsave(image_name, image_np_with_annotations) else: plt.imshow(image_np_with_annotations) . I manually inspected the images (that&#39;s the 100% scores below) to get the class for each one, note that: . 0 is for a cat image | 1 is for a dog image | . classes = [ np.ones(shape=(1), dtype=np.int32), np.ones(shape=(1), dtype=np.int32), np.zeros(shape=(1), dtype=np.int32), np.ones(shape=(1), dtype=np.int32), np.zeros(shape=(1), dtype=np.int32) ] # give boxes a score of 100% scores = np.array([1.0], dtype=np.float32) . Vizualise the images with their bounding boxes . plt.figure(figsize=(30, 15)) for idx in range(5): plt.subplot(2, 3, idx+1) plot_detections(images_np[idx], boxes[idx], classes[idx], scores, category_index) plt.show() .",
            "url": "https://dzlab.github.io/notebooks/2020/07/19/Image_Annotation_on_Colab.html",
            "relUrl": "/2020/07/19/Image_Annotation_on_Colab.html",
            "date": " • Jul 19, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Captum PyTorch Vision Example",
            "content": "Captum (translates to comprehension in Latin) is an open source library for model interpretability. It helps model developers understand which features are contributing to their model’s output. It implements state-of-the-art interpretability algorithms in PyTorch, and provide them as an easy to use API. . The rest of this notebook illustrates how to use this library to interpret a fastai v2 based image classification model. . Setup . %%capture %%bash pip install fastai2 pip install psutil pip install captum . from matplotlib.colors import LinearSegmentedColormap from fastai2.vision.all import * from captum.attr import IntegratedGradients from captum.attr import GradientShap from captum.attr import Occlusion from captum.attr import NoiseTunnel from captum.attr import visualization as viz . Data . Download data for training an image classification model . path = untar_data(URLs.PETS)/&#39;images&#39; imgs = get_image_files(path) def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, imgs, valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) . Model . Fine tune an Imagenet-based model on the new images dataset. . learn = cnn_learner(dls, resnet34, metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.169865 | 0.020558 | 0.004736 | 00:23 | . epoch train_loss valid_loss error_rate time . 0 | 0.060073 | 0.026544 | 0.008119 | 00:24 | . Basic interpertation of the model prediected classes vs. actual ones. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Visualize top losses, e.g. to check if the images themselves are correctly annotated. . interp.plot_top_losses(5, nrows=1) . Store/Restore the fine tuned model . learn.export(&#39;/tmp/model.pkl&#39;) learn_inf = load_learner(&#39;/tmp/model.pkl&#39;) . Select a random image and plot it . idx = random.randint(0, len(imgs)) . image = PILImage.create(imgs[idx]) image . image = learn_inf.dls.after_item(image) image = learn_inf.dls.after_batch(image) . pred,pred_idx,probs = learn_inf.predict(imgs[idx]) pred, pred_idx, probs . (&#39;False&#39;, tensor(0), tensor([9.9998e-01, 2.0485e-05])) . Interpretability . Let&#39;s use Captum.ai to interpret the model predictions and to have a visual on where the network focused more in the input image. . Gradient-based attribution . Integrated Gradients is an interpretaility technique based on the approximation of integral gradients. The basic implementation works as followss: . Given as input target image and a baseline image (usually a black image), generate multiple images between both starting from darker to lighter. | Do forward pass with each of those images to predict a class and calculate the gradient. | Approximate the integral of the gradients of all those images | . The following example, illustrates how to use Captum IntegratedGradients to compute the attributions using Integrated Gradients and visualize them on the target image. . integrated_gradients = IntegratedGradients(learn_inf.model) attr_ig = integrated_gradients.attribute(image, target=pred_idx, n_steps=200) . transposed_attr_ig = np.transpose(attr_ig.squeeze().numpy(), (1,2,0)) transposed_image = np.transpose(image.squeeze().numpy(), (1,2,0)) . default_cmap = LinearSegmentedColormap.from_list(&#39;custom blue&#39;, [(0, &#39;#ffffff&#39;), (0.25, &#39;#000000&#39;), (1, &#39;#000000&#39;)], N=256) _ = viz.visualize_image_attr(transposed_attr_ig, transposed_image, method=&#39;heat_map&#39;, cmap=default_cmap, show_colorbar=True, sign=&#39;positive&#39;, outlier_perc=1) . For a better visual of the attribution, the images between baseline and target are sampled using a noise tunnel (by adding gaussian noise). And when the gradients are calulcated, we smoothe them by calculating their mean squared. . noise_tunnel = NoiseTunnel(integrated_gradients) attributions_ig_nt = noise_tunnel.attribute(image, n_samples=10, nt_type=&#39;smoothgrad_sq&#39;, target=pred_idx) transposed_attr_ig_nt = np.transpose(attributions_ig_nt.squeeze().numpy(), (1,2,0)) _ = viz.visualize_image_attr_multiple(transposed_attr_ig_nt, transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], cmap=default_cmap, show_colorbar=True) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Occlusion-based attribution . Occlusion-based attribution is a different interepretation technique based on perturbing parts of the original image (e.g. by zeroing) and computing how this affects the model decision. This technique is implemented by: . Slide a window of shape (h, w) on the target image with a stride s | Replace the window with a baseline (e.g. with black) and qunatify the effect on model decision. | Repeat previous steps until all of the target image is covered. | . Simiarly to convolution, this technique can become very slow when used in large models and large input images. . As a first exercise, we run a sliding window of size 15x15 and a stride of 8 along both image dimensions. For each window, we occlude the image with a baseline value of 0. . occlusion = Occlusion(learn_inf.model) attr_occ = occlusion.attribute(image, strides = (3, 8, 8), target=pred_idx, sliding_window_shapes=(3,15, 15), baselines=0) . _ = viz.visualize_image_attr_multiple(np.transpose(attr_occ.squeeze().numpy(), (1,2,0)), transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], show_colorbar=True, outlier_perc=2, ) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Let&#39;s try different window shape and sliding window and visuzalize the result, by rescaling back to the shape of the original image. . Experimenting with different windows is important because: . Different shape may lead to a significaly different result. | Larger windows is useful when the input image presents some local coherence. | . In this case, we run a sliding window of size 60x60 and a stride of 50 along both image dimensions. For each window, we occlude the image with a baseline value of 0. . occlusion = Occlusion(learn_inf.model) attr_occ = occlusion.attribute(image, strides = (3, 50, 50), target=pred_idx, sliding_window_shapes=(3,60, 60), baselines=0) _ = viz.visualize_image_attr_multiple(np.transpose(attr_occ.squeeze().numpy(), (1,2,0)), transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], show_colorbar=True, outlier_perc=2, ) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . References . Axiomatic Attribution for Deep Networks - link | Towards better understanding of gradient-based attribution methods for Deep Neural Networks - link | .",
            "url": "https://dzlab.github.io/notebooks/jupyter/2020/04/18/Captum_PyTorch_Vision_Example.html",
            "relUrl": "/jupyter/2020/04/18/Captum_PyTorch_Vision_Example.html",
            "date": " • Apr 18, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Mask-RCNN Tensorflow v1 image examples",
            "content": "Project Setup . Make sure we&#39;re running TensorFlow v1 . try: %tensorflow_version 1.x except Exception: pass . TensorFlow 1.x selected. . Install Mask-RCNN model . %%capture %%bash pip install -U git+https://github.com/matterport/Mask_RCNN . Download weights of pretrained Mask-RCNN . !curl -L -o mask_rcnn_balloon.h5 https://github.com/matterport/Mask_RCNN/releases/download/v2.1/mask_rcnn_balloon.h5?raw=true . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 611 100 611 0 0 2246 0 --:--:-- --:--:-- --:--:-- 2254 100 244M 100 244M 0 0 40.0M 0 0:00:06 0:00:06 --:--:-- 47.2M . import cv2 import math import numpy as np import matplotlib.pyplot as plt import os import sys from mrcnn import utils from mrcnn import model as modellib from mrcnn.config import Config from PIL import Image plt.rcParams[&quot;figure.figsize&quot;]= (10,10) np.set_printoptions(precision=3) . Mask-RCNN setup . # Load the pre-trained model data ROOT_DIR = os.getcwd() MODEL_DIR = os.path.join(ROOT_DIR, &quot;logs&quot;) COCO_MODEL_PATH = os.path.join(ROOT_DIR, &quot;mask_rcnn_coco.h5&quot;) if not os.path.exists(COCO_MODEL_PATH): utils.download_trained_weights(COCO_MODEL_PATH) . Downloading pretrained model to /content/mask_rcnn_coco.h5 ... ... done downloading pretrained model! . class InferenceConfig(Config): &quot;&quot;&quot;Configuration for training on MS COCO. Derives from the base Config class and overrides values specific to the COCO dataset. &quot;&quot;&quot; # Give the configuration a recognizable name NAME = &quot;coco&quot; # Number of images to train with on each GPU. A 12GB GPU can typically # handle 2 images of 1024x1024px. IMAGES_PER_GPU = 1 # Uncomment to train on 8 GPUs (default is 1) GPU_COUNT = 1 # Number of classes (including background) NUM_CLASSES = 1 + 80 # COCO has 80 classes . %%capture # COCO dataset object names model = modellib.MaskRCNN( mode=&quot;inference&quot;, model_dir=MODEL_DIR, config=InferenceConfig() ) model.load_weights(COCO_MODEL_PATH, by_name=True) class_names = [ &#39;BG&#39;, &#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, &#39;motorcycle&#39;, &#39;airplane&#39;, &#39;bus&#39;, &#39;train&#39;, &#39;truck&#39;, &#39;boat&#39;, &#39;traffic light&#39;, &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;parking meter&#39;, &#39;bench&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;sheep&#39;, &#39;cow&#39;, &#39;elephant&#39;, &#39;bear&#39;, &#39;zebra&#39;, &#39;giraffe&#39;, &#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;frisbee&#39;, &#39;skis&#39;, &#39;snowboard&#39;, &#39;sports ball&#39;, &#39;kite&#39;, &#39;baseball bat&#39;, &#39;baseball glove&#39;, &#39;skateboard&#39;, &#39;surfboard&#39;, &#39;tennis racket&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;, &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;, &#39;pizza&#39;, &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;couch&#39;, &#39;potted plant&#39;, &#39;bed&#39;, &#39;dining table&#39;, &#39;toilet&#39;, &#39;tv&#39;, &#39;laptop&#39;, &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;, &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;, &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;, &#39;hair drier&#39;, &#39;toothbrush&#39; ] . The following function will apply to the origianl image, the pixels from the gray image is 0, otherwise keep the pixels from original picture. . # This function is used to change the colorful background information to grayscale. # image[:,:,0] is the Blue channel,image[:,:,1] is the Green channel, image[:,:,2] is the Red channel # mask == 0 means that this pixel is not belong to the object. # np.where function means that if the pixel belong to background, change it to gray_image. # Since the gray_image is 2D, for each pixel in background, we should set 3 channels to the same value to keep the grayscale. def apply_mask(image, mask_image, mask): &quot;&quot;&quot;Helper function to apply a mask to an image.&quot;&quot;&quot; image[:, :, 0] = np.where( mask == 0, mask_image[:, :, 0], image[:, :, 0] ) image[:, :, 1] = np.where( mask == 0, mask_image[:, :, 1], image[:, :, 1] ) image[:, :, 2] = np.where( mask == 0, mask_image[:, :, 2], image[:, :, 2] ) return image . def process_image(image, mask_image, boxes, masks, ids, names, scores, target_label): &quot;&quot;&quot;Helper function to find the object with biggest bounding box and apply mask to it.&quot;&quot;&quot; # max_area will save the largest object for all the detection results max_area = 0 # n_instances saves the amount of all objects n_instances = boxes.shape[0] if not n_instances: print(&#39;NO INSTANCES TO DISPLAY&#39;) else: assert boxes.shape[0] == masks.shape[-1] == ids.shape[0] for i in range(n_instances): if not np.any(boxes[i]): continue # compute the square of each object y1, x1, y2, x2 = boxes[i] square = (y2 - y1) * (x2 - x1) # use label to select the object with given label from all the 80 classes in COCO dataset current_label = names[ids[i]] if target_label is not None or current_label == target_label: # save the largest object in the image as main character # other people will be regarded as background if square &gt; max_area: max_area = square mask = masks[:, :, i] else: continue else: continue # apply mask for the image # by mistake you put apply_mask inside for loop or you can write continue in if also image = apply_mask(image, mask_image, mask) return image . Now the mode is ready to use . !curl -L -o cat_input.jpg https://unsplash.com/photos/7GX5aICb5i4/download?force=true&amp;w=640 . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 232 0 232 0 0 666 0 --:--:-- --:--:-- --:--:-- 666 100 5442k 100 5442k 0 0 10.5M 0 --:--:-- --:--:-- --:--:-- 10.5M . # Credit for the image: https://unsplash.com/photos/7GX5aICb5i4 image = cv2.imread(&#39;./cat_input.jpg&#39;) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7f4274e4b710&gt; . Application 1: Grayscale the background . Recognize the main character, keep it colorfull while grayscal the background of the image. . # Use cvtColor to accomplish image transformation from RGB image to gray image mask_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) mask_image = np.stack([mask_image, mask_image, mask_image], axis=2) plt.imshow(mask_image) . &lt;matplotlib.image.AxesImage at 0x7f4274e21a90&gt; . results = model.detect([image], verbose=0) output_dict = results[0] rois, class_ids, scores, masks = output_dict.values() . result = process_image( image.copy(), mask_image, rois, masks, class_ids, class_names, scores, &#39;cat&#39; ) plt.imshow(result) . &lt;matplotlib.image.AxesImage at 0x7f427458c860&gt; . Let&#39;s take this cat to the beach . !curl -L -o beach.jpg https://unsplash.com/photos/DH_u2aV3nGM/download?force=true&amp;w=640 . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 242 0 242 0 0 733 0 --:--:-- --:--:-- --:--:-- 731 100 4000k 100 4000k 0 0 9828k 0 --:--:-- --:--:-- --:--:-- 9828k . image_beach = cv2.imread(&#39;./beach.jpg&#39;) image_beach = cv2.cvtColor(image_beach, cv2.COLOR_BGR2RGB) plt.imshow(image_beach) . &lt;matplotlib.image.AxesImage at 0x7f427456cc18&gt; . Reshape the new mask image so that it matches the size of the original image. . image_beach = cv2.resize(image_beach, dsize=(image.shape[1], image.shape[0]), interpolation = cv2.INTER_AREA) . result = process_image( image.copy(), image_beach, rois, masks, class_ids, class_names, scores, &#39;cat&#39; ) plt.imshow(result) . &lt;matplotlib.image.AxesImage at 0x7f42744ccba8&gt; . Think of the possibilites :) .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/segmentation/2020/03/29/Mask_RCNN_Tensorflow_v1_image_examples.html",
            "relUrl": "/tensorflow/vision/segmentation/2020/03/29/Mask_RCNN_Tensorflow_v1_image_examples.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Text classification with BERT using TF Text",
            "content": "Setup . try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass . TensorFlow 2.x selected. . Install dependencies . %%capture %%bash pip install -U tensorflow-text . Import modules . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split import tensorflow as tf import tensorflow_text as text import tensorflow_hub as hub import tensorflow_datasets as tfds from tensorflow.keras.layers import Dense, Dropout, Input from tensorflow.keras.models import Model . Set default options for modules . pd.set_option(&#39;display.max_colwidth&#39;, -1) . GPU check . num_gpus_available = len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;)) print(&quot;Num GPUs Available: &quot;, num_gpus_available) assert num_gpus_available &gt; 0 . Num GPUs Available: 1 . config = { &#39;seed&#39;: 31, &#39;batch_size&#39;: 64, &#39;epochs&#39;: 10, &#39;max_seq_len&#39;: 128 } . Data . Download the pretrained BERT model . BERT_URL = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1&quot; bert_layer = hub.KerasLayer(BERT_URL, trainable=False) vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() print(f&#39;BERT vocab is stored at : {vocab_file}&#39;) print(f&#39;BERT model is case sensitive: {do_lower_case}&#39;) . BERT vocab is stored at : b&#39;/tmp/tfhub_modules/03d6fb3ce1605ad9e5e9ed5346b2fb9623ef4d3d/assets/vocab.txt&#39; BERT model is case sensitive: True . Load the vocab file that corresponds to the pretrained BERT . def load_vocab(vocab_file): &quot;&quot;&quot;Load a vocabulary file into a list.&quot;&quot;&quot; vocab = [] with tf.io.gfile.GFile(vocab_file, &quot;r&quot;) as reader: while True: token = reader.readline() if not token: break token = token.strip() vocab.append(token) return vocab vocab = load_vocab(vocab_file) . Use BERT vocab to create a word to index lookup table . def create_vocab_table(vocab, num_oov=1): &quot;&quot;&quot;Create a lookup table for a vocabulary&quot;&quot;&quot; vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64) init = tf.lookup.KeyValueTensorInitializer(keys=vocab, values=vocab_values, key_dtype=tf.string, value_dtype=tf.int64) vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov, lookup_key_dtype=tf.string) return vocab_table vocab_lookup_table = create_vocab_table(vocab) . Use BERT vocab to create a index to word lookup table . def create_index2word(vocab): # Create a lookup table for a index to token vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64) init = tf.lookup.KeyValueTensorInitializer(keys=vocab_values, values=vocab) return tf.lookup.StaticHashTable(initializer=init, default_value=tf.constant(&#39;unk&#39;), name=&quot;index2word&quot;) index2word = create_index2word(vocab) . Check out the indices for the following tokens . vocab_lookup_table.lookup(tf.constant([&#39;[PAD]&#39;, &#39;[UNK]&#39;, &#39;[CLS]&#39;, &#39;[SEP]&#39;, &#39;[MASK]&#39;])) . &lt;tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 0, 100, 101, 102, 103])&gt; . Check out the token corresponding to an index . index2word.lookup(tf.constant([0], dtype=&#39;int64&#39;)).numpy() . [b&#39;[PAD]&#39;] . Create a BERT tokenizer using TF Text . tokenizer = text.BertTokenizer( vocab_lookup_table, token_out_type=tf.int64, lower_case=do_lower_case ) . Lookup for the BERT token IDs for padding and start/end of sentence. . PAD_ID = vocab_lookup_table.lookup(tf.constant(&#39;[PAD]&#39;)) # padding token CLS_ID = vocab_lookup_table.lookup(tf.constant(&#39;[CLS]&#39;)) # class token SEP_ID = vocab_lookup_table.lookup(tf.constant(&#39;[SEP]&#39;)) # sequence separator token . Preprocessing . Define the logic to preprocess data and format it as required by BERT . def preprocess(record): review, label = record[&#39;text&#39;], record[&#39;label&#39;] # process review to calculate BERT input ids, mask, type_ids = preprocess_bert_input(review) return (ids, mask, type_ids), label def preprocess_bert_input(review): # calculate tokens ID ids = tokenize_text(review, config[&#39;max_seq_len&#39;]) # calculate mask mask = tf.cast(ids &gt; 0, tf.int64) mask = tf.reshape(mask, [-1, config[&#39;max_seq_len&#39;]]) # calculate tokens type ID zeros_dims = tf.stack(tf.shape(mask)) type_ids = tf.fill(zeros_dims, 0) type_ids = tf.cast(type_ids, tf.int64) return (ids, mask, type_ids) def tokenize_text(review, seq_len): # convert text into token ids tokens = tokenizer.tokenize(review) # flatten the output ragged tensors tokens = tokens.merge_dims(1, 2)[:, :seq_len] # Add start and end token ids to the id sequence start_tokens = tf.fill([tf.shape(review)[0], 1], CLS_ID) end_tokens = tf.fill([tf.shape(review)[0], 1], SEP_ID) tokens = tokens[:, :seq_len - 2] tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1) # truncate sequences greater than MAX_SEQ_LEN tokens = tokens[:, :seq_len] # pad shorter sequences with the pad token id tokens = tokens.to_tensor(default_value=PAD_ID) pad = seq_len - tf.shape(tokens)[1] tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID) # and finally reshape the word token ids to fit the output # data structure of TFT return tf.reshape(tokens, [-1, seq_len]) . Dataset . Download the dataset from TF Hub and process it . train_ds, valid_ds = tfds.load(&#39;imdb_reviews&#39;, split=[&#39;train&#39;, &#39;test&#39;], shuffle_files=True) train_ds = train_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) valid_ds = valid_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) train_ds, valid_ds = train_ds.map(preprocess), valid_ds.map(preprocess) . Model . input_ids = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_ids&quot;) input_mask = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_mask&quot;) input_type_ids = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_type_ids&quot;) pooled_output, sequence_output = bert_layer([input_ids, input_mask, input_type_ids]) drop_out = Dropout(0.3, name=&quot;dropout&quot;)(pooled_output) output = Dense(1, activation=&#39;sigmoid&#39;, name=&quot;linear&quot;)(drop_out) model = Model(inputs=[input_ids, input_mask, input_type_ids], outputs=output) model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . model.summary() . Model: &#34;model_1&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_mask (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_type_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ keras_layer (KerasLayer) [(None, 768), (None, 109482241 input_ids[0][0] input_mask[0][0] input_type_ids[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 768) 0 keras_layer[1][0] __________________________________________________________________________________________________ linear (Dense) (None, 1) 769 dropout[0][0] ================================================================================================== Total params: 109,483,010 Trainable params: 769 Non-trainable params: 109,482,241 __________________________________________________________________________________________________ . Training . model.fit(train_ds, validation_data=valid_ds, epochs=config[&#39;epochs&#39;]) . Epoch 1/10 391/391 [==============================] - 499s 1s/step - loss: 0.6654 - accuracy: 0.6016 - val_loss: 0.5977 - val_accuracy: 0.7028 Epoch 2/10 391/391 [==============================] - 510s 1s/step - loss: 0.6063 - accuracy: 0.6712 - val_loss: 0.5650 - val_accuracy: 0.7282 Epoch 3/10 391/391 [==============================] - 510s 1s/step - loss: 0.5839 - accuracy: 0.6969 - val_loss: 0.5494 - val_accuracy: 0.7362 Epoch 4/10 391/391 [==============================] - 511s 1s/step - loss: 0.5730 - accuracy: 0.7025 - val_loss: 0.5388 - val_accuracy: 0.7455 Epoch 5/10 391/391 [==============================] - 510s 1s/step - loss: 0.5696 - accuracy: 0.7058 - val_loss: 0.5376 - val_accuracy: 0.7417 Epoch 6/10 391/391 [==============================] - 510s 1s/step - loss: 0.5613 - accuracy: 0.7146 - val_loss: 0.5268 - val_accuracy: 0.7517 Epoch 7/10 391/391 [==============================] - 510s 1s/step - loss: 0.5608 - accuracy: 0.7130 - val_loss: 0.5233 - val_accuracy: 0.7544 Epoch 8/10 391/391 [==============================] - 510s 1s/step - loss: 0.5625 - accuracy: 0.7106 - val_loss: 0.5217 - val_accuracy: 0.7555 Epoch 9/10 391/391 [==============================] - 510s 1s/step - loss: 0.5603 - accuracy: 0.7125 - val_loss: 0.5199 - val_accuracy: 0.7535 Epoch 10/10 391/391 [==============================] - 510s 1s/step - loss: 0.5567 - accuracy: 0.7159 - val_loss: 0.5150 - val_accuracy: 0.7591 . &lt;tensorflow.python.keras.callbacks.History at 0x7f2fffddba58&gt; . Evaluation . test_text_ds = tfds.load(&#39;imdb_reviews&#39;, split=&#39;unsupervised&#39;, shuffle_files=True) test_ds = test_text_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) test_ds = test_ds.map(preprocess) . Check how test text is tokenized . test_text = [record[&#39;text&#39;].numpy() for record in test_text_ds.take(10)] . ids = tokenize_text(test_text, config[&#39;max_seq_len&#39;]) . tokens = [b&#39; &#39;.join(tokens_array) for tokens_array in index2word.lookup(ids).numpy()] . pd.DataFrame({&#39;tokens&#39;: tokens}) . tokens . 0 b&quot;[CLS] spoil ##er - now knowing the ending i find it so clever that the whole movie takes place in a motel and each character has a different room . even sane people have many different aspects to their personality , but they don &#39; t let them become dominant - - they are controlled . malcolm &#39; s various personalities and needs were person ##ified in each character . the prostitute mother ( amanda pee ##t ) , the part of him who hated her for being a prostitute ( larry ) , the loving mother he wish he had , the loving father he wish he had , the selfish part of himself ( actress ) , the violent part of his personality ( ray [SEP]&quot; | . 1 b&quot;[CLS] i knew about this film long before i saw it . in fact , i had to buy the dvd in order to see it because no video store carried it . i didn &#39; t mind spending the $ 12 to buy it used because i collect off the wall movies . the new limited edition double dvd has great sound and visually not bad . i found myself laughing much more then &lt; br / &gt; &lt; br / &gt; jolt ##ing in fear , although there were a few scenes were i was startled . &lt; br / &gt; &lt; br / &gt; if you enjoy off the wall 70s sci - fi / horror movies , you probably will eat this one [SEP]&quot; | . 2 b&quot;[CLS] this movie is really really awful . it &#39; s as bad as zombie 90 well maybe not that bad but pretty close . if your a fan of the italian horror movies then you might like this movie . i thought that it was dam near un ##watch ##able of course i &#39; m not a fan of the italian movies . the only italian movie that was ok was jungle holocaust . which is one over ##rated movie . this film is way over ##rated . but let &#39; s get started with how horrible this film really is shall we . the acting is goofy and horrible . the effects suck . no plot with this movie . little gore which is the [SEP]&quot; | . 3 b&#39;[CLS] wait a minute . . . yes i do . &lt; br / &gt; &lt; br / &gt; the director of &#39; the breed &#39; has obviously seen terry gill ##iam &#39; s &#39; brazil &#39; a few too many times and asked himself the question , &quot; if &#39; brazil &#39; had been an ill - conceived tale about vampires in the near future , what would it be like ? &quot; well , i &#39; ll tell ya , it &#39; d be like 91 minutes of a swedish whore kicking you in the groin , only not as satisfying . the dialogue was laced with gr ##at ##uit ##ous curse words and tri ##te one - liner ##s , and whoever edited this [SEP]&#39; | . 4 b&quot;[CLS] this is the type of movie that &#39; s just barely involving enough for one viewing , but i don &#39; t think i could stand to watch it again . it looks and plays like a mid - seventies tv movie , only with some gr ##at ##uit ##ous sex and violence thrown in . &lt; br / &gt; &lt; br / &gt; i agree with several previous posters - - her ##ve ville ##chai ##ze is not very menacing , and at times even comes off as un ##int ##ended comedy . at least the other two villains make up for that . also , it was jolt ##ing to see jonathan fr ##id is such a pedestrian role , which definitely under - [SEP]&quot; | . 5 b&quot;[CLS] i like sci - fi movies and everything &#39; bout it and aliens , so i watched this flick . nothing new , nothing special , average acting , typical h . b . davenport &#39; story , weak and che ##es ##y fx &#39; s , bad ending of movie , but still the author idea is good . the marines on lost island find the truth about alien landing there and truth about past - experiments on them . they die one after one , some of them were killed by lonely alien , and others by human enemies . ufo effects , when it flees and crush ##es are bad , too . the voices of angry alien are funny , too . [SEP]&quot; | . 6 b&quot;[CLS] i was lucky enough to see a preview of this film tonight . this was a very cool , eerie film . well acted , especially by ska ##rs ##gard who played his role of terry glass perfectly . sob ##ies ##ki did a very good job too as it seems to me that she has a bright future ahead of her . the music was well placed but was fairly standard . the use of shadows was quite interesting as well . overall , this was quite a nice surprise considering i &#39; m not much a fan of this genre . 7 / 10 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]&quot; | . 7 b&#39;[CLS] my kids and i love this movie ! ! we think that richard pry ##or and the whole cast did a wonderful job in the movie . it means more to us now since the passing of richard ! ! we will miss his sense of humor . but his movies and shows will stay with us forever ! ! we especially love the parts of brad , frank crawford and ar ##lo pear ! ! they had some one liner ##s in the movie that were great ! ! my son and i love to quote those one liner ##s when we see each other and my daughter will join us when we discuss the movie . we thought the moving guys were terrific . [SEP]&#39; | . 8 b&quot;[CLS] somehow the an ##ima ##trix shorts with the most interesting premises have the worst outcome . mat ##ric ##ulated is the worst of the bunch ( although it &#39; s a close call with program ) , as it takes a great idea ( showing the machines the beauty of mankind by plug ##ging them in ) and turns it into the worst experience of the 9 . &lt; br / &gt; &lt; br / &gt; as i said , the story begins promising and interesting , but ends with a long , long , long sequence of &#39; weird &#39; images , a cross between the famous scenes from 2001 and v ##ga - rain ( who can remember it ) , but not as [SEP]&quot; | . 9 b&quot;[CLS] while holiday ##ing in the basque region of spain , two couples discover a child whose hands are severely miss ##ha ##pen . the child has been gravely mist ##reate ##d , and , as a result , cannot communicate . the two couples reluctantly decide to rescue her and report her circumstances to the authorities . however , severe weather and the dense ##ness of the forest surrounding their holiday home make it impossible for them to make a quick get ##away . soon , the local inhabitants become aware that the girl is missing , and they right ##ly suspect the holiday - makers of taking her . suspicions and paranoia begin to fest ##er , and it isn &#39; t long before violence [SEP]&quot; | . Run prediction on test reviews . result = model.predict(test_ds) . result.shape . (50000, 1) . result_df = pd.DataFrame({&#39;label&#39;: tf.squeeze(result[:10]).numpy(), &#39;text&#39;: test_text}) result_df.head() . label text . 0 0.464566 | b&quot;SPOILER - Now knowing the ending I find it so clever that the whole movie takes place in a motel and each character has a different room. Even sane people have many different aspects to their personality, but they don&#39;t let them become dominant -- they are controlled. Malcolm&#39;s various personalities and needs were personified in each character. The prostitute mother (Amanda Peet), the part of him who hated her for being a prostitute (Larry), the loving mother he wish he had, the loving father he wish he had, the selfish part of himself (actress), the violent part of his personality (Ray Liotta and Busey), the irrational emotions he feels and his need to be loved (Ginnie) and his attempts to control those feelings (Lou), the hurt little boy who sees far too many traumatic things in his life, and of course, John Cusack who seems to represent Malcolm himself trying to analyze and understand all the craziness in his mind, tries to follow the rules (accepting responsibility for the car accident), help others (giving Amanda Peet a ride, and stitching up the mother). Very cleverly done!&quot; | . 1 0.252326 | b&#39;I knew about this film long before I saw it. In fact, I had to buy the DVD in order to see it because no video store carried it. I didn &#39;t mind spending the $12 to buy it used because I collect off the wall movies. The new limited edition double DVD has great sound and visually not bad. I found myself laughing much more then&lt;br /&gt;&lt;br /&gt;jolting in fear, although there were a few scenes were I was startled.&lt;br /&gt;&lt;br /&gt;If you enjoy off the wall 70s sci-fi/horror movies, you probably will eat this one up. I was a little dissapointed at how abrubtly it ended. I wanted the movie to keep going, see how things pan out. The DVD revolution has brought so many&lt;br /&gt;&lt;br /&gt;lost clasics back to life, it is truly wonderful. Blue Sunshine is one of those lost &quot;missing links&quot; of the cinema. Enjoy!&#39; | . 2 0.485239 | b&quot;This movie is really really awful. It&#39;s as bad as Zombie 90 well maybe not that bad but pretty close. If your a fan of the Italian horror movies then you might like this movie. I thought that it was dam near unwatchable of course I&#39;m not a fan of the Italian movies. The only Italian movie that was OK was Jungle holocaust. Which is one overrated movie. This film is way overrated. But let&#39;s get started with how horrible this film really is shall we. The acting is goofy and horrible. The effects suck. No plot with this movie. Little gore which is the only good thing in the film isn&#39;t showed nearly enough to be worth watching this wreck. The zombies are very fake looking. It looks like it&#39;s a bunch of dudes wearing cheap dollar store masks. Please avoid this film at all costs.&quot; | . 3 0.251897 | b&#39;Wait a minute... yes I do.&lt;br /&gt;&lt;br /&gt;The director of &#39;The Breed &#39; has obviously seen Terry Gilliam &#39;s &#39;Brazil &#39; a few too many times and asked himself the question, &quot;If &#39;Brazil &#39; had been an ill-conceived tale about vampires in the near future, what would it be like?&quot; Well, I &#39;ll tell ya, it &#39;d be like 91 minutes of a Swedish whore kicking you in the groin, only not as satisfying. The dialogue was laced with gratuitous curse words and trite one-liners, and whoever edited this piece of crap should be shot. I have no real idea of exactly how the whole thing ended because I &#39;m not really sure what happened during the first part of the film. With so many subplots your head begins to hurt and so much bad acting your head wants to explode this movie should only be viewed with large quantities of beer and at least two other people you can MST3K with. The only thing that made me not stab myself in the eye with a dirty soup spoon was this line: Evil Doctor Guy: &quot;That &#39;s it, you are not James Bond, and I am not Blofeld. No more explanations!&quot; Dude From Jason &#39;s Lyric: &quot;I &#39;m getting paid scale!&quot; The cinematography was shaky at best and the acting was putrid. Also, what was with all the pseudo-1984 posters and PA announcements? The costumes were from the 50 &#39;s, the cars were from the 60 &#39;s, the music was from the 90 &#39;s and I wish I were dead. This movie sucks.&#39; | . 4 0.274131 | b&#39;This is the type of movie that &#39;s just barely involving enough for one viewing, but I don &#39;t think I could stand to watch it again. It looks and plays like a mid-Seventies TV movie, only with some gratuitous sex and violence thrown in.&lt;br /&gt;&lt;br /&gt;I agree with several previous posters -- Herve Villechaize is NOT very menacing, and at times even comes off as unintended comedy. At least the other two villains make up for that. Also, it was jolting to see Jonathan Frid is such a pedestrian role, which definitely under-utilized his enormous talents.&lt;br /&gt;&lt;br /&gt;But I think the basic problem with &quot;Seizure&quot; is in the storyline. The evil trio that are conjured up from Frid &#39;s mind are seen too early and too often. They appear to everyone at once, and announce their (murky) plans too early in the picture. In fact, Stone takes this idea and literally shoves it in the viewer &#39;s face, with a series of challenges for the guests; challenges that it doesn &#39;t seem like they have any chance of winning, anyway. How much more effective would have been keeping the evil ones in the shadows, preying on each house guest in turn, sowing confusion and doubt among the remaining house guests, who don &#39;t know who or what is causing the carnage. By having the trio appear early on, to all the &quot;assembled guests&quot;, and announcing their plan (confusing as that plan is), much potential for tension and suspense are lost.&lt;br /&gt;&lt;br /&gt;Also, a more gradual appearance of the evil ones would indicate Frid is slowing losing control of his subconscious. To have Frid subconsciously conjure up these baddies, because he &#39;s got hidden grudges against his wife and friends, would have been a far more logical plot device. Instead of having Frid play an intended victim from the get-go, it would have worked better to have him slowing becoming helpless to control the menace he &#39;s created, with mixed feelings of guilt and satisfaction as his shallow, superficial friends are killed off. The plot Stone offers up is confusing as to the origins and, most importantly, the motivations of the evil trio, and never gives any explanation why Frid, from whose mind they came from, can exercise absolutely no control over them. Confusing is the word that best sums up the whole picture, and the end feels like a total cheat. Better to have some great showdown in which Frid is finally able to banish the creations of his own tormented mind.&lt;br /&gt;&lt;br /&gt;Oliver Stone has done some notable work in his career, but sadly &quot;Seizure&quot; is not among them.&#39; | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/nlp/2020/03/15/Text_classification_with_BERT_and_TF_Text.html",
            "relUrl": "/tensorflow/nlp/2020/03/15/Text_classification_with_BERT_and_TF_Text.html",
            "date": " • Mar 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://dzlab.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
      ,"page8": {
          "title": "",
          "content": "{“/about/”:”https://dzlab.github.io/about/”} .",
          "url": "https://dzlab.github.io/notebooks/redirects.json",
          "relUrl": "/redirects.json",
          "date": ""
      }
      
  

  

  
  

}