{
  
    
        "post0": {
            "title": "Zero-Shot image classification with CLIP and Flax",
            "content": "CLIP (Contrastive Language-Image Pre-training) was created by OpenAI. This model is trained on image-caption pairs with a task to learn the perfect embeddings mapping for images and text. CLIP is composed of a text encoder and an image encoder that produce embeddings in a shared space. The learned embeddings are similar when the concepts (image or text) closer together, and not similar for unrelated concepts. . Furthermore, these embeddings capture rich semantic information, which turns out to be very useful for many downstream tasks such as image generation, or zero-shot image classification. . . In this article, we will see how to evalute the performance of CLIP on the task of zero-shot image classification on 3 datasets with variying difficuly. . As depicted in the diagram above (credit), CLIP is used for classification as follows: . For each label in our classification labels we generate a prompt text like this &#39;a photo of a {label}&#39; | We embed these prompts with the CLIP text encoder. | We embed the images with the CLIP image encoder. | Using cosine similarity find the best match for the image embeddings from all of the prompts embeddings. | Optionally use Softmax to convert this to a probablity | Setup . We will use Flax-implementation of CLIP available from the transformers library. So let&#39;s install our dependencies. . %%capture %%bash pip install --upgrade flax transformers pip install --upgrade &quot;jax[cuda]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html . import jax import jax.numpy as jnp from sklearn import metrics from tqdm.auto import tqdm from fastai.vision.all import * from transformers import CLIPProcessor, FlaxCLIPModel . Set a seed for reproducibility . seed = 123 random.seed(seed) . We will use the checkpoints for CLIP model and inputs processing of OpenAI&#39;s clip-vit-base-patch32 availble in Hugging Face. . model = FlaxCLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;) processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;) . Helper functions . We need couple of helper functions that we will use in the classification sections. . First, the following fuctions scans for image files on disk using a regex pattern. And optionally a label dictionary used in case we want use something else as label instead of labels extracted from the file&#39;s path. For instance, imagenet labels are numbers and it would be better to map them to class text. . def scan_images(pattern, class_to_label=None): files = glob.glob(pattern) random.shuffle(files) print(f&#39;Found {len(files)} files&#39;) labels = [f.split(&#39;/&#39;)[-2] for f in files] if class_to_label: labels = [class_to_label[cls] for cls in labels] print(&#39;First few labels:&#39;, labels[:3]) print(&#39;First few filenames:&#39;, files[:3]) return files, labels . The following functions performs classifcation prediction on batches of the image files. It first creates text prompts of the form &quot;a photo of a {label}&quot; using the labels. Then, uses the CLIPProcessor to prepare those prompts and the images before passing them to the FlaxCLIPModel for predictions. The output of the model FlaxCLIPModel is passed through a softmax to calculate the probabilities of each label, the highest probablity is used for picking the predicted label. . def predict(files, classes, batch_size=256): texts = [f&#39;a photo of a {cl}&#39; for cl in classes] y_pred = jnp.asarray([]) for start in tqdm(range(0, len(files), batch_size)): end = min(start+batch_size, len(files)) # read each image images = [Image.open(f) for f in files[start:end]] # pre-process the texts and images inputs = processor( text=texts, images=images, return_tensors=&quot;np&quot;, padding=True ) # run CLIP outputs = model(**inputs) # get the image-text similarity score logits_per_image = outputs.logits_per_image # apply softmax to get the label probabilities probs = jax.nn.softmax(logits_per_image, axis=1) y_pred = jnp.append(y_pred, np.argmax(probs, axis=-1)) return y_pred . There will be some mis-classified images, the following helper function select some of those images and load them from disk into PIL. . def select_misclassified(y_true, y_pred, classes, files, k=9): indecies = jnp.where((y_true==y_pred)==False)[0] indecies = [int(idx) for idx in list(indecies)] indecies_k9 = random.sample(indecies, k=k) pil_files = [files[i] for i in indecies_k9] pil_labels = [f&#39;{classes[int(y_pred[i])]}/{classes[int(y_true[i])]}&#39; for i in indecies_k9] pil_images = [Image.open(f) for f in pil_files] return pil_images, pil_labels . The following helper function will be used to plot a collection of images along with their descriptions. . def ceildiv(a, b): return -(-a // b) def plots_pil_images(pil_images, figsize=(10,5), rows=1, cols=None, titles=None, maintitle=None): f = plt.figure(figsize=figsize) if maintitle is not None: plt.suptitle(maintitle, fontsize=10) cols = cols if cols else ceildiv(len(pil_images), rows) for i in range(len(pil_images)): sp = f.add_subplot(rows, cols, i+1) sp.axis(&#39;Off&#39;) if titles is not None: sp.set_title(titles[i], fontsize=16) img = np.asarray(pil_images[i]) plt.imshow(img) . Finally, a helper function to download an imge from the internet . def get_image(url): headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64)&quot;} resp = requests.get(url, headers = headers) return Image.open(io.BytesIO(resp.content)) . Image Classification . In this section, we will try zero-shot classfication on the following datasets starting from the easiest one to the more challenging. For each one we will evaluate the model performance. . Dogs vs Cats . Dogs vs Cats is a binary classification dataset, let&#39;s download it and use the images from the validation set to run our predictions. . dogs_path = untar_data(URLs.DOGS) . . 100.00% [839286784/839285364 00:18&lt;00:00] Load the files and true labels of this dataset . files1, labels1 = scan_images(f&#39;{dogs_path}/valid/*/*.jpg&#39;) . Found 2000 files First few labels: [&#39;dogs&#39;, &#39;cats&#39;, &#39;dogs&#39;] First few filenames: [&#39;/root/.fastai/data/dogscats/valid/dogs/dog.6312.jpg&#39;, &#39;/root/.fastai/data/dogscats/valid/cats/cat.6086.jpg&#39;, &#39;/root/.fastai/data/dogscats/valid/dogs/dog.6427.jpg&#39;] . Replace the true labels with the respective class index dogs -&gt; 0 and cats -&gt; 1. . classes1 = [&#39;dogs&#39;, &#39;cats&#39;] y_true1 = jnp.asarray([classes1.index(l) for l in labels1]) y_pred1 = predict(files1, classes1) . Let&#39;s calculate the accuracy score of our predictions. . metrics.accuracy_score(y_true1, y_pred1) . 0.995 . Let&#39;s calculate the confusion matrix our predictions to find out where our model got the labels wrong. . metrics.ConfusionMatrixDisplay.from_predictions(y_true1, y_pred1, display_labels=classes1, cmap=&#39;Blues&#39;); . . Note: see how CLIP performed zero-shot classification on this dataset with 99.5% accuracy! . Let&#39;s see some of the images that the model got wrong. . pil_images1, pil_labels1 = select_misclassified(y_true1, y_pred1, classes1, files1) . plots_pil_images(pil_images1, figsize=(13, 7), rows=3, titles=pil_labels1, maintitle=&#39;classifications y_pred/y_true&#39;) . IMAGENETTE_160 . As a next dataset to try, we pick Imagenette, which is a subset of the Imagenet dataset containing only 10 classes. Let&#39;s download the dataset. . path2 = untar_data(URLs.IMAGENETTE_160) imagenet_class_to_class_label = { &#39;n01440764&#39;: &#39;tench&#39;, &#39;n02102040&#39;: &#39;English springer&#39;, &#39;n02979186&#39;: &#39;cassette player&#39;, &#39;n03000684&#39;: &#39;chain saw&#39;, &#39;n03028079&#39;: &#39;church&#39;, &#39;n03394916&#39;: &#39;French horn&#39;, &#39;n03417042&#39;: &#39;garbage truck&#39;, &#39;n03425413&#39;: &#39;gas pump&#39;, &#39;n03445777&#39;: &#39;golf ball&#39;, &#39;n03888257&#39;: &#39;parachute&#39; } classes2 = list(imagenet_class_to_class_label.values()) print(classes2) . . 100.01% [99008512/99003388 00:02&lt;00:00] [&#39;tench&#39;, &#39;English springer&#39;, &#39;cassette player&#39;, &#39;chain saw&#39;, &#39;church&#39;, &#39;French horn&#39;, &#39;garbage truck&#39;, &#39;gas pump&#39;, &#39;golf ball&#39;, &#39;parachute&#39;] . Next, we scan for the images and their labels from the validation set. . files2, labels2 = scan_images(f&#39;{path2}/val/*/*.JPEG&#39;, imagenet_class_to_class_label) . Found 3925 files First few labels: [&#39;golf ball&#39;, &#39;parachute&#39;, &#39;English springer&#39;] First few filenames: [&#39;/root/.fastai/data/imagenette2-160/val/n03445777/n03445777_11822.JPEG&#39;, &#39;/root/.fastai/data/imagenette2-160/val/n03888257/n03888257_37950.JPEG&#39;, &#39;/root/.fastai/data/imagenette2-160/val/n02102040/n02102040_2890.JPEG&#39;] . Then, we get the predictions from CLIP . y_true2 = jnp.asarray([classes2.index(l) for l in labels2]) y_pred2 = predict(files2, classes2) . Let&#39;s calculate the model accuracy . metrics.accuracy_score(y_true2, y_pred2) . 0.9859872611464968 . Then find out what labels got wrongly classified using the confusing matrix. . metrics.ConfusionMatrixDisplay.from_predictions( y_true2, y_pred2, display_labels=classes2, xticks_rotation=&#39;vertical&#39;, cmap=&#39;Blues&#39; ); . . Note: you can see that CLIP did a good job in this dataset too with an accuray of 98.5% . Let&#39;s visualize some of the iamges the model classified wrongly. . pil_images2, pil_labels2 = select_misclassified(y_true2, y_pred2, classes2, files2) . plots_pil_images(pil_images2, figsize=(11, 8), rows=3, titles=pil_labels2, maintitle=&#39;classifications y_pred/y_true&#39;) . You can see that those images are confusing, for intance the picture of the man playing french horn in a church. The model picked church over french horn but both are in fact accurate classification. . Dog breads . Moving to a harder dataset Imagewoof which consists of images of 10 hard to tell appart dog breeds. . path3 = untar_data(URLs.IMAGEWOOF_160) imagenet_class_to_dogbread = { &#39;n02086240&#39;: &#39;Shih-Tzu&#39;, &#39;n02087394&#39;: &#39;Rhodesian ridgeback&#39;, &#39;n02088364&#39;: &#39;Beagle&#39;, &#39;n02089973&#39;: &#39;English foxhound&#39;, &#39;n02093754&#39;: &#39;Border terrier&#39;, &#39;n02096294&#39;: &#39;Australian terrier&#39;, &#39;n02099601&#39;: &#39;Golden retriever&#39;, &#39;n02105641&#39;: &#39;Old English sheepdog&#39;, &#39;n02111889&#39;: &#39;Samoyed&#39;, &#39;n02115641&#39;: &#39;Dingo&#39; } classes3 = list(imagenet_class_to_dogbread.values()) print(classes3) . . 100.01% [92618752/92612825 00:02&lt;00:00] [&#39;Shih-Tzu&#39;, &#39;Rhodesian ridgeback&#39;, &#39;Beagle&#39;, &#39;English foxhound&#39;, &#39;Border terrier&#39;, &#39;Australian terrier&#39;, &#39;Golden retriever&#39;, &#39;Old English sheepdog&#39;, &#39;Samoyed&#39;, &#39;Dingo&#39;] . After downloading the dataset, let&#39;s scan the images in the validation set folder. . files3, labels3 = scan_images(f&#39;{path3}/val/*/*.JPEG&#39;, imagenet_class_to_dogbread) . Found 3929 files First few labels: [&#39;Samoyed&#39;, &#39;Beagle&#39;, &#39;Beagle&#39;] First few filenames: [&#39;/root/.fastai/data/imagewoof2-160/val/n02111889/n02111889_6962.JPEG&#39;, &#39;/root/.fastai/data/imagewoof2-160/val/n02088364/n02088364_12710.JPEG&#39;, &#39;/root/.fastai/data/imagewoof2-160/val/n02088364/n02088364_6092.JPEG&#39;] . Run CLIP to predict a label for each image . y_true3 = jnp.asarray([classes3.index(l) for l in labels3]) y_pred3 = predict(files3, classes3) . Next we get the accuracy . metrics.accuracy_score(y_true3, y_pred3) . 0.8819037923135657 . Then we calculate the confusion matrix to see any interesting mis-classification . metrics.ConfusionMatrixDisplay.from_predictions( y_true3, y_pred3, display_labels=classes3, xticks_rotation=&#39;vertical&#39;, cmap=&#39;Blues&#39; ); . Notice how CLIP had harder time classifying Bealges from English foxhound. In fact, when looking to images of these two breeds it is really hard to tell which one is which as you can see in the following pictures. . img1 = get_image(&#39;https://upload.wikimedia.org/wikipedia/commons/b/b7/Beagle_Faraon.JPG&#39;) img2 = get_image(&#39;https://d17fnq9dkz9hgj.cloudfront.net/breed-uploads/2018/08/english-foxhound-detail.jpg&#39;) plots_pil_images([img1, img2], rows=1, titles=[&#39;Beagle&#39;, &#39;English foxhound&#39;]) . Overall CLIP did not very bad on this harder dataset performing a shy accuracy of 88.2%. Let&#39;s visualize some of the images the model did mis-classify. . pil_images3, pil_labels3 = select_misclassified(y_true3, y_pred3, classes3, files3) . plots_pil_images(pil_images3, figsize=(13, 8), rows=3, titles=pil_labels3, maintitle=&#39;classifications y_pred/y_true&#39;) . That&#39;s all folks . CLIP is very powerful and can be used for many different tasks from data filtering/search to image generation. In this post, we saw how to use CLIP to perform image classification without having to fine-tune the model. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc. .",
            "url": "https://dzlab.github.io/notebooks/flax/vision/classification/2022/12/26/Zero_Shot_CLIP.html",
            "relUrl": "/flax/vision/classification/2022/12/26/Zero_Shot_CLIP.html",
            "date": " • Dec 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Textual Inversion inference with diffusers and Flax",
            "content": ". Textual Inversion was first introduced in An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion - website. This technique tries to find new embeddings that represent user-provided visual concepts (e.g. image of an object, or hand drawings). These embeddings are then linked to new pseudo-words (the paper uses the $S_{*}$ term) which can be incorporated into typical prompts. Example of prompts including a new concept: . &quot;A carpet with $S{*}$ embroidery&quot;_ | &quot;A stained glass window depicting $S{*}$&quot;_ | &quot;Painting of $S{*}$ in the style of Monet&quot;_ | . Surprisingly, the training of this technique involves the use of 3 to 5 images to teach models like Stable Diffusion to use the new concept for personalized image generation. . In this article, we will adapt the FlaxStableDiffusionPipeline to use new concepts when generating images from text. The concepts we will use are downloaded from the Stable Diffusion Textual Inversion Concepts Library which has tons of different publically available concepts. Feel free to browse through this library and choose other concepts to play with. . Setup and imports . First, we need to install some libraries include diffusers and Flax. . %%capture %%bash pip install --upgrade scipy flax pip install --upgrade &quot;jax[cuda]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html pip install --upgrade diffusers transformers . . Note: if you can have access to TPU then you should skip installing jax[cuda] and instead just connect JAX to the TPU machines as described in this article. . Log into your Hugging Face account with an access token and accept the terms of the licence for this model - see model card. . !huggingface-cli login . _| _| _| _| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|_|_|_| _|_| _|_|_| _|_|_|_| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _|_|_|_| _| _| _| _|_| _| _|_| _| _| _| _| _| _|_| _|_|_| _|_|_|_| _| _|_|_| _| _| _| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _| _| _|_| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _| _| _| _|_|_| _|_|_|_| To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens . Token: Add token as git credential? (Y/n) n Token is valid. Your token has been saved to /root/.huggingface/token Login successful . Import the libraries we will be using. . import os import jax import jax.numpy as jnp import numpy as np from flax.jax_utils import replicate from flax.training.common_utils import shard from PIL import Image from huggingface_hub import hf_hub_download import matplotlib.pyplot as plt %matplotlib inline . Loading the pipeline . Now, we can instanciate a Flax Diffusion pipeline and load the propoer weights for precision. . dtype = jax.numpy.bfloat16 . model_id = &quot;CompVis/stable-diffusion-v1-4&quot; revision = &quot;bf16&quot; # &quot;flax&quot; . from diffusers import FlaxStableDiffusionPipeline pipeline, pipeline_params = FlaxStableDiffusionPipeline.from_pretrained( &quot;CompVis/stable-diffusion-v1-4&quot;, revision=&quot;bf16&quot;, dtype=dtype ) . To introduce new components to our pipeline, we need to add new tokens and link them to their respective embeddings. This is done by acting on the pipeline&#39;s tokenizer and text_encoder. . tokenizer = pipeline.tokenizer text_encoder = pipeline.text_encoder . At this stage, the text embeddings are available in pipeline_params and will be loaded by the pipeline later into the text_encoder later. . text_encoder_params = pipeline_params[&quot;text_encoder&quot;] . Adding concepts . All of the concepts embbedings we will be using here come from Hugging Face&#39;s Concepts Library. So let&#39;s define a helper function to download all the necessary files for a given concept. . Note: you can use your own concepts or get them from a different source. They just need to be available locally when we load them later. . def download_embeddings(repo_id): embeds_path = hf_hub_download(repo_id=repo_id, filename=f&quot;learned_embeds.bin&quot;) token_path = hf_hub_download(repo_id=repo_id, filename=f&quot;token_identifier.txt&quot;) return embeds_path, token_path . Let&#39;s download the embeddings for the &lt;cat-toy&gt; concept. . repo_id_cat_toy = &quot;sd-concepts-library/cat-toy&quot; embeds_path, token_path = download_embeddings(repo_id_cat_toy, &quot;cat_toy&quot;) . Because the original embeddings were stored with pytorch, we cannot just load them into a JAX array but instead a conversion step is needed. One simpler way to convert the embeddings to JAX is to: . load the embeddings with pytorch, | convert them to a numpy array | read the numpy array with JAX | The following helper function do just that: . import torch def load_embeds(embeds_path): pytorch_params = torch.load(embeds_path, map_location=&#39;cpu&#39;) trained_token = list(pytorch_params.keys())[0] pytorch_embeds = pytorch_params[trained_token] embeds = jnp.asarray(pytorch_embeds.numpy()).astype(dtype) return trained_token, embeds . Let&#39;s load the embeddings . token, embeds = load_embeds(embeds_path) . First, let&#39;s check the current size of the tokenizer vocabulary before adding our new token . len(tokenizer) . To add a token we simply do . tokenizer.add_tokens(token) . Now let&#39;s confirm that the vocabulary size increased by one after we added our new token . len(tokenizer) . We can get the ID associated to our new token just to confirm everything is fine so far . tokenizer.convert_tokens_to_ids(token) . We are not done yet with adding our concepts. After, adding new tokens to the vocabulary, we need to make sure to also resize the token embedding matrix of the text_encoder model to match vocabulary size. . The following helper function: . updates the vocabulary size in the text_encoder model configruation and also | adds the embeddings of the concept to the text_encoder&#39;s token embeddings matrix, | . def add_token_embeddings(token_embeds, text_encoder_params): if len(token_embeds.shape) == 1: token_embeds = jnp.expand_dims(token_embeds, axis=0) # update vocab size text_encoder._config.vocab_size = len(tokenizer) # retrive the token embeddings from the encoder parameters text_model_embeds = text_encoder_params[&#39;text_model&#39;][&#39;embeddings&#39;] token_embedding = text_model_embeds[&#39;token_embedding&#39;] text_encoder_embeds = token_embedding[&#39;embedding&#39;] # append the new embeddings new_text_encoder_embeds = jnp.append(text_encoder_embeds, token_embeds, axis=0) token_embedding[&#39;embedding&#39;] = new_text_encoder_embeds . Let&#39;s add the embeddings for the &lt;cat-toy&gt; concept . add_token_embeddings(embeds, text_encoder_params) . To make sure nothing is broken, we can test that we can successfully tokenize and encode a text that contains our new concept. . text_input = tokenizer( token, padding=&quot;max_length&quot;, max_length=tokenizer.model_max_length, truncation=True, return_tensors=&quot;np&quot;, ) prompt_ids = text_input.input_ids text_embeddings = text_encoder(prompt_ids, params=text_encoder_params)[0] text_embeddings.shape . Generating images . The overall logic for using the pipeline to generate images from prompts is pretty much the same one we used in the introduction example, regarless of using the new concept in the prompt or not. . We first use pipeline.prepare_inputs to convert the text prompt to tokens, then get their embeddings. After that, we sample from random noise our initial latent representation. And call the pipeline in a loop as many times as num_inference_steps to denoise that initial latent representation. Finally, we use pipeline.numpy_to_pil to convert the JAX array into an actual PIL image. . guidance_scale = 8 #@param {type:&quot;slider&quot;, min:0, max:100, step:0.5} num_inference_steps = 30 #@param . def generate(prompt, seed=0, num_inference_steps=50, guidance_scale=7.5): num_samples = jax.device_count() if not isinstance(prompt, list): prompt = num_samples * [prompt] else: assert num_samples == len(prompt) prng_seed = jax.random.PRNGKey(seed) prompt_ids = pipeline.prepare_inputs(prompt) prompt_ids = shard(prompt_ids) # shard inputs and rng params = replicate(pipeline_params) prng_seed = jax.random.split(prng_seed, num_samples) output = pipeline(prompt_ids, params, prng_seed, num_inference_steps, guidance_scale=guidance_scale, jit=True) images = output.images images = np.asarray(images.reshape((num_samples,) + images.shape[-3:])) images = pipeline.numpy_to_pil(images) return images . Let&#39;s use our &lt;cat-toy&gt; concept in a prompt and see how it is looks in the generated image. . %%time prompt = &quot;an oil painting of a &lt;cat-toy&gt; in a town by the river in Andalucia&quot; images = generate(prompt, seed=0) . CPU times: user 2min 22s, sys: 1.9 s, total: 2min 24s Wall time: 2min 19s . def image_grid(imgs, rows, cols): assert len(imgs) == rows*cols w, h = imgs[0].size grid = Image.new(&#39;RGB&#39;, size=(cols*w, rows*h)) grid_w, grid_h = grid.size for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h)) return grid . image_grid(images, 1, 1) . As you can see getting a prompt to generate the right image we want is hard. The result have a silouete of cat but not the cat toy we expects. We have to try something different, in fact, some objects may need the concept at the begining, other styles work better at the end. . One simple thing we could try is to simplify the prompt as follows: . %%time prompt = &quot;&lt;cat-toy&gt; in Seville&quot; images = generate(prompt, seed=0) . CPU times: user 32.1 s, sys: 216 ms, total: 32.3 s Wall time: 31.5 s . image_grid(images, 1, 1) . The result is kind better as we have something close to our cat toy object but the image looks boring. We probably need to try modifying the prompt, for inspiration it is good to browse prompts used in lexica.art or playgroundai.com. . Now, let&#39;s try playing with other concepts from the Stable Diffusion Textual Inversion Concepts Library to generate images of different styles: . styles = [&#39;birb-style&#39;, &#39;midjourney-style&#39;, &#39;style-of-marc-allante&#39;] for style in styles: embeds_path, token_path1 = download_embeddings(f&#39;sd-concepts-library/{style}&#39;, style) token, embeds = load_embeds(embeds_path) tokenizer.add_tokens(token) add_token_embeddings(embeds, text_encoder_params) . %%time images = [] prompts = [f&quot;an oil painting of a town by the river in Andalucia in the style of &lt;{style}&gt;&quot; for style in styles] for prompt in prompts: images = images + generate(prompt, seed=0) . CPU times: user 2min 25s, sys: 1.27 s, total: 2min 26s Wall time: 2min 20s . image_grid(images, 1, 3) . That&#39;s all folks . Stable Diffusion is a very neat model, it allows us to generate fantastic pictures with a simple text prompt. It is very flexible and can be customized so it does not generate random images. In this article, we saw one way of conditioning the model output called Textual Inversion. Furthermore, we saw that this technique is easy to implement in Flax with the diffusers library and leveraging many of the public concepts available on Hugging Face&#39;s Concept Library. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc. .",
            "url": "https://dzlab.github.io/notebooks/flax/vision/diffusion/2022/12/24/Stable_Diffusion_Textual_Inversion.html",
            "relUrl": "/flax/vision/diffusion/2022/12/24/Stable_Diffusion_Textual_Inversion.html",
            "date": " • Dec 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Image2Image with Stable Diffusion in Flax",
            "content": "In previous posts we saw how to easily generate images from text in few lines of code using FlaxStableDiffusionPipeline (see link) and dived deep into the details of the diffusion loop (see link). . As we gained more more in depth understanding of Stable Diffusion, we can now be more dangerous and start experimenting. In this article, we will try to start the diffusion loop from a noised version of an input image instead of starting from random noise (aka image2image). . . Setup and Imports . Same as in previous Stable Diffusion articles, let&#39;s install packages, accept license for using Stable Diffusion and import modules. . %%capture %%bash pip install --upgrade scipy flax pip install --upgrade &quot;jax[cuda]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html pip install --upgrade diffusers transformers . !huggingface-cli login . _| _| _| _| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|_|_|_| _|_| _|_|_| _|_|_|_| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _|_|_|_| _| _| _| _|_| _| _|_| _| _| _| _| _| _|_| _|_|_| _|_|_|_| _| _|_|_| _| _| _| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _| _| _|_| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _| _| _| _|_|_| _|_|_|_| To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens . Token: Add token as git credential? (Y/n) n Token is valid. Your token has been saved to /root/.huggingface/token Login successful . import jax import jax.numpy as jnp import numpy as np from flax.jax_utils import replicate from flax.training.common_utils import shard from PIL import Image import matplotlib.pyplot as plt %matplotlib inline . from transformers import CLIPFeatureExtractor, CLIPTokenizer, FlaxCLIPTextModel . from diffusers import FlaxAutoencoderKL, FlaxUNet2DConditionModel, FlaxPNDMScheduler . Loading the model . Download the checkpoints for Stable Diffusion and instantiate the model components. . dtype = jax.numpy.bfloat16 . model_id = &quot;CompVis/stable-diffusion-v1-4&quot; revision = &quot;bf16&quot; # &quot;flax&quot; . tokenizer = CLIPTokenizer.from_pretrained(model_id, revision=revision, subfolder=&quot;tokenizer&quot;, dtype=dtype) text_encoder = FlaxCLIPTextModel.from_pretrained(model_id, revision=revision, subfolder=&quot;text_encoder&quot;, dtype=dtype) . vae, vae_params = FlaxAutoencoderKL.from_pretrained(model_id, revision=revision, subfolder=&quot;vae&quot;, dtype=dtype) unet, unet_params = FlaxUNet2DConditionModel.from_pretrained(model_id, revision=revision, subfolder=&quot;unet&quot;, dtype=dtype) . scheduler, scheduler_params = FlaxPNDMScheduler.from_pretrained(model_id, revision=revision, subfolder=&quot;scheduler&quot;) . Diffusion . For the diffusion we&#39;ll use a similar loop as in Stable Diffusion Deep Dive, except we will skip the first start_step steps. We will use a random choosen image as a starting point, add some noise to it and then do the remaining few denoising steps in the loop. . Setup . First, lets set a random seed for reproducibility . seed = 123 num_samples = jax.device_count() prng_seed = jax.random.PRNGKey(seed) prng_seed = jax.random.split(prng_seed, num_samples) . Set the guidance factor and total number of inference steps . guidance_scale = 7.5 #@param {type:&quot;slider&quot;, min:0, max:100, step:0.5} num_inference_steps = 30 #@param . Define some parameters for the diffusion loop . # init vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1) # call height = unet.config.sample_size * vae_scale_factor width = unet.config.sample_size * vae_scale_factor . text_encoder_params = None . Image . Get an image to use as the input . !curl -s -o input_image.jpeg https://images.unsplash.com/photo-1670139015746-832eaa4460c1?ixlib=rb-4.0.3&amp;dl=peter-thomas-mcV0gUPvGXE-unsplash.jpg&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=224&amp;q=224 . pil_image = Image.open(&#39;input_image.jpeg&#39;) . pil_image.resize((512, 512)) . We need a helper function to encode a PIL image into its latent representation . def pil_to_latents(pil_image): # Single image -&gt; single latent in a batch (so size 1, 4, 64, 64) image = np.asarray(pil_image) image = jax.image.resize(image, (512, 512, 3), &quot;bicubic&quot;) image = (image / 127.5 - 1.0).astype(np.float32) input_im = jnp.expand_dims(image, axis=0) input_im = jnp.transpose(input_im, (0, 3, 1, 2)) # encode the image latents = vae.apply({&quot;params&quot;: vae_params}, input_im, method=vae.encode) return 0.18215 * latents.latent_dist.sample(prng_seed) . We need a helper function to decode a latent representation into a PIL image . def latents_to_pil(latents): # scale and decode the image latents with vae latents = 1 / 0.18215 * latents images = vae.apply({&quot;params&quot;: vae_params}, latents, method=vae.decode).sample # convert JAX to numpy images = (images / 2 + 0.5).clip(0, 1).transpose(0, 2, 3, 1) images = np.asarray(images) # convert numpy array to PIL images = (images * 255).round().astype(&quot;uint8&quot;) if images.shape[-1] == 1: # special case for grayscale (single channel) images pil_images = [Image.fromarray(image.squeeze(), mode=&quot;L&quot;) for image in images] else: pil_images = [Image.fromarray(image) for image in images] return pil_images . Encode the image and check the shape of its latent representation . %%time encoded = pil_to_latents(pil_image) . CPU times: user 33.1 s, sys: 2.52 s, total: 35.6 s Wall time: 24.6 s . encoded.shape . (1, 64, 64, 4) . encoded = jnp.transpose(encoded, (0, 3, 1, 2)) encoded.shape . (1, 4, 64, 64) . We cannot use the encoded image as is, we need to add noise to it using the scheduler to a level equivalent to the target start step. As an example, let&#39;s visualize what it looks like to add a bit of noise of an image at the step 13. . Note: you can try a different step number to have a sense of how deteriorate is the image. . scheduler_state = scheduler.set_timesteps(scheduler_params, num_inference_steps=15, shape=encoded.shape) noise = jax.random.normal(prng_seed, shape=encoded.shape, dtype=jnp.float32) # Random noise sampling_step = 13 # Equivalent to step 13 out of 15 in the schedule above encoded_and_noised = scheduler.add_noise(encoded, noise, timesteps=scheduler_state.timesteps[sampling_step]) latents_to_pil(encoded_and_noised)[0] # Display . Prompt . Choose a prompt . prompt = &quot;a photo of abandoned cars in the desert&quot; . Let&#39;s tokenize then encode to tokens into embeddings . %%time # prepare_inputs text_input = tokenizer( prompt, padding=&quot;max_length&quot;, max_length=tokenizer.model_max_length, truncation=True, return_tensors=&quot;np&quot;, ) prompt_ids = text_input.input_ids prompt_ids . CPU times: user 1.4 ms, sys: 0 ns, total: 1.4 ms Wall time: 3.66 ms . array([[49406, 320, 1125, 539, 11227, 3346, 530, 518, 7301, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407]]) . %%time # get prompt text embeddings text_embeddings = text_encoder(prompt_ids, params=text_encoder_params)[0] . CPU times: user 2.1 s, sys: 97.1 ms, total: 2.2 s Wall time: 4.21 s . batch_size = prompt_ids.shape[0] max_length = prompt_ids.shape[-1] . We also need the embeddings of the blank text . uncond_input = tokenizer( [&quot;&quot;] * batch_size, padding=&quot;max_length&quot;, max_length=max_length, return_tensors=&quot;np&quot; ).input_ids uncond_embeddings = text_encoder(uncond_input, params=text_encoder_params)[0] . context = jnp.concatenate([uncond_embeddings, text_embeddings]) . latents_shape = ( batch_size, unet.in_channels, height // vae_scale_factor, width // vae_scale_factor, ) latents_shape . (1, 4, 64, 64) . Diffusion . Set the timestamps of the scheduler based on the number of steps . scheduler_state = scheduler.set_timesteps( scheduler_params, num_inference_steps=num_inference_steps, shape=latents_shape ) . As illustarted earlier we cannot use the input image as is, we need to prepare its latents by adding the appropriate amount of noise that matches the start step. . start_step = 10 . noise = jax.random.normal(prng_seed, shape=latents_shape, dtype=jnp.float32) # scale the initial noise by the standard deviation required by the scheduler noise = noise * scheduler.init_noise_sigma # apply noise to the latents of the input image latents = scheduler.add_noise(encoded, noise, timesteps=scheduler_state.timesteps[start_step]) . The diffusion step is unchanged from the Stable Diffusion original loop. For details check - Stable Diffusion Deep Dive. . def diffusion_step(step, args): latents, scheduler_state = args # For classifier free guidance, we need to do two forward passes. # Here we concatenate the unconditional and text embeddings into a single batch # to avoid doing two forward passes latents_input = jnp.concatenate([latents] * 2) t = jnp.array(scheduler_state.timesteps, dtype=jnp.int32)[step] timestep = jnp.broadcast_to(t, latents_input.shape[0]) latents_input = scheduler.scale_model_input(scheduler_state, latents_input, t) # predict the noise residual noise_pred = unet.apply( {&quot;params&quot;: unet_params}, jnp.array(latents_input), jnp.array(timestep, dtype=jnp.int32), encoder_hidden_states=context, ).sample # perform guidance noise_pred_uncond, noise_prediction_text = jnp.split(noise_pred, 2, axis=0) noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond) # compute the previous noisy sample x_t -&gt; x_t-1 latents, scheduler_state = scheduler.step(scheduler_state, noise_pred, t, latents).to_tuple() return latents, scheduler_state . Only difference is that instead of starting the diffusion loop from 0 we will start from 10 (or the step of your choice). . %%time latents, _ = jax.lax.fori_loop(start_step, num_inference_steps, diffusion_step, (latents, scheduler_state)) . CPU times: user 17min 29s, sys: 11 s, total: 17min 40s Wall time: 9min 36s . Now let&#39;s decode the latents and inspect the model output image. . %%time images = latents_to_pil(latents) . CPU times: user 1min, sys: 1.82 s, total: 1min 2s Wall time: 33.9 s . See how the model generate an image that corresponds to the text prompt and is very close to the input image. . images[0] . That&#39;s all folks . Stable Diffusion is a very cool model and can be easily customized to condition how the images are generate. In this article we saw how to use a source image and force the model to generate something that looks like it and corresponds to the prompt. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc. .",
            "url": "https://dzlab.github.io/notebooks/flax/vision/diffusion/2022/12/18/Stable_Diffusion_Image2Image_in_Flax.html",
            "relUrl": "/flax/vision/diffusion/2022/12/18/Stable_Diffusion_Image2Image_in_Flax.html",
            "date": " • Dec 18, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Deep Dive into Stable Diffusion - Part II",
            "content": ". In Part I we re-created the functionality of FlaxStableDiffusionPipeline step by step and gained a better understanding of the inner working of the diffusion loop. In Part II, we will inspect each of the main components of Stable Diffusion. . Setup and Imports . First, we need to install some libraries include diffusers and Flax. . %%capture %%bash pip install --upgrade scipy flax pip install --upgrade &quot;jax[cuda]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html pip install --upgrade diffusers transformers . Log into your Hugging Face account with an access token and accept the terms of the licence for this model - see model card. . !huggingface-cli login . _| _| _| _| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|_|_|_| _|_| _|_|_| _|_|_|_| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _|_|_|_| _| _| _| _|_| _| _|_| _| _| _| _| _| _|_| _|_|_| _|_|_|_| _| _|_|_| _| _| _| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _| _| _|_| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _| _| _| _|_|_| _|_|_|_| To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens . Token: Add token as git credential? (Y/n) n Token is valid. Your token has been saved to /root/.huggingface/token Login successful . Import the libraries we will use, e.g. matplotlib for plotting the resulting images. . import jax import jax.numpy as jnp import numpy as np from flax.jax_utils import replicate from flax.training.common_utils import shard from PIL import Image import matplotlib.pyplot as plt %matplotlib inline . from transformers import CLIPFeatureExtractor, CLIPTokenizer, FlaxCLIPTextModel . from diffusers import FlaxAutoencoderKL, FlaxUNet2DConditionModel, FlaxPNDMScheduler . Model and weights . Refer to Part I for more details on the model and the checkpoint we are choosing. Let&#39;s just run those cells to instantiate the components. . dtype = jax.numpy.float16 . model_id = &quot;CompVis/stable-diffusion-v1-4&quot; revision = &quot;bf16&quot; # &quot;flax&quot; . tokenizer = CLIPTokenizer.from_pretrained(model_id, revision=revision, subfolder=&quot;tokenizer&quot;, dtype=dtype) text_encoder = FlaxCLIPTextModel.from_pretrained(model_id, revision=revision, subfolder=&quot;text_encoder&quot;, dtype=dtype) . vae, vae_params = FlaxAutoencoderKL.from_pretrained(model_id, revision=revision, subfolder=&quot;vae&quot;, dtype=dtype) unet, unet_params = FlaxUNet2DConditionModel.from_pretrained(model_id, revision=revision, subfolder=&quot;unet&quot;, dtype=dtype) . scheduler, scheduler_params = FlaxPNDMScheduler.from_pretrained(model_id, revision=revision, subfolder=&quot;scheduler&quot;) . Components of Stable Diffusion . The following diagram illustrates the different components of Stable Diffusion and how they are combined to generate an image from text. For details on how those components work together to make a diffusion loop refer to Part I. . In this section we will play with some of those components to better understand them. . . First, let&#39;s initialize the random seed so we can reproduce the results. . seed = 123 num_samples = jax.device_count() prng_seed = jax.random.PRNGKey(seed) prng_seed = jax.random.split(prng_seed, num_samples) . The Text Encoder . The first component of the Stable Diffusion model is the Text Encoder that turns the prompt text into embeddings. . prompt = &quot;a photo of a car orbiting earth in van Gogh style&quot; . The tokenizer turns the prompt into token IDs. . token_ids = tokenizer.encode(prompt) . token_ids . [49406, 320, 1125, 539, 320, 1615, 523, 23016, 3475, 530, 2451, 19697, 1844, 49407] . Some tokens have special meaning, for instance the tokens: . &lt;|startoftext|&gt; with id 49406 is added to the begning of the prompt | &lt;|endoftext|&gt; with id 49407 is added to the end of the prompt | . tokenizer.decode(49406), tokenizer.decode(49407) . (&#39;&lt;|startoftext|&gt;&#39;, &#39;&lt;|endoftext|&gt;&#39;) . When the target max_length is much bigger than the prompt length, padding is added to the end using the &lt;|endoftext|&gt; token. . text_input = tokenizer( prompt, padding=&quot;max_length&quot;, max_length=tokenizer.model_max_length, truncation=True, return_tensors=&quot;np&quot;, ) . text_input.input_ids . array([[49406, 320, 1125, 539, 320, 1615, 523, 23016, 3475, 530, 2451, 19697, 1844, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407]]) . Using those token IDs we covert them into embeddings like this . text_embeddings = text_encoder(text_input.input_ids)[0] . text_embeddings.shape . (1, 77, 768) . The Autoencoder (AE) . The Autoencoder (AE) is a very important component of Stable Diffusion. Its main purpose is to &#39;encode&#39; an image into a latent representation, and also decode this latent back into the original image. By doing this, it is able to significaly reduce the input image size (in fact by a factor of 64) without loosing much of information. . This capability allows Stable Diffusion to perform the denoising on the latent representation instead of using the original image, hence the less memory footprint and compuation efficiency of this model. . In this section, we will walkthrough how AE can encode an image and decode it back without loosing information. . Encoding . !curl -s -o flower.jpeg https://images.unsplash.com/photo-1604085572504-a392ddf0d86a?ixlib=rb-1.2.1&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=224&amp;q=224 . pil_image = Image.open(&#39;flower.jpeg&#39;) . pil_image.resize((256, 256)) . Let&#39;s definte a helper function to convert a PIL image into latent embeddings using the AE Encoder. . def pil_to_latents(pil_image): # Single image -&gt; single latent in a batch (so size 1, 4, 64, 64) image = np.asarray(pil_image) image = jax.image.resize(image, (512, 512, 3), &quot;bicubic&quot;) image = (image / 127.5 - 1.0).astype(np.float32) input_im = jnp.expand_dims(image, axis=0) input_im = jnp.transpose(input_im, (0, 3, 1, 2)) # encode the image latents = vae.apply({&quot;params&quot;: vae_params}, input_im, method=vae.encode) return 0.18215 * latents.latent_dist.sample(prng_seed) . Take the image and pass it through the AE encoder to generate its latent embeddings . %%time flower_latents = pil_to_latents(pil_image) . CPU times: user 35.1 s, sys: 1.2 s, total: 36.3 s Wall time: 26.5 s . Latent . The latent is of shape 64 x 64 x 4, let&#39;s define the following helper function to plot each channel. . def plot_latents(latents, figsize=(10, 5), maintitle=None): latents = latents.squeeze() rows, cols = 1, latents.shape[-1] f = plt.figure(figsize=figsize) if maintitle is not None: plt.suptitle(maintitle, fontsize=10) for i in range(cols): sp = f.add_subplot(rows, cols, i+1) sp.axis(&#39;Off&#39;) sp.set_title(&#39;Channel &#39;+str(i), fontsize=16) img = np.asarray(latents[:, :, i]) plt.imshow(img) . Plotting the different channel we can see that the latent preserved a lot of the characteristics of the original flower image. . plot_latents(flower_latents) . Decoding . Let&#39;s definte the following helper function to convert the model output latent embeddings into an actual PIL image for plotting. . def latents_to_pil(latents): # scale and decode the image latents with vae latents = 1 / 0.18215 * latents images = vae.apply({&quot;params&quot;: vae_params}, latents, method=vae.decode).sample # convert JAX to numpy images = (images / 2 + 0.5).clip(0, 1).transpose(0, 2, 3, 1) images = np.asarray(images) # convert numpy array to PIL images = (images * 255).round().astype(&quot;uint8&quot;) if images.shape[-1] == 1: # special case for grayscale (single channel) images pil_images = [Image.fromarray(image.squeeze(), mode=&quot;L&quot;) for image in images] else: pil_images = [Image.fromarray(image) for image in images] return pil_images . From those latents we can go back to our exact original picture (more or less). . %%time images = latents_to_pil(flower_latents) . CPU times: user 1min 5s, sys: 1.41 s, total: 1min 7s Wall time: 54.2 s . See how AE decoder generate an image very close to the original one. . images[0].resize((256, 256)) . The Scheduler . As part of the diffusion loop, we add some noise to an image an then let the model try to predict a denoised image. If we always add too much noise, the model will have harder time denoising the image. If we add a tiny fraction, the model won&#39;t be able to do much with the random starting points we use for sampling. The Scheduler is used to regulate the amount of noise to apply at each step of the diffusion loop, according to some distribution. . In this section, we will use the previously created scheduler to add some noise to our test image. The amount of noise to add corresponds to the noise from the step with number sampling_step out of total_steps. . encoded = flower_latents . Equivalent to step 10 out of 15 in the scheduler above . total_steps = 15 # @param sampling_step = 10 # @param . scheduler_state = scheduler.set_timesteps(scheduler_params, num_inference_steps=total_steps, shape=encoded.shape) noise = jax.random.normal(prng_seed, shape=encoded.shape, dtype=jnp.float32) # Random noise encoded_and_noised = scheduler.add_noise(encoded, noise, timesteps=scheduler_state.timesteps[sampling_step]) . Decode latents into an image . noised = latents_to_pil(encoded_and_noised)[0] . See how the image looks like when the scheduler applies noise at step number 10: . noised.resize((256, 256)) # Display . That&#39;s all folks . Stable Diffusion is a complex model that comprises of many components. In this article, we played with some of those components in Flax. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc. .",
            "url": "https://dzlab.github.io/notebooks/flax/vision/diffusion/2022/12/17/Stable_Diffusion_in_Flax_Deep_Dive_Part_II.html",
            "relUrl": "/flax/vision/diffusion/2022/12/17/Stable_Diffusion_in_Flax_Deep_Dive_Part_II.html",
            "date": " • Dec 17, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Deep Dive into Stable Diffusion - Part I",
            "content": ". Stable Diffusion is a powerful text-to-image model. Its success lead many websites and tools to provide easy access to it so that anyone can use it to generate images from text. It is also integrated into the Huggingface diffusers library where generating images in python can be as simple as: . from diffusers import FlaxStableDiffusionPipeline pipeline, pipeline_params = FlaxStableDiffusionPipeline.from_pretrained( &quot;CompVis/stable-diffusion-v1-4&quot;, revision=&quot;bf16&quot;, dtype=jax.numpy.bfloat16 ) prng_seed = jax.random.split(jax.random.PRNGKey(seed), 1) prompt_ids = pipeline.prepare_inputs([prompt]) images = pipeline(prompt_ids, params, prng_seed).images images = pipeline.numpy_to_pil(images) . You can refer to this article for a complete walk-through of how to use the FlaxStableDiffusionPipeline API to generate images from your prompt. . In Part I we will dig into the actual code behind such an easy-to-use API to better understand the Stable Diffusion model. First, we will re-create the functionality of FlaxStableDiffusionPipeline step by step, and then we will inspect its main components. . By the end of this notebook we will have a good undertanding of how Stable Diffusion works and be able to tweak and modify the inner working. . Introduction . Stable Diffusion is trained to remove noise from an image. By repeating this process mutiple times, it is able to generate images of great quality. How much noise is removed in every step depends on the input textual description. This helps guide the model toward generating an image that matches the input description. . This is how Stable Diffusion generate text from image . The textual description of the target image is passed through a text encoder to generate the text embeddings | Random noisy image latent (think of it as a compressed of shape 64 x 64) is generate as a starting point | The noise is passed to a U-Net model along with the text embeddings | The U-Net generates a denoised image latent conditioned by the text embeddings | The denoised predicted image is passed through a Sechduler which will add little bit of noise based on the current number of steps | The output of the scheduler is used as input for next denoising step | The denoising loop is repeated couple steps | The final noisy image latent is passed to a Decoder which will generate the final image with shape 512 x 512. | . The above diagram illustrates the different components of Stable Diffusion and how they are combined to generate an image from text. The remaining of this artcile we will dive deeper into the implementation details. . Setup and Imports . First, we need to install some libraries include diffusers and Flax. . %%capture %%bash pip install --upgrade scipy flax pip install --upgrade &quot;jax[cuda]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html pip install --upgrade diffusers transformers . Log into your Hugging Face account with an access token and accept the terms of the licence for this model - see model card. . !huggingface-cli login . _| _| _| _| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|_|_|_| _|_| _|_|_| _|_|_|_| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _|_|_|_| _| _| _| _|_| _| _|_| _| _| _| _| _| _|_| _|_|_| _|_|_|_| _| _|_|_| _| _| _| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _| _| _|_| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _| _| _| _|_|_| _|_|_|_| To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens . Token: Add token as git credential? (Y/n) n Token is valid. Your token has been saved to /root/.huggingface/token Login successful . Import the libraries we will use, e.g. matplotlib for plotting the resulting images. . import jax import jax.numpy as jnp import numpy as np from flax.jax_utils import replicate from flax.training.common_utils import shard from PIL import Image import matplotlib.pyplot as plt %matplotlib inline . from transformers import CLIPFeatureExtractor, CLIPTokenizer, FlaxCLIPTextModel . from diffusers import FlaxAutoencoderKL, FlaxUNet2DConditionModel, FlaxPNDMScheduler . Model and weights . This will download and set up the relevant models and components we&#39;ll be using. Let&#39;s just run this for now and move on to the next section to check that it all works before diving deeper. . Let&#39;s first instantiate the different components of Stable Diffusion which are representation with the following classes: . CLIPTokenizer used to transform the text prompt into token IDs. | FlaxCLIPTextModel used to encode token IDs into the corresponding embeddings | FlaxAutoencoderKL or Variational Autoencoder used for encoding/decoding images to/from a latent representation | FlaxUNet2DConditionModel conditional U-Net model used to denoise the latent representation of an image | FlaxPNDMScheduler used to add noise to the latent representation of an image | . We will download and set up the previous components using checkpoints of Stable Diffusion from CompVis/stable-diffusion-v1-4. . Note: We will use half-precision (i.e. revision bf16) for a faster weights download, to save on memory and avoid issues like slower predictions or OOM. If you want use float32 precision instead then use revision flax. . dtype = jax.numpy.float16 . model_id = &quot;CompVis/stable-diffusion-v1-4&quot; revision = &quot;bf16&quot; # &quot;flax&quot; . tokenizer = CLIPTokenizer.from_pretrained(model_id, revision=revision, subfolder=&quot;tokenizer&quot;, dtype=dtype) text_encoder = FlaxCLIPTextModel.from_pretrained(model_id, revision=revision, subfolder=&quot;text_encoder&quot;, dtype=dtype) . vae, vae_params = FlaxAutoencoderKL.from_pretrained(model_id, revision=revision, subfolder=&quot;vae&quot;, dtype=dtype) unet, unet_params = FlaxUNet2DConditionModel.from_pretrained(model_id, revision=revision, subfolder=&quot;unet&quot;, dtype=dtype) . scheduler, scheduler_params = FlaxPNDMScheduler.from_pretrained(model_id, revision=revision, subfolder=&quot;scheduler&quot;) . Note: Instead of individually loading those components we could also load a pipeline and then access them using pipe.unet, pipe.vae and so on. Something like this: . from diffusers import FlaxStableDiffusionPipeline pipe, pipe_params = FlaxStableDiffusionPipeline.from_pretrained(model_id, revision=revision, dtype=jax.numpy.bfloat16) unet, vae = pipe.unet, pipe.vae . Diffusion Loop . First, let&#39;s initialize the random seed so we can reproduce the results. . seed = 123 num_samples = jax.device_count() prng_seed = jax.random.PRNGKey(seed) prng_seed = jax.random.split(prng_seed, num_samples) . The following are two important parameters: . num_inference_steps corresponds to the number of steps to take in the diffusion loop | guidance_scale is used to regulate how the noisy predicted image will be update | . guidance_scale = 7.5 #@param {type:&quot;slider&quot;, min:0, max:100, step:0.5} num_inference_steps = 30 #@param . Choose a random text prompt that describes the target image . prompt = &quot;a photo of a car orbiting earth in van Gogh style&quot; . text_encoder_params = None . Initialize some parameters . # init vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1) # call height = unet.config.sample_size * vae_scale_factor width = unet.config.sample_size * vae_scale_factor . Now, we need to use the tokenizer and text_encoder to calculate the embeddings for the input text prompt. . %%time text_input = tokenizer( prompt, padding=&quot;max_length&quot;, max_length=tokenizer.model_max_length, truncation=True, return_tensors=&quot;np&quot;, ) prompt_ids = text_input.input_ids text_embeddings = text_encoder(prompt_ids, params=text_encoder_params)[0] batch_size = prompt_ids.shape[0] max_length = prompt_ids.shape[-1] . We also need to calculate the embeddings for the blank text. This embedding will be later combined with the predicted noiy image using the previous guidance_factor. . uncond_input = tokenizer( [&quot;&quot;] * batch_size, padding=&quot;max_length&quot;, max_length=max_length, return_tensors=&quot;np&quot; ).input_ids uncond_embeddings = text_encoder(uncond_input, params=text_encoder_params)[0] . Here we concatenate the unconditional and text embeddings into a single batch to avoid doing two forward passes . context = jnp.concatenate([uncond_embeddings, text_embeddings]) . Now we define a diffusion step where we use the unet component to generate noise conditioned by the prompt embeddings. Then, combine the predicted noise with the embeddings form the blank text. . def diffusion_step(step, args): latents, scheduler_state = args latents_input = jnp.concatenate([latents] * 2) t = jnp.array(scheduler_state.timesteps, dtype=jnp.int32)[step] timestep = jnp.broadcast_to(t, latents_input.shape[0]) latents_input = scheduler.scale_model_input(scheduler_state, latents_input, t) # predict the noise residual noise_pred = unet.apply( {&quot;params&quot;: unet_params}, jnp.array(latents_input), jnp.array(timestep, dtype=jnp.int32), encoder_hidden_states=context, ).sample # perform guidance noise_pred_uncond, noise_prediction_text = jnp.split(noise_pred, 2, axis=0) noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond) # compute the previous noisy sample x_t -&gt; x_t-1 latents, scheduler_state = scheduler.step(scheduler_state, noise_pred, t, latents).to_tuple() return latents, scheduler_state . latents_shape = ( batch_size, unet.in_channels, height // vae_scale_factor, width // vae_scale_factor, ) . The following defines the starting latents which is is randomly sampled from a Normal distribution. . latents = jax.random.normal(prng_seed, shape=latents_shape, dtype=jnp.float32) . Here is the diffusion loop where we will call the previously defined diffusion_step as many steps as num_inference_steps. . # set the timestamps based on the number of steps scheduler_state = scheduler.set_timesteps( scheduler_params, num_inference_steps=num_inference_steps, shape=latents.shape ) # scale the initial noise by the standard deviation required by the scheduler latents = latents * scheduler.init_noise_sigma latents, _ = jax.lax.fori_loop(0, num_inference_steps, diffusion_step, (latents, scheduler_state)) . latents.shape . (1, 4, 64, 64) . Let&#39;s definte the following helper function to convert the model output latent embeddings into an actual PIL image for plotting. . def latents_to_pil(latents): # scale and decode the image latents with vae latents = 1 / 0.18215 * latents images = vae.apply({&quot;params&quot;: vae_params}, latents, method=vae.decode).sample # convert JAX to numpy images = (images / 2 + 0.5).clip(0, 1).transpose(0, 2, 3, 1) images = np.asarray(images) # convert numpy array to PIL images = (images * 255).round().astype(&quot;uint8&quot;) if images.shape[-1] == 1: # special case for grayscale (single channel) images pil_images = [Image.fromarray(image.squeeze(), mode=&quot;L&quot;) for image in images] else: pil_images = [Image.fromarray(image) for image in images] return pil_images . Now we use the Autoencoder (through the previous helper function) to generate an image from its latent representation . %%time images = latents_to_pil(latents) . CPU times: user 1min 4s, sys: 1.44 s, total: 1min 6s Wall time: 38.4 s . Finally, we can check the resuling image . images[0] . That&#39;s all folks . Stable Diffusion is a complex model that comprises of many components and uses clever tricks so it can be trained with relatively cheap hardware compared to other difussion models (e.g. Google&#39;s Imagen or OpenAI’s DALL-E 2). . In this article, we dived deep into the Flax implementation of the Stable Diffusion model to better understand how it works. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc. .",
            "url": "https://dzlab.github.io/notebooks/flax/vision/diffusion/2022/12/16/Stable_Diffusion_in_Flax_Deep_Dive_Part_I.html",
            "relUrl": "/flax/vision/diffusion/2022/12/16/Stable_Diffusion_in_Flax_Deep_Dive_Part_I.html",
            "date": " • Dec 16, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Mean Shift clustering with JAX",
            "content": "Clustering techniques are unsupervised learning algorithms that try to group unlabelled data that look similar into groups (also called clusters). In this article, we will have a detailed look at one of those techniques and then implement it in JAX. . . Setup and import . First, lets import JAX and other needed libraries, then initialize JAX random number generator. . from functools import partial import jax import jax.numpy as jnp import math import matplotlib.pyplot as plt . seed = 123 key = jax.random.PRNGKey(seed) . WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . jnp.set_printoptions(precision=3, threshold=5, linewidth=200) . Data . We need some data to illustrate how the clustering algorithm works. We could download something from the internet or randomly generate observations. . n_clusters = 7 n_samples = 250 . To generate our data, we&#39;re going to pick 7 random points that represent the actual clusters centroids, then for each one of those centroids we generate few random points around them. . centroids = jax.random.uniform(key, shape=(n_clusters, 2))*100 - 50 . The observations around a centroid will be randomly sampled using Multivariate normal distribution. Which as the name suggests, will allow us to generate an observation vector where each element is randomly sampled following Normal distribution. . . JAX allows to sample following Multivariate normal distribution thanks to jax.random.multivariate_normal(). . def sample(mean): cov = jnp.diag(jnp.array([5, 5])) shape = [n_samples] return jax.random.multivariate_normal(key, mean, cov, shape) . sample(centroids[0]).shape, centroids[0].shape . ((250, 2), (2,)) . slices = [sample(mean) for mean in centroids] data = jnp.concatenate(slices) data.shape . (1750, 2) . To have a better sense around the generated data, we plot each cluster and its centroid as follows. . def plot_data(centroids, data, n_samples, ax=None): if not ax: _, ax = plt.subplots() for i, centroid in enumerate(centroids): samples = data[n_samples*i: n_samples*(i+1)] ax.scatter(samples[:, 0], samples[:, 1], s=1) # plot samples ax.plot(*centroid, markersize=10, marker=&#39;x&#39;, color=&#39;k&#39;, mew=5) ax.plot(*centroid, markersize=10, marker=&#39;x&#39;, color=&#39;m&#39;, mew=2) . plot_data(centroids, data, n_samples) . We can also plot the data along the central point which has equal distance to each point in the data. . midp = data.mean(axis=0) midp . DeviceArray([ 9.896, -14.421], dtype=float32) . plot_data([midp]*n_clusters, data, n_samples) . Mean shift . Mean shift is a less known clustering algorithm that has some interesting advantages compared to the more popular k-means algorithm: . Instead of requiring the exact number of clusters ahead of time, it requires a bandwidth to be specified, which can be easily chosen automatically | Out of the box is able to handle clusters of any shape (e.g. circles or moon shapes like below), whereas k-means (without using special extensions) can properly handle only clusters of a ball shape. | . from sklearn.datasets import make_circles, make_moons . # Cricles X1 = make_circles(factor=0.5, noise=0.05, n_samples=1500) # Moons X2 = make_moons(n_samples=1500, noise=0.05) fig, ax = plt.subplots(1, 2) for i, X in enumerate([X1, X2]): fig.set_size_inches(11, 5) ax[i].scatter(X[0][:, 0], X[0][:, 1]) plt.tight_layout(); . MeanShift components . The MeanShift clustering algorithm works as follows: . For each data point $x_i$ in the sample $X$, find the distance $d_{ij}$ between $x_i$ and every other point $x_j$ in $X$. i.e. $d_{ij} = | x_i - x_j |$ | Calculate weights $w_{ij}$ for each point $x_i$ in $X$ by applying the Gaussian kernel (with standard deviation set to bandwidth) to that point&#39;s distance to $x_j$. | Update x as the weighted average of all other points in X, weighted based on the previous step | The algorithm converge iteratively by pushing closer points even closer until they are next to each other. . Note: . This weighting approach penalizes points further away from each other | The rate at which the weights fall to zero is determined by the bandwidth. | The value of bandwidth should be choosen so that it covers one third of the data. | . Distance . The first component of MeanShift is the distance function, which is simply the Euclidean distance (also known as Norm 2 distance) and defined as follows: $ d left( x_i, x_j right) = sqrt { sum _{k=1}^{K} left( x_{ik}-x_{jk} right)^2 } $ where $x_i$ and $x_j$ are two observation arrays of dimension $K$. . The rest of this section implments the Norm2 distance in JAX. . X = data.clone() x0 = data[0] x0.shape, X.shape . ((2,), (1750, 2)) . X[None].shape, X[:, None].shape, (X[None]-X[:, None]).shape . ((1, 1750, 2), (1750, 1, 2), (1750, 1750, 2)) . dist = jnp.sqrt(((X[None]-X[:, None])**2).sum(axis=1)) dist.shape . (1750, 2) . def distance(X, x): diff = (X - x) if len(x.shape) == 1 else (X[None]-x[:, None]) return jnp.sqrt((diff**2).sum(axis=-1)) . X[2].shape, X[:2].shape, X[:2][:, None].shape . ((2,), (2, 2), (2, 1, 2)) . distance(X, X).shape, distance(X, X[:10]).shape, distance(X, X[0]).shape . ((1750, 1750), (10, 1750), (1750,)) . Gaussian kernel . MeanShift uses the Gaussian kernel to calculate the weights by applying it to the distance between $x_i$ and $x_j$ as follows $w_{ij} = varphi( | x_i - x_j |)$. It is defined by the following equation: . $$ varphi(z) = frac{1}{ sigma* sqrt{2 pi}}e^{ frac{-z^2}{2* sigma^2}}$$ . Note: $ sigma$ is the standard deviation, and $ mu$ (i.e. the mean) is 0 . In JAX, it is implemented as follows: . def gaussian(x, bandwidth, mean=0): return jnp.exp(-0.5 * ((x-mean)/bandwidth)**2) / (bandwidth*jnp.sqrt(2*math.pi)) . Let&#39;s plot the above function to have a better sence of how its output looks like . def plot_func(f): x=jnp.linspace(0, 10, 100) plt.plot(x, f(x)) . With a bandwidth of value 2.5 we get the following plot. . plot_func(partial(gaussian, bandwidth=2.5)) . Notice how the output of the gaussion follows a decreasing line then literally becomes 0 for input greater or equal to 8. In fact, we can approximate this Gaussion with a much faster to calculate function defined as follows: . def tri(x, i): return (-x + i).clip(0)/i . You can see from the plot that the output looks very similar to a gaussian. . plot_func(partial(tri, i=8)) . Before going further let&#39;s try the gaussian on some inputs for validation . dist_0 = jnp.sqrt(((x0-X)**2).sum(axis=1)) dist_0.shape, dist_0 . ((1750,), DeviceArray([ 0. , 3.869, 5.187, ..., 81.709, 76.189, 83.284], dtype=float32)) . weight_0 = gaussian(dist_0, 2.5) weight_0.shape, weight_0 . ((1750,), DeviceArray([0.16 , 0.048, 0.019, ..., 0. , 0. , 0. ], dtype=float32)) . weight_0[:,None] * X . DeviceArray([[ 7.814, 4.246], [ 2.522, 1.374], [ 0.812, 0.485], ..., [-0. , -0. ], [-0. , 0. ], [-0. , 0. ]], dtype=float32) . Calculating the weights . Now we can finally claculate the weights . weight = gaussian(distance(X, X[:10]), 2) . weight_tri = tri(distance(X, X[:10]), 8) . weight.shape, weight_tri.shape, X.shape . ((10, 1750), (10, 1750), (1750, 2)) . The weight matrix is used in the Mean Shift algorithm to normalize X as follows: . num = jnp.dot(weight, X) div = weight.sum(-1, keepdims=True) X_out = num/div . num.shape, div.shape, X_out.shape, X_out . ((10, 2), (10, 1), (10, 2), DeviceArray([[48.542, 25.583], [50.357, 26.867], [45.713, 25.075], ..., [47.695, 24.937], [47.378, 23.747], [47.36 , 22.198]], dtype=float32)) . weight.shape, weight.sum(axis=1).shape . ((10, 1750), (10,)) . Putting everything together . After defining all the components, the following method group them to apply one step of the Mean Shift algorithm on a batch of oberservations . def batched_meanshift_fn(X, bw=2): @jax.jit def apply(Xb): wb = gaussian(distance(X, Xb), bw) Xb_out = jnp.dot(wb, X) / wb.sum(-1, keepdims=True) return Xb_out return apply . For reference this is the expected shape of each of the vectors that the above function manipulate: . array shape . X | (N, 2) | . Xb | (batch_size, 2) | . wb | (batch_size, N) | . Xb_out | (batch_size, 2) | . func = batched_meanshift_fn(X, 2) func(X[:10]) . DeviceArray([[48.542, 25.583], [50.357, 26.867], [45.713, 25.075], ..., [47.695, 24.937], [47.378, 23.747], [47.36 , 22.198]], dtype=float32) . func = batched_meanshift_fn(X, 2) func(X[0]) . ((2,), DeviceArray([48.542, 25.583], dtype=float32)) . Manually batching the data . Even if it is slower, we should first try the algorithm on manually batched data to check that the final result matches the expectations . def meanshift_step_1(step, args): X, bs, bw = args n = X.shape[0] batches = [] batch_apply = batched_meanshift_fn(X, bw) for i in range(0, n, bs): s = slice(i, min(i+bs, n)) Xb = batch_apply(X[s]) batches.append(Xb) X_out = jnp.concatenate(batches, axis=0) return (X_out, bs, bw) def meanshift_1(data, bs=500, bw=2, steps=5): X = data.clone() Xs = [X] for i in range(steps): X, _, _ = meanshift_step_1(i, (X, bs, bw)) Xs.append(X) return X, Xs . Check how long it takes to run this implementation using the default batch size of 500. . %%time X_out, _ = meanshift_1(data) . CPU times: user 1.07 s, sys: 10.2 ms, total: 1.08 s Wall time: 895 ms . plot_data(centroids+3, X_out, n_samples) . The implementation is slower when using fews observations by batch. It is the worst when using the lowest batch size of 1. . %%time X_out, _ = meanshift_1(data, 1) . CPU times: user 5.12 s, sys: 159 µs, total: 5.12 s Wall time: 5.12 s . plot_data(centroids+3, X_out, n_samples) . Increasing the batch size, the algorithm finishes earlier. . %%time X_out, _ = meanshift_1(data, 1000) . CPU times: user 1.04 s, sys: 12.2 ms, total: 1.05 s Wall time: 884 ms . plot_data(centroids+3, X_out, n_samples) . Auto-vectorization with vmap . We can mush faster when using JAX vectorizing map vmap. This will allow us to run a function on each element of the array in parallel. . def meanshift_step_2(step, args): X, bw = args func = batched_meanshift_fn(X, bw) X_out = jax.vmap(func)(X) return (X_out, bw) def meanshift_2(data, bw=2, steps=5): X = data.clone() X, _ = jax.lax.fori_loop(0, steps, meanshift_step_2, (X, bw)) return X . %%time X_out = meanshift_2(data, n_clusters*n_samples) . CPU times: user 406 ms, sys: 0 ns, total: 406 ms Wall time: 223 ms . . Warning: we are using JAX with a CPU backend, hence it took our implementation that much of time to finish. We could benefit of vmap a lot more when using JAX with a GPU or TPU backend. . plot_data(centroids+3, X_out, n_samples) . MeanShift steps animated . Because Mean Shift is an iterative algorithm, we can visualize how the clusters change on every step. . from matplotlib.animation import FuncAnimation from IPython.display import HTML . _, Xs = meanshift_1(data, 500) . def do_one(d): X = Xs[d] ax.clear() plot_data(centroids+3, X, n_samples, ax=ax) . fig,ax = plt.subplots() ani = FuncAnimation(fig, do_one, frames=5, interval=500, repeat=False) plt.close() HTML(ani.to_jshtml()) . &lt;/input&gt; Once Loop Reflect That&#39;s all folks . We have seen that MeanShift can be easily implemented in JAX. Similarly we could easily implement with JAX any of the other clustering algorithms: k-means clustering, dbscan, locality sensitive hashing. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc. .",
            "url": "https://dzlab.github.io/notebooks/jax/clustering/2022/12/15/MeanShift_with_Jax.html",
            "relUrl": "/jax/clustering/2022/12/15/MeanShift_with_Jax.html",
            "date": " • Dec 15, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Interpolation with Stable Diffusion in Flax",
            "content": "Stable Diffusion is a powerful model that can used to generate an image form random text prompts. Hugging Face&#39;s diffusers library provides access to such model in a very convenient way. . In this article we will use this library to load the Flax-based Stable Diffusion model and its weights to generate a sequence of interpolated images. . . Setup and Imports . First, we need to install some libraries include diffusers and JAX/Flax. . Note: we will use JAX with GPU backend, alternatively you can use TPU backend by initializing JAX like this import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() . %%capture %%bash pip install --upgrade scipy flax pip install --upgrade &quot;jax[cuda]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html pip install --upgrade diffusers transformers . To be able to use Stable Diffusion we need to login to Hugging Face and accept the terms of the licence for this model. . Note: to login you will need a Token that you can get from the settings page of your Hugging Face account. . !huggingface-cli login . _| _| _| _| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|_|_|_| _|_| _|_|_| _|_|_|_| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _|_|_|_| _| _| _| _|_| _| _|_| _| _| _| _| _| _|_| _|_|_| _|_|_|_| _| _|_|_| _| _| _| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _| _| _|_| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _| _| _| _|_|_| _|_|_|_| To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens . Token: Add token as git credential? (Y/n) n Token is valid. Your token has been saved to /root/.huggingface/token Login successful . import os import jax import jax.numpy as jnp import numpy as np from flax.jax_utils import replicate from flax.training.common_utils import shard from PIL import Image from tqdm.auto import tqdm import matplotlib.pyplot as plt %matplotlib inline . Generating many images with Stable Diffusion in Colab can take time, so to save progress if we had to restart we need to mount Google Drive (or other persistent storage). . from google.colab import drive drive.mount(&#39;/content/drive/&#39;, force_remount=True) . Mounted at /content/drive/ . PATH = &#39;/content/drive/My Drive/Colab Notebooks/diffusion&#39; . Model and components . Now, we can instanciate a Flax Diffusion pipeline and load the propoer weights. . from transformers import CLIPFeatureExtractor, CLIPTokenizer, FlaxCLIPTextModel from diffusers import FlaxAutoencoderKL, FlaxUNet2DConditionModel, FlaxPNDMScheduler . dtype = jax.numpy.float16 . We will use the checkpoints of Stable Diffusion from CompVis/stable-diffusion-v1-4 with half-precision (i.e. revision bf16) to save on memory and avoid issues like OOM. . Note: if you want to use float32 precision then replace revision with flax . model_id = &quot;CompVis/stable-diffusion-v1-4&quot; revision = &quot;bf16&quot; # &quot;flax&quot; . To have more control on the difusion loop we will not use FlaxStableDiffusionPipeline as is but instead instanciate the components of the model individually. . tokenizer = CLIPTokenizer.from_pretrained(model_id, revision=revision, subfolder=&quot;tokenizer&quot;, dtype=dtype) text_encoder = FlaxCLIPTextModel.from_pretrained(model_id, revision=revision, subfolder=&quot;text_encoder&quot;, dtype=dtype) . vae, vae_params = FlaxAutoencoderKL.from_pretrained(model_id, revision=revision, subfolder=&quot;vae&quot;, dtype=dtype) unet, unet_params = FlaxUNet2DConditionModel.from_pretrained(model_id, revision=revision, subfolder=&quot;unet&quot;, dtype=dtype) . scheduler, scheduler_params = FlaxPNDMScheduler.from_pretrained(model_id, revision=revision, subfolder=&quot;scheduler&quot;) . Initialize the random seed for JAX. . seed = 123 num_samples = jax.device_count() prng_seed = jax.random.PRNGKey(seed) prng_seed = jax.random.split(prng_seed, num_samples) . Set the number of steps in the diffusion loop that are needed to generate one image, as well as the guidance factor. . You can learn more about the effects of those parameters in this article. . guidance_scale = 8 #@param {type:&quot;slider&quot;, min:0, max:100, step:0.5} num_inference_steps = 30 #@param . text_encoder_params = None . Helper functions . We need to convert the input text prompt into tokens and then into embeddings as expected by Stable Diffusion model. . def embed(prompt): # prepare_inputs text_input = tokenizer( prompt, padding=&quot;max_length&quot;, max_length=tokenizer.model_max_length, truncation=True, return_tensors=&quot;np&quot;, ) prompt_ids = text_input.input_ids # get prompt text embeddings text_embeddings = text_encoder(prompt_ids, params=text_encoder_params)[0] batch_size = prompt_ids.shape[0] max_length = prompt_ids.shape[-1] uncond_input = tokenizer( [&quot;&quot;] * batch_size, padding=&quot;max_length&quot;, max_length=max_length, return_tensors=&quot;np&quot; ).input_ids uncond_embeddings = text_encoder(uncond_input, params=text_encoder_params)[0] context = jnp.concatenate([uncond_embeddings, text_embeddings]) return context . The following function defines one diffusion step: . Predict noise from the input prompt and noise embeddings | Combine this predicted noise with the noise of a blank string using guidance factor | Pass everthing to a scheduler to calculate the embeddings of a previous noise | def diffusion_step(step, args): latents, context, scheduler_state = args latents_input = jnp.concatenate([latents] * 2) t = jnp.array(scheduler_state.timesteps, dtype=jnp.int32)[step] timestep = jnp.broadcast_to(t, latents_input.shape[0]) latents_input = scheduler.scale_model_input(scheduler_state, latents_input, t) # predict the noise residual noise_pred = unet.apply( {&quot;params&quot;: unet_params}, jnp.array(latents_input), jnp.array(timestep, dtype=jnp.int32), encoder_hidden_states=context, ).sample # perform guidance noise_pred_uncond, noise_prediction_text = jnp.split(noise_pred, 2, axis=0) noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond) # compute the previous noisy sample x_t -&gt; x_t-1 latents, scheduler_state = scheduler.step(scheduler_state, noise_pred, t, latents).to_tuple() return latents, context, scheduler_state . Because we will generate many images for simplification and reuse we group the diffusion logic in the following helper function . def diffuse(context, init_noise): latents = init_noise # set the timestamps based on the number of steps scheduler_state = scheduler.set_timesteps( scheduler_params, num_inference_steps=num_inference_steps, shape=latents.shape ) # scale the initial noise by the standard deviation required by the scheduler latents = latents * scheduler.init_noise_sigma latents, _, _ = jax.lax.fori_loop(0, num_inference_steps, diffusion_step, (latents, context, scheduler_state)) return latents . Set some global parameters like the expected shape of the embeddings. . vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1) height = unet.config.sample_size * vae_scale_factor width = unet.config.sample_size * vae_scale_factor batch_size = 1 latents_shape = ( batch_size, unet.in_channels, height // vae_scale_factor, width // vae_scale_factor, ) . The output of Stable Diffusion is an embedding, the following function process it properly and convert it into a PIL image. . def latents_to_pil(latents): # scale and decode the image latents with vae latents = 1 / 0.18215 * latents images = vae.apply({&quot;params&quot;: vae_params}, latents, method=vae.decode).sample # convert JAX to numpy images = (images / 2 + 0.5).clip(0, 1).transpose(0, 2, 3, 1) images = np.asarray(images) # convert numpy array to PIL images = (images * 255).round().astype(&quot;uint8&quot;) if images.shape[-1] == 1: # special case for grayscale (single channel) images pil_images = [Image.fromarray(image.squeeze(), mode=&quot;L&quot;) for image in images] else: pil_images = [Image.fromarray(image) for image in images] return pil_images . Interpolation . The following illustration explains the interpolation approach: . We generate the embeddings for the prompts of the source (e.g. a photo of a model t) and target (e.g. a photo of a mustang) images | We interpolate many new mbeddings between the previous two | For each of those embeddings we generate the corresponding image | We group the resulting images to generate a video or gif | . First, generate the embeddings of the source and target prompts . %%time prompt_1 = &quot;a realistic photo of a ford model t&quot; text_embed_1 = embed(prompt_1) noise_1 = jax.random.normal(prng_seed, shape=latents_shape, dtype=jnp.float32) latent_1 = diffuse(text_embed_1, noise_1) prompt_2 = &quot;a realistic photo of a ford mustang&quot; text_embed_2 = embed(prompt_2) noise_2 = jax.random.normal(prng_seed, shape=latents_shape, dtype=jnp.float32) latent_2 = diffuse(text_embed_2, noise_2) . CPU times: user 1h 28min 53s, sys: 1min 24s, total: 1h 30min 17s Wall time: 47min 42s . The outputs for the previous prompts may not match our expectation and more tweaking maybe needed until we could reach good looking initial images. Thus before going further, let&#39;s visualize the embeddings and make sure they look good. . image1 = latents_to_pil(latent_1)[0] image2 = latents_to_pil(latent_2)[0] . f = plt.figure(figsize=(20,10)) images, prompts = [image1, image2], [prompt_1, prompt_2] for i, image in enumerate(images): sp = f.add_subplot(1, 2, i + 1) sp.axis(&#39;off&#39;) sp.set_title(prompts[i], fontsize=16) plt.imshow(image) . To generate the $t^{th}$ embeddings vector we use Spherical Linear Interpolation (SLERP) which is defined by the following formula: . $$ operatorname{Slerp}(p_0,p_1; t) = frac{ sin {[(1-t) theta}]}{ sin theta} p_0 + frac{ sin [t theta]}{ sin theta} p_1.$$ . Unlinke Linear Interpolation (LERP), SLERP allows us to generate embeddings which will be spaced uniformly. . . The following function implements SLERP in JAX: . def slerp(t, v0, v1, DOT_THRESHOLD=0.9995): &quot;&quot;&quot; helper function to spherically interpolate two arrays v1 v2 &quot;&quot;&quot; dot = jnp.sum(v0 * v1 / (jnp.linalg.norm(v0) * jnp.linalg.norm(v1))) if jnp.abs(dot) &gt; DOT_THRESHOLD: v2 = (1 - t) * v0 + t * v1 else: theta_0 = jnp.arccos(dot) sin_theta_0 = jnp.sin(theta_0) theta_t = theta_0 * t sin_theta_t = jnp.sin(theta_t) s0 = jnp.sin(theta_0 - theta_t) / sin_theta_0 s1 = sin_theta_t / sin_theta_0 v2 = s0 * v0 + s1 * v1 return v2 . Because generating many images with Stable Diffusion takes time, let&#39;s save our progress so far so we could skip all the previous steps in case we needed to restart this notebook. . !mkdir -p &#39;{PATH}/cars&#39; . Save the embeddings . jnp.save(f&#39;{PATH}/cars/text_embed_1.npy&#39;, text_embed_1) jnp.save(f&#39;{PATH}/cars/text_embed_2.npy&#39;, text_embed_2) jnp.save(f&#39;{PATH}/cars/noise_1.npy&#39;, noise_1) jnp.save(f&#39;{PATH}/cars/noise_2.npy&#39;, noise_2) jnp.save(f&#39;{PATH}/cars/latent_1.npy&#39;, latent_1) jnp.save(f&#39;{PATH}/cars/latent_2.npy&#39;, latent_2) . Load the embeddings . text_embed_1 = jnp.load(f&#39;{PATH}/cars/text_embed_1.npy&#39;) text_embed_2 = jnp.load(f&#39;{PATH}/cars/text_embed_2.npy&#39;) noise_1 = jnp.load(f&#39;{PATH}/cars/noise_1.npy&#39;) noise_2 = jnp.load(f&#39;{PATH}/cars/noise_2.npy&#39;) . The number of interpolation steps is an important parameter because the value depends on how far looking are the source and target images. You many need lot more steps or less, let&#39;s pick a value. . num_steps = 50 # @param interpolation step . Lets generate as many as num_steps values for the SLERP parameter $t$. . steps = jnp.linspace(0.0, 1.0, num_steps).tolist() . For each $t$ we interpolate the embeddings based on SLERP then use diffusion loop to generate the corresponding image. . for i, t in enumerate(tqdm(steps)): outpath = f&#39;{PATH}/cars/frame{i:06}.jpeg&#39; if os.path.exists(outpath): continue cond_embedding = slerp(t, text_embed_1, text_embed_2) initial_noise = slerp(t, noise_1, noise_2) latents = diffuse(cond_embedding, initial_noise) image = latents_to_pil(latents)[0] image.save(outpath) . After generate all the intermediate images, we can group them into a video or GIF as follows. . !ffmpeg -r 10 -f image2 -s 512x512 -i &#39;{PATH}&#39;/cars/frame%06d.jpeg -vcodec libx264 -crf 10 -pix_fmt yuv420p evolution_cars.mp4 . !ffmpeg -f image2 -framerate 1 -i &#39;{PATH}&#39;/cars/frame%06d.jpeg -loop -1 evolution_cars.gif . !ls -alt evolution_cars.* . -rw-r--r-- 1 root root 3871184 Dec 14 10:19 evolution_cars.gif -rw-r--r-- 1 root root 2466461 Dec 14 10:19 evolution_cars.mp4 . The resulting interpolation is illustrated in the video in at the top of this article. . That&#39;s all folks . Stable Diffusion is a powerful model with many applications. In this article we saw how to combine it with interpolation techniques to generated a compelling sequence of images. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc. .",
            "url": "https://dzlab.github.io/notebooks/flax/vision/diffusion/2022/12/14/Stable_Diffusion_interpolation_Flax.html",
            "relUrl": "/flax/vision/diffusion/2022/12/14/Stable_Diffusion_interpolation_Flax.html",
            "date": " • Dec 14, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Get started with Stable Diffusion using Flax and TPU",
            "content": "Diffusion models are some of the most recent disrubtive models. Outperforming generative models, they have been made popular as result of the success of DALL-E 2 or Imagen to generate photorealistic images when prompted on text. . Thanks to Hugging Face, Diffusion models are available to anyone to use via the diffusers library. In this post, we will explore the diffusers Flax API to generate images from a prompt. . . Setup and Imports . First, we need to install some libraries include diffusers and Flax. . %%capture %%bash pip install --upgrade diffusers transformers scipy pip install --upgrade flax . The diffusers librar contains the Flax implementation of the model, but we need to also use some weights to initialize it. We will use checkpoints from CompVis/stable-diffusion-v1-4, but first we need to accept the terms of the licence for this model. . One running the following cell, it will asks for an access token which you can get from the settings page of your Hugging Face account - link. . !huggingface-cli login . _| _| _| _| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|_|_|_| _|_| _|_|_| _|_|_|_| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _|_|_|_| _| _| _| _|_| _| _|_| _| _| _| _| _| _|_| _|_|_| _|_|_|_| _| _|_|_| _| _| _| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _| _| _|_| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _| _| _| _|_|_| _|_|_|_| To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens . Token: Add token as git credential? (Y/n) n Token is valid. Your token has been saved to /root/.huggingface/token Login successful . We need to setup JAX to use TPU to, see link. . import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() . We need to make sure that JAX is using TPU as backend before proceeding further . Note: The number of TPU cores is displayed, this should be 8 if you are running on a v2 or v3 TPU or 4 if you are running on a v4 TPU. . assert jax.device_count() == 8 . Import the libraries we will use, e.g. matplotlib for plotting the resulting images. . import jax import jax.numpy as jnp import numpy as np from flax.jax_utils import replicate from flax.training.common_utils import shard import matplotlib.pyplot as plt import matplotlib.image as mpimg %matplotlib inline . Loading the models . Now, we can instanciate a Flax Diffusion pipeline and load the propoer weights. . Note: we are using half-precision weights (i.e. float16 wheras the original model weights are in float32) to save on memory and avoid issues like OOM. . from diffusers import FlaxStableDiffusionPipeline pipeline, pipeline_params = FlaxStableDiffusionPipeline.from_pretrained( &quot;CompVis/stable-diffusion-v1-4&quot;, revision=&quot;bf16&quot;, dtype=jax.numpy.bfloat16 ) . If you want to use the orignal precision (i.e. float32) then you need to change how the pipeline is loaded to this: . pipeline, pipeline_params = FlaxStableDiffusionPipeline.from_pretrained( &quot;CompVis/stable-diffusion-v1-4&quot;, revision=&quot;flax&quot;, dtype=jax.numpy.bfloat32 ) . Helper functions . The input promot is text, we need to be able to convert it to numbers as expected by this Diffusion model. Hence, the following function wraps pipeline.prepare_inputs to properly convert the text to Token IDs. . def prepare_inputs(prompt, num_samples): prompt = num_samples * [prompt] prompt_ids = pipeline.prepare_inputs(prompt) prompt_ids = shard(prompt_ids) return prompt_ids . The next helper function will run the pipeline with the input prompt text. This will generate a JAX represnetation of the image corresponding to that prompt. We will at the end covert this JAX array into an actual PIL image. . def generate1(prompt, num_samples, seed=0, num_inference_steps=50, guidance_scale=7.5): prng_seed = jax.random.PRNGKey(seed) prompt_ids = prepare_inputs(prompt, num_samples) # shard inputs and rng params = replicate(pipeline_params) prng_seed = jax.random.split(prng_seed, num_samples) output = pipeline(prompt_ids, params, prng_seed, num_inference_steps, guidance_scale=guidance_scale, jit=True) images = output.images images = np.asarray(images.reshape((num_samples,) + images.shape[-3:])) images = pipeline.numpy_to_pil(images) return images . Our final helper function is for plotting the model output images into a grid. . def ceildiv(a, b): return -(-a // b) def plots_pil_images(pil_images, figsize=(10,5), rows=1, cols=None, titles=None, maintitle=None): f = plt.figure(figsize=figsize) if maintitle is not None: plt.suptitle(maintitle, fontsize=10) cols = cols if cols else ceildiv(len(pil_images), rows) for i in range(len(pil_images)): sp = f.add_subplot(rows, cols, i+1) sp.axis(&#39;Off&#39;) if titles is not None: sp.set_title(titles[i], fontsize=16) img = np.asarray(pil_images[i]) plt.imshow(img) . Generating images . The main input to the Stable Diffusion pipeline is the text describing what we want the model to render, aka prompt. . prompt = &quot;A road across trees with snow and moutains in the horizon in fresco style&quot; . You can try your own prompts, for instance: . A city, morning sunrise, clouds, beautiful, summer, calm Paris by night, studio ghibli, art by hayao miyazaki Hyperrealist photo of a ford mustang . . . . You can also try other painting style like: Expressionist, Oil, Surrealism. See more here - link . The only limit is our imagination. Note that you may need many iteration on your promot to endup with an image close to what you actually want. . num_samples = jax.device_count() . Let&#39;s pass our prompt to the pipeline and examine the different images that the model generated. . %%time images = generate1(prompt, num_samples, seed=0) plots_pil_images(images, figsize=(16, 8), rows=2, cols=num_samples/2) . CPU times: user 8.25 s, sys: 15.2 s, total: 23.5 s Wall time: 34.7 s . . Note: because we are using a TPU which have 8 cores, we needed to duplicate the prompt 8 times which explains why we get back 8 images. . We can changing the seed which will result in the model returning completely different images for the same prompt . %%time images = generate1(prompt, num_samples, seed=13) plots_pil_images(images, figsize=(16, 8), rows=2, cols=num_samples/2) . CPU times: user 8.64 s, sys: 12.9 s, total: 21.5 s Wall time: 34 s . Guidance scale . Another parameter to control the look and quality of the resulting image is guidance_scale. To better understand what this value does we need to understand how the model generate an image. . Stable Diffusion is a multi-step model. At each step, the model predicts some noise using the input prompt and combines this with noise generating from blank input (i.e. empty string) as follows: . noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) . In this case, noise_pred_uncond is the noise predicted from a blank input, and noise_pred_text is the noise predicted from the input prompt. . The parameter guidance_scale controls how mush difference (or we can think of it as distance) of noise_pred_uncond to noise_pred_text the model we will incorporate in our final image. The value of guidance_scale can be anything but usual 7.5 seem to provide good results. . Let&#39;s try 1.1 as a value for our guidance and examine the resulting images. . %%time images = generate1(prompt, num_samples, seed=0, guidance_scale=1.1) plots_pil_images(images, figsize=(16, 8), rows=2, cols=num_samples/2) . CPU times: user 9.31 s, sys: 11.6 s, total: 20.9 s Wall time: 33.4 s . You may notice that the resulting images are not as close to our prompt then the ones generated earlier using the default value of 7.5. . Negative prompt . The Stable Diffusion pipeline accepts a paramter called neg_prompt_ids. This is basically the Token IDs of a Negative prompt. . In simple terms, a negative prompt instructs the Stable Diffusion model to not include certain things in the generated image. This allow us to remove any object, styles, or abnormalities from the original generated image. . def generate2(prompt, neg_prompt, num_samples, seed=0, num_inference_steps=50, guidance_scale=7.5): prng_seed = jax.random.PRNGKey(seed) prompt_ids = prepare_inputs(prompt, num_samples) neg_prompt_ids = prepare_inputs(neg_prompt, num_samples) # shard inputs and rng params = replicate(pipeline_params) prng_seed = jax.random.split(prng_seed, num_samples) output = pipeline(prompt_ids, params, prng_seed, num_inference_steps, guidance_scale=guidance_scale, jit=True, neg_prompt_ids=neg_prompt_ids) images = output.images images = np.asarray(images.reshape((num_samples,) + images.shape[-3:])) images = pipeline.numpy_to_pil(images) return images . Let&#39;s tell Stable Diffusion to not include the sun . %%time images = generate2(prompt, &#39;shiny sun&#39;, num_samples, seed=0) plots_pil_images(images, figsize=(16, 8), rows=2, cols=num_samples/2) . CPU times: user 8.69 s, sys: 13.3 s, total: 22 s Wall time: 33.5 s . Let&#39;s make sure Stable Diffusion does not generate images of sunnet . %%time images = generate2(prompt, &#39;sunset&#39;, num_samples, seed=0) plots_pil_images(images, figsize=(16, 8), rows=2, cols=num_samples/2) . CPU times: user 10.1 s, sys: 12.8 s, total: 22.9 s Wall time: 33.2 s . Let&#39;s try not having snow storms, and notice how the model removed snow from the resulting images. . %%time images = generate2(prompt, &#39;snow storm&#39;, num_samples, seed=0) plots_pil_images(images, figsize=(16, 8), rows=2, cols=num_samples/2) . CPU times: user 8.74 s, sys: 12.8 s, total: 21.5 s Wall time: 33.2 s . That&#39;s all folks . Stable Diffusion is a very neat model, and Hugging Face&#39;s diffusers library makes it very easy to play with such models. Furthermore, with the Flax implementation and the use of TPUs we can generate images and play with the model in an almost interactive way. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc. .",
            "url": "https://dzlab.github.io/notebooks/flax/vision/diffusion/2022/12/09/Stable_Diffusion_in_Flax_TPU.html",
            "relUrl": "/flax/vision/diffusion/2022/12/09/Stable_Diffusion_in_Flax_TPU.html",
            "date": " • Dec 9, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Visual Attention Network Explained with a TensorFlow implementation",
            "content": "In this post we will examine Visual Attention Network for image classification and walk through its implementation in TensorFlow. . The paper introducing VAN proposed the following contributions: . A novel Large Kernel Attention (LKA) module which is a self-attention mechanism that takes advantage of the 2D structure of images by capturing channel adaptability in addition to spatial adaptability. | A novel neural network based on LKA, called Visual Attention Network (VAN) that outperforms vision transformers and convolutional neural networks in many computer vision tasks. | . The following charts from the paper highlights the results of different models on ImageNet-1K validation set. We can see that VAN performs better while keeping the computation cost comparable to other models. . . Implementation . Let&#39;s look at the different components of the VAN architecture and understand how to implement it in TensorFlow. We will be referencing the code from the original PyTorch implementation in this repository - VAN-Classification. . You can find another TensorFlow implementation in the following repository tfvan. . import math import numpy as np import tensorflow as tf from tensorflow.keras import initializers, layers, Model . Multilayer Perceptrons . VAN relies on a variation of Multilayer Perceptrons (MLPs) layer that decouple standard MLP into spatial MLP and channel MLP to reduce computational cost of the standard MLP. Furthermore, it uses an attention mechanism similarly to gMLP but without sensitivity to input size or the constraint of processing fixed-size images. . Below is the implementation of the MLP layer as proposed in the VAN architecture . class MLP(layers.Layer): def __init__(self, in_features, hidden_features=None, out_features=None, activation=&quot;gelu&quot;, drop=0.): super().__init__() out_features = out_features or in_features hidden_features = hidden_features or in_features self.fc1 = layers.Conv2D(hidden_features, 1) self.dwconv = layers.DepthwiseConv2D( kernel_size=3, strides=1, padding=&#39;same&#39;, use_bias=True, activation=activation ) self.fc2 = layers.Conv2D(out_features, 1) self.drop = layers.Dropout(drop) def call(self, x): x = self.fc1(x) x = self.dwconv(x) x = self.drop(x) x = self.fc2(x) x = self.drop(x) return x . mlp = MLP(768) y = mlp(tf.zeros((1, 224, 224, 3))) y.shape . TensorShape([1, 224, 224, 768]) . Large Kernel Attention . Attention mechanisms are used to capture relationship between input features and to produce an attention map indicating the importance of different them. To learn this relationship, we could use: . Self-attention mechanism to capture long-range dependence. Self-attention was successful in NLP tasks, in computer vision it has following drawbacks It treats images as 1D sequences which neglects the 2D structure of images. | The quadratic complexity is too expensive for high-resolution images. | It only achieves spatial adaptability but ignores the adaptability in channel dimension | . | Large kernel convolution to build relevance and produce attention map. But this approach has its own limitations, Large-kernel convolution comes with a huge amount of computational overhead and additional parameters to learn. | . . The authors proposed a new attention mechanism that combines the pros of the previous approaches while overcome their drawbacks. This is achieved by decomposing a large kernel convolution as depicted in the above picture. . For instance, given a $K × K$ convolution and dilation rate $d$, we decompose into: . a spatial local convolution (DW-Conv or depth-wise convolution) of $⌈ frac{K}{d}⌉×⌈ frac{K}{d}⌉$, | a spatial long-range convolution ((DW-D-Conv or depth-wise dilation convolution) of $(2d − 1) × (2d − 1)$, and | a channel convolution ($1×1$ convolution). | . This decomposition can be writting in the following formula: . $$ Attention = Conv_{1 x 1} ( text{DW-D-Conv} ( text{DW-Conv} (F)) ) $$ $$Output = Attention otimes F $$ . With $F ∈ R^{C×H×W}, Attention ∈ R^{C×H×W} text{ and } otimes text{ stands for element-wise product}$ . The architecture of a Large Kernel Attention layer would look like this: . . In TensorFlow, the Large Kernel Attention layer can be implemented as follows: . class LKA(layers.Layer): def __init__(self, dim): super().__init__() self.conv0 = layers.Conv2D(dim, kernel_size=5, padding=&quot;same&quot;, groups=dim) self.conv_spatial = layers.Conv2D(dim, kernel_size=7, strides=1, padding=&quot;same&quot;, groups=dim, dilation_rate=3) self.conv1 = layers.Conv2D(dim, kernel_size=1) def call(self, x): attn = self.conv0(x) attn = self.conv_spatial(attn) attn = self.conv1(attn) return x * attn . attn = LKA(4) y = attn(tf.zeros((1, 224, 224, 4))) y.shape . TensorShape([1, 224, 224, 4]) . class SpatialAttention(layers.Layer): def __init__(self, d_model, activation=&quot;gelu&quot;): super().__init__() self.proj_1 = layers.Conv2D(d_model, kernel_size=1, activation=activation) self.spatial_gating_unit = LKA(d_model) self.proj_2 = layers.Conv2D(d_model, kernel_size=1) def call(self, x): attn = self.proj_1(x) attn = self.spatial_gating_unit(attn) attn = self.proj_2(attn) attn = x + attn return attn . attn = SpatialAttention(4) y = attn(tf.zeros((1, 224, 224, 4))) y.shape . TensorShape([1, 224, 224, 4]) . DropPath layer . DropPath is used in the VAN model as an alternative to the Dropout layer, it was originally proposed in the FractalNet paper. Below is the implementation of DropPath in TensorFlow. Instead of using custom layer, we could alternatively we could simply use this StochasticDepth layer. . class DropPath(layers.Layer): def __init__(self, rate, **kwargs): super().__init__(**kwargs) self.rate = rate def call(self, inputs, training=None, **kwargs): if 0. == self.rate: return inputs if training is None: training = tf.keras.backend.learning_phase() training = tf.constant(training, dtype=tf.bool) outputs = tf.cond(training, lambda: self.drop(inputs), lambda: tf.identity(inputs)) return outputs def drop(self, inputs): keep = 1.0 - self.rate batch = tf.shape(inputs)[0] shape = [batch] + [1] * (inputs.shape.rank - 1) random = tf.random.uniform(shape, dtype=self.compute_dtype) &lt;= keep random = tf.cast(random, self.compute_dtype) / keep outputs = inputs * random return outputs . attn = DropPath(0.1) y = attn(tf.zeros((1, 224, 224, 3))) y.shape . TensorShape([1, 224, 224, 3]) . VAN stage . VAN uses multiple stages that downsample the input tensor and pass it through combines two Transofmer Encoder blocks but uses a window based self-attentions as illustrated in the following diagram. In this section, we will examine each component of this bloc and implement it in TensorFlow. . . Downsampling layer . The patch embedding layer is used to downsample a tensor using convolutional layers . class OverlapPatchEmbed(layers.Layer): def __init__(self, img_size=224, patch_size=7, patch_stride=4, in_chans=3, embed_dim=768, dilation_rate=1, **kwargs): super().__init__(**kwargs) img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1] self.num_patches = self.H * self.W # same pad layer dilation_rate = (dilation_rate, dilation_rate) total_pad = (patch_size[0] - 1) * dilation_rate[0], (patch_size[1] - 1) * dilation_rate[1] top_pad = total_pad[0] // 2 bottom_pad = total_pad[0] - top_pad left_pad = total_pad[1] // 2 right_pad = total_pad[1] - top_pad # noinspection PyAttributeOutsideInit self.pad = layers.ZeroPadding2D(((top_pad, bottom_pad), (left_pad, right_pad))) # embedding self.proj = layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_stride) self.norm = layers.BatchNormalization() def call(self, x): x = self.pad(x) x = self.proj(x) x = self.norm(x) return x . embed = OverlapPatchEmbed() y = embed(tf.zeros((1, 224, 224, 3))) y.shape . TensorShape([1, 56, 56, 768]) . Block layer . The Block layer is repeated multiple times in a stage. It is composed of the following building blocks: . . In TensorFlow, we can implement this layer as follows: . class Block(layers.Layer): def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., activation=&quot;gelu&quot;, **kwargs): super().__init__(**kwargs) self.norm1 = layers.BatchNormalization() self.attn = SpatialAttention(dim) self.drop_path = DropPath(drop_path) self.norm2 = layers.BatchNormalization() mlp_hidden_dim = int(dim * mlp_ratio) self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, activation=activation, drop=drop) layer_scale_init_value = 1e-2 self.layer_scale_1 = tf.Variable( initial_value=layer_scale_init_value * tf.ones((1, 1, 1, dim)), trainable=True) self.layer_scale_2 = tf.Variable( layer_scale_init_value * tf.ones((1, 1, 1, dim)), trainable=True) def call(self, x): x = x + self.drop_path(self.layer_scale_1 * self.attn(self.norm1(x))) x = x + self.drop_path(self.layer_scale_2 * self.mlp(self.norm2(x))) return x . block = Block(3) y = block(tf.zeros((1, 224, 224, 3))) y.shape . TensorShape([1, 224, 224, 3]) . Stage layer . The following layer implements a VAN&#39;s stage by using the OverlapPatchEmbed layer and a sequence of Block layers . class Stage(layers.Layer): def __init__(self, i, embed_dims, mlp_ratios, depths, path_drops, drop_rate=0., **kwargs): super().__init__(**kwargs) # downsample self.patch_embed = OverlapPatchEmbed( patch_size = 7 if 0 == i else 3, patch_stride = 4 if 0 == i else 2, embed_dim = embed_dims[i], name = f&#39;patch_embed{i + 1}&#39; ) # blocks self.blocks = [Block( dim=embed_dims[i], mlp_ratio=mlp_ratios[i], drop=drop_rate, drop_path=path_drops[sum(depths[:i]) + j], name=f&#39;block{i + 1}.{j}&#39; ) for j in range(depths[i])] # normalization self.norm = layers.LayerNormalization(name=f&#39;norm{i + 1}&#39;) def call(self, x): x = self.patch_embed(x) for block in self.blocks: x = block(x) x = self.norm(x) return x . Putting it together . After defining all the major components of the VAN model, we can put them together to build the model. This is fairly straightforward now as we just need to pass the input image through a sequence of Stage layers. Then, add a head to the model that consists of a pooling followed by a dense layer that outputs the result (e.g. class ID if the task is classification). . def create_VAN(embed_dims, mlp_ratios, depths, drop_rate=0., path_drop=0.1, input_shape=(224, 224, 3), pooling=None, classes=2): # stochastic depth decay rule path_drops = np.linspace(0., path_drop, sum(depths)) # input image inputs = layers.Input(shape=input_shape, name=&#39;image&#39;) x = inputs # create stages for i in range(len(depths)): stage = Stage(i, embed_dims, mlp_ratios, depths, path_drops, drop_rate, name = f&#39;stage_{i}&#39;) x = stage(x) # pooling layer if pooling in {None, &#39;avg&#39;}: x = layers.GlobalAveragePooling2D(name=&#39;avg_pool&#39;)(x) elif pooling == &#39;max&#39;: x = layers.GlobalMaxPooling2D(name=&#39;max_pool&#39;)(x) else: raise ValueError(f&#39;Expecting pooling to be one of None/avg/max. Found: {pooling}&#39;) # head classifier x = layers.Dense(classes, name=&#39;head&#39;)(x) outputs = layers.Activation(&#39;softmax&#39;, dtype=&#39;float32&#39;, name=&#39;pred&#39;)(x) # Create model. model = Model(inputs, outputs, name=&#39;van&#39;) return model . The smallest model in terms of parameters is TinyVAN, which we create as follows: . model = create_VAN(embed_dims=(32, 64, 160, 256), mlp_ratios=(8, 8, 4, 4), depths=(3, 3, 5, 2)) . tf.keras.utils.plot_model(model, rankdir=&#39;LR&#39;, show_shapes=True) . model.summary() . Model: &#34;van&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= image (InputLayer) [(None, 224, 224, 3)] 0 stage_0 (Stage) (None, 56, 56, 32) 80384 stage_1 (Stage) (None, 28, 28, 64) 286528 stage_2 (Stage) (None, 14, 14, 160) 1608480 stage_3 (Stage) (None, 7, 7, 256) 1880832 avg_pool (GlobalAveragePool (None, 256) 0 ing2D) head (Dense) (None, 2) 514 pred (Activation) (None, 2) 0 ================================================================= Total params: 3,856,738 Trainable params: 3,849,314 Non-trainable params: 7,424 _________________________________________________________________ . Training . The authors trained the VAN model for various vision tasks (e.g. classification or object detection). They trained the model during 310 epochs using AdamW optimizer with momentum=0.9, weight decay=5 × 10−2 and batch size = 1,024. . For the learning rate (LR), Cosine scheduling and warm-up strategy were used. The initial LR is set to 5 × 10−4 . They used the following data augmentation techniques: . random clipping | random horizontal flipping | label-smoothing | mixup | cutmix | random erasing. | . That&#39;s all folks . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/classification/2022/05/31/VAN_Classification.html",
            "relUrl": "/tensorflow/vision/classification/2022/05/31/VAN_Classification.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Swin Transformer Explained with a TensorFlow implementation",
            "content": "After the introduction of the ViT architecture for image classification, many similar works attempted to use a Transormer based architecture for building a classification mode. One of the top performer is proposed in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. This paper come with the following import contributions: . Building a hierarchical representation of the input image by taking patches of small sizes and gradually increasing their size. | Instead of applying Transformer&#39;s attention to the entire input as usually done in NLP tasks, attention is calculated locally from non-overlapping shifted windows. | Comprehensive experiments on various vision tasks and model architectures and strong results, e.g. 58.7 box AP and 51.1 mask AP on test-dev on COCO object detection. | . . Key differences between Swin Transformer and original Vision Transformer (ViT) model is that ViT produced feature maps of a single low resolution and because it uses a global self-attention ViT has a quadratic computation complexity to input image size. On the other hand, Swin Transformer builds hierarchical feature maps by merging image patches and it has a linear computation complexity to input image size because it uses local windows for calculating selt-attention. . In this article, we will discuss the different concepts of the Swin Transformers (the name Swin stands for Shifted window) model and implement it in TensorFlow. . Overview . As described in the paper arxiv.org and depicted in the following diagram, SWin Transformer works as follows: . Each image is split into fixed-size patches of size 4 x 4 then passed to a sequence of stages | The first stage, Calculate a Patch Embeddings for each patch and also the positional embeddings of the patch then add everything together | The second, third and forth stages starts by merging patches and then passing them through a Swin Transformer block to calculate feature maps | At eash of those later stages, the number of channels of the patches is multiplied by 2 and the width and height of the images is divided by 2. | . . Implementation . Having built a high level idea of the Swin Transformer Architecture, let&#39;s now look at the code-implementation and understand how to implement this architecture in TensorFlow. We will be referencing the code from the Swin-Transformer-Tensorflow repository to explain the implementation. For the PyTorch implementation you can refer to the original implementation of Swin-Transformer. . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.layers import ( Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention, Softmax ) from tensorflow.keras.initializers import TruncatedNormal . Patch Partition Layer . The first component of the Swin-T architecture is a Path Parition layer which is used to partition an input image into multiple small patches. . In TensorFlow, we can simply use the tf.image.extract_patches function to extract patches. We can use it inside a custom Layer to make it easy to use later when building the model . class PatchPartition(Layer): def __init__(self, window_size=4, channles=3): super(PatchPartition, self).__init__() self.window_size = window_size def call(self, images): batch_size = tf.shape(images)[0] patches = tf.image.extract_patches( images=images, sizes=[1, self.window_size, self.window_size, 1], strides=[1, self.window_size, self.window_size, 1], rates=[1, 1, 1, 1], padding=&quot;VALID&quot;, ) patch_dims = patches.shape[-1] patches = tf.reshape(patches, [batch_size, -1, patch_dims]) return patches . Note how similar is this patch extraction layer to the one used in ViT . We could also use Conv2d layer to do the same as a 2-D Convolution uses kernel size (which in our case will be the patch size) and the stride also will be equal to the patch size as we want the windows to not be overlapping. . Alternatively we can implemnt this using tf.reshape like this for instance: . batch, height, width, channels = images.shape patch_num_x = width // window_size patch_num_y = height // window_size new_shape = (-1, patch_num_y, window_size, patch_num_x, window_size, channels) tf.reshape(images, shape=new_shape) . Let&#39;s download a test image and make sure it matches the shape of images in the ImageNet dataset. . !curl -s -o flower.jpeg https://images.unsplash.com/photo-1604085572504-a392ddf0d86a?ixlib=rb-1.2.1&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=224&amp;q=224 . image = plt.imread(&#39;flower.jpeg&#39;) image = tf.image.resize(tf.convert_to_tensor(image), size=(224, 224)) plt.imshow(image.numpy().astype(&quot;uint8&quot;)) plt.axis(&quot;off&quot;); . We can apply the images through our PatchParition layer . batch = tf.expand_dims(image, axis=0) patches = PatchPartition()(batch) patches.shape . TensorShape([1, 3136, 48]) . Now we can examine the partitions that we just created . n = int(np.sqrt(patches.shape[1])) for i, patch in enumerate(patches[0]): ax = plt.subplot(n, n, i + 1) patch_img = tf.reshape(patch, (4, 4, 3)) ax.imshow(patch_img.numpy().astype(&quot;uint8&quot;)) ax.axis(&quot;off&quot;) . Linear Embedding layer . The second component in the Swin-T architecture is the Linear Embedding layer which is simply a combination of projection and embedding layers. It is used to calculate the patch embedding and the position embedding then add both as illustrated with the following diagram . . class LinearEmbedding(Layer): def __init__(self, num_patches, projection_dim, **kwargs): super(LinearEmbedding, self).__init__(**kwargs) self.num_patches = num_patches self.projection = Dense(projection_dim) self.position_embedding = Embedding(input_dim=num_patches, output_dim=projection_dim) def call(self, patch): # calculate patches embeddings patches_embed = self.projection(patch) # calcualte positional embeddings positions = tf.range(start=0, limit=self.num_patches, delta=1) positions_embed = self.position_embedding(positions) # add both embeddings encoded = patches_embed + positions_embed return encoded . We can confirm that the output of this layer is as expected 1, 3136, 96 number of patches is num_patch_x * num_patch_y = 224 / window_size . embeddings = LinearEmbedding(3136, 96)(patches) embeddings.shape . TensorShape([1, 3136, 96]) . Patch Merging layer . Patch merging as it may indicate is used to merge smaller patches into larger ones. Let&#39;s look at the illustration to understand what this layer is doing, if the input of the layer is an 8 x 8 pixel image with 4 patches of 4 x 4 then the output becomes an 8 x 8 pixels but with one large patch of 8 x 8 plus an additional channel layer as channel layers are doubled. . . This layer can be simply implemented as a linear layer so that we can easily define the size of the output and double the channels as follows: . class PatchMerging(Layer): def __init__(self, input_resolution, channels): super(PatchMerging, self).__init__() self.input_resolution = input_resolution self.channels = channels self.linear_trans = Dense(2 * channels, use_bias=False) def call(self, x): height, width = self.input_resolution _, _, C = x.get_shape().as_list() x = tf.reshape(x, shape=(-1, height, width, C)) x0 = x[:, 0::2, 0::2, :] x1 = x[:, 1::2, 0::2, :] x2 = x[:, 0::2, 1::2, :] x3 = x[:, 1::2, 1::2, :] x = tf.concat((x0, x1, x2, x3), axis=-1) x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C)) return self.linear_trans(x) . channels = 96 num_patch_x = 224 // 4 num_patch_y = 224 // 4 out_patches = PatchMerging((num_patch_x, num_patch_y), channels)(patches) print(f&#39;Input shape (B, H * W, C) = {patches.shape}&#39;) print(f&#39;Ouput shape (B, H/2*W/2, 4C) = {out_patches.shape}&#39;) . Input shape (B, H * W, C) = (1, 3136, 48) Ouput shape (B, H/2*W/2, 4C) = (1, 784, 192) . Swin Transfomer block . The Swin Transformer block combines two Transofmer Encoder blocks but uses a window based self-attentions as illustrated in the following diagram. In this section, we will examine each component of this bloc and implement it in TensorFlow . Note: W-MSA stands for Window Multi-head Self-Attention and SW-MSA stands for Shifted Window Multi-head Self-Attention. . . Multilayer Perceptron . A Multilayer Perceptron (MLP) consists basically two dense layers and a GELU activation layer. It is used in the classical Transformer architecture and it is also used in the Swin Transformer blocks. We can simply implement it as custom layer as follows: . class MLP(Layer): def __init__(self, hidden_features, out_features, dropout_rate=0.1): super(MLP, self).__init__() self.dense1 = Dense(hidden_features, activation=tf.nn.gelu) self.dense2 = Dense(out_features) self.dropout = Dropout(dropout_rate) def call(self, x): x = self.dense1(x) x = self.dropout(x) x = self.dense2(x) y = self.dropout(x) return y . mlp = MLP(768 * 2, 768) y = mlp(tf.zeros((1, 197, 768))) y.shape . TensorShape([1, 197, 768]) . Window multi-head self-attention . The following implementaiton support boths of shifted and non-shifted window attention. . class WindowAttention(Layer): def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.): super().__init__() self.dim = dim self.window_size = window_size # Wh, Ww self.num_heads = num_heads head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 # define a parameter table of relative position bias initializer = TruncatedNormal(mean=0., stddev=.02) # position table shape is: (2*Wh-1 * 2*Ww-1, nH) table_shape = ((2*self.window_size[0]-1) * (2*self.window_size[1]-1), num_heads) self.relative_position_bias_table = tf.Variable(initializer(shape=table_shape)) # get pair-wise relative position index for each token inside the window coords_h = tf.range(self.window_size[0]) coords_w = tf.range(self.window_size[1]) coords = tf.stack(tf.meshgrid(coords_h, coords_w)) # 2, Wh, Ww coords_flatten = tf.reshape(coords, [2, -1]) # 2, Wh*Ww relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :] # 2, Wh*Ww, Wh*Ww relative_coords = tf.transpose(relative_coords, perm=[1,2,0]) # Wh*Ww, Wh*Ww, 2 relative_coords = relative_coords + [self.window_size[0] - 1, self.window_size[1] - 1] # shift to start from 0 relative_coords = relative_coords * [2*self.window_size[1] - 1, 1] self.relative_position_index = tf.math.reduce_sum(relative_coords,-1) # Wh*Ww, Wh*Ww self.qkv = Dense(dim * 3, use_bias=qkv_bias, kernel_initializer=initializer) self.attn_drop = Dropout(attn_drop) self.proj = Dense(dim, kernel_initializer=initializer) self.proj_drop = Dropout(proj_drop) self.softmax = Softmax(axis=-1) def call(self, x, mask=None): _, L, N, C = x.shape qkv = tf.transpose(tf.reshape(self.qkv(x), [-1, N, 3, self.num_heads, C // self.num_heads]), perm=[2, 0, 3, 1, 4]) # [3, B_, num_head, Ww*Wh, C//num_head] q, k, v = tf.unstack(qkv) # make torchscript happy (cannot use tensor as tuple) q = q * self.scale attn = tf.einsum(&#39;...ij,...kj-&gt;...ik&#39;, q, k) relative_position_bias = tf.reshape(tf.gather(self.relative_position_bias_table, tf.reshape(self.relative_position_index, [-1])), [self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1]) # Wh*Ww,Wh*Ww,nH relative_position_bias = tf.transpose(relative_position_bias, perm=[2, 0, 1]) # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias if mask is not None: nW = mask.shape[0] # every window has different mask [nW, N, N] attn = tf.reshape(attn, [-1 // nW, nW, self.num_heads, N, N]) + mask[:, None, :, :] # add mask: make each component -inf or just leave it attn = tf.reshape(attn, [-1, self.num_heads, N, N]) attn = self.softmax(attn) else: attn = self.softmax(attn) attn = self.attn_drop(attn) x = tf.reshape(tf.transpose(attn @ v, perm=[0, 2, 1, 3]), [-1, L, N, C]) x = self.proj(x) x = self.proj_drop(x) return x . attn = WindowAttention(96, window_size=(4, 4), num_heads=8, qkv_bias=True, qk_scale=None, attn_drop=0.0, proj_drop=0.0) y = attn(tf.zeros((1, 196, 16, 96))) y.shape . TensorShape([1, 196, 16, 96]) . Helper functions . Before defining the Swin Transformer block, we need couple helper functions to create create windows and merge them . First, window_partition which as the name suggest create windows from the input tensor . def window_partition(x, window_size): _, H, W, C = x.shape num_patch_y = H // window_size num_patch_x = W // window_size x = tf.reshape(x, [-1, num_patch_y, window_size, num_patch_x, window_size, C]) x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5]) windows = tf.reshape(x, [-1, num_patch_x * num_patch_y, window_size, window_size, C]) return windows . windows = window_partition(batch, 4) print(f&#39;Input shape (B, H, W, C) = {batch.shape}&#39;) print(f&#39;Ouput shape (num_windows*B, window_size, window_size, C) = {windows.shape}&#39;) . Input shape (B, H, W, C) = (1, 224, 224, 3) Ouput shape (num_windows*B, window_size, window_size, C) = (1, 3136, 4, 4, 3) . Second, window_reverse which as the name suggest reverse the created windows . def window_reverse(windows, window_size, H, W): C = windows.shape[-1] B = int(windows.shape[1] / (H * W / window_size / window_size)) x = tf.reshape(windows, [B, H // window_size, W // window_size, window_size, window_size, C]) x = tf.reshape(tf.transpose(x, perm=[0, 1, 3, 2, 4, 5]), [-1, H, W, C]) return x . y = window_reverse(windows, 4, 224, 224) print(f&#39;Input shape (B, num_windows*B, window_size, window_size, C) = {windows.shape}&#39;) print(f&#39;Ouput shape (B, H, W, C) = {y.shape}&#39;) . Input shape (B, num_windows*B, window_size, window_size, C) = (1, 3136, 4, 4, 3) Ouput shape (B, H, W, C) = (1, 224, 224, 3) . class DropPath(Layer): def __init__(self, prob): super().__init__() self.drop_prob = prob def call(self, x, training=None): if self.drop_prob == 0. or not training: return x keep_prob = 1 - self.drop_prob shape = (x.shape[0],) + (1,) * (x.ndim - 1) random_tensor = tf.random.uniform(shape=shape) random_tensor = tf.where(random_tensor &lt; keep_prob, 1, 0) output = x / keep_prob * random_tensor return output . drop = DropPath(0.2) y = drop(tf.zeros((1, 197, 768))) y.shape . TensorShape([1, 197, 768]) . SwinTransformerBlock . With the shifted window partitioning approach, consecutive Swin Transformer blocks are computed as . class SwinTransformerBlock(Layer): def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.): super().__init__() self.dim = dim self.input_resolution = input_resolution self.num_heads = num_heads self.window_size = window_size self.shift_size = shift_size self.mlp_ratio = mlp_ratio if min(self.input_resolution) &lt;= self.window_size: # if window size is larger than input resolution, we don&#39;t partition windows self.shift_size = 0 self.window_size = min(self.input_resolution) assert 0 &lt;= self.shift_size &lt; self.window_size, &quot;shift_size must in 0-window_size&quot; self.norm1 = LayerNormalization(epsilon=1e-5) self.attn = WindowAttention( dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop) self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else tf.identity self.norm2 = LayerNormalization(epsilon=1e-5) mlp_hidden_dim = int(dim * mlp_ratio) self.mlp = MLP(mlp_hidden_dim, dim, dropout_rate=drop) if self.shift_size &gt; 0: # calculate attention mask for SW-MSA H, W = self.input_resolution img_mask = np.zeros([1, H, W, 1]) # 1 H W 1 h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) cnt = 0 for h in h_slices: for w in w_slices: img_mask[:, h, w, :] = cnt cnt += 1 img_mask = tf.constant(img_mask) mask_windows = window_partition(img_mask, self.window_size) # nW, window_size, window_size, 1 mask_windows = tf.reshape(mask_windows, [-1, self.window_size * self.window_size]) attn_mask = mask_windows[:, None, :] - mask_windows[:, :, None] self.attn_mask = tf.where(attn_mask==0, -100., 0.) else: self.attn_mask = None def call(self, x): H, W = self.input_resolution B, L, C = x.shape assert L == H * W, &quot;input feature has wrong size&quot; shortcut = x x = self.norm1(x) x = tf.reshape(x, [-1, H, W, C]) # cyclic shift if self.shift_size &gt; 0: shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=(1, 2)) else: shifted_x = x # partition windows x_windows = window_partition(shifted_x, self.window_size) # nW*B, window_size, window_size, C x_windows = tf.reshape(x_windows, [-1, x_windows.shape[1], self.window_size * self.window_size, C]) # nW*B, window_size*window_size, C # W-MSA/SW-MSA attn_windows = self.attn(x_windows, mask=self.attn_mask) # nW*B, window_size*window_size, C # merge windows attn_windows = tf.reshape(attn_windows, [-1, x_windows.shape[1], self.window_size, self.window_size, C]) shifted_x = window_reverse(attn_windows, self.window_size, H, W) # B H&#39; W&#39; C # reverse cyclic shift if self.shift_size &gt; 0: x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=(1, 2)) else: x = shifted_x x = tf.reshape(x, [-1, H * W, C]) # FFN x = shortcut + self.drop_path(x) x = x + self.drop_path(self.mlp(self.norm2(x))) return x . block = SwinTransformerBlock(96, (56, 56), 8, window_size=4) y = block(embeddings) y.shape . TensorShape([1, 3136, 96]) . Putting it together . After defining all the major components of the Swin-T architecture, we can put them together to build the model. This is fairly straightforward now as we just need to plug a window partition to an blocks of Swin Transformers separated by a merging layer. For classification, we add a pooling then a dense layer to the form the head of the model. . . def create_SwinTransformer(num_classes, input_shape=(224, 224, 3), window_size=4, embed_dim=96, num_heads=8): num_patch_x = input_shape[0] // window_size num_patch_y = input_shape[1] // window_size inputs = Input(shape=input_shape) # Patch extractor patches = PatchPartition(window_size)(inputs) patches_embed = LinearEmbedding(num_patch_x * num_patch_y, embed_dim)(patches) # first Swin Transformer block out_stage_1 = SwinTransformerBlock( dim=embed_dim, input_resolution=(num_patch_x, num_patch_y), num_heads=num_heads, window_size=window_size, shift_size=0 )(patches_embed) # second Swin Transformer block out_stage_1 = SwinTransformerBlock( dim=embed_dim, input_resolution=(num_patch_x, num_patch_y), num_heads=num_heads, window_size=window_size, shift_size=1 )(out_stage_1) # patch merging representation = PatchMerging((num_patch_x, num_patch_y), channels=embed_dim)(out_stage_1) # pooling representation = GlobalAveragePooling1D()(representation) # logits output = Dense(num_classes, activation=&quot;softmax&quot;)(representation) # Create model model = Model(inputs=inputs, outputs=output) return model . . Note: In our case we are using only one stage of Swin Transformer blocks. In the original paper, there are 4 of those stages. . model = create_SwinTransformer(2) . model.summary() . Model: &#34;model_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_21 (InputLayer) [(None, 224, 224, 3)] 0 patch_partition_21 (PatchPa (None, None, 48) 0 rtition) linear_embedding_22 (Linear (None, 3136, 96) 305760 Embedding) swin_transformer_block_65 ( (None, 3136, 96) 112232 SwinTransformerBlock) swin_transformer_block_66 ( (None, 3136, 96) 112232 SwinTransformerBlock) patch_merging_15 (PatchMerg (None, 784, 192) 73728 ing) global_average_pooling1d_3 (None, 192) 0 (GlobalAveragePooling1D) dense_344 (Dense) (None, 2) 386 ================================================================= Total params: 604,338 Trainable params: 604,338 Non-trainable params: 0 _________________________________________________________________ . That&#39;s all folks . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/classification/2022/02/27/Swin_Transfomer.html",
            "relUrl": "/tensorflow/vision/classification/2022/02/27/Swin_Transfomer.html",
            "date": " • Feb 27, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Bedtime stories generated by AI - the speech part",
            "content": "This article is a deed dive on how episodes for the Bedtime stories podcast are generated. Specifically, how the speech is generated and how it is composed with background audion. To learn more about the podcast, check this overview article - Bedtime stories generated by AI. . &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; For the speech generation, I use the TensorFlowTTS library and the pre-trained models. Unfortunately, this library provides only one voice but hopefully in the future there will be more voices available. . Setup . First, we need to install the speech libraries . %%capture %%bash pip install pydub pip install git+https://github.com/TensorSpeech/TensorFlowTTS.git pip install git+https://github.com/repodiac/german_transliterate.git#egg=german_transliterate . . Note: You must restart the runtime in order to use newly installed versions. . import os import re import numpy as np from pydub import AudioSegment from pydub.playback import play import soundfile import subprocess import tempfile import IPython.display as ipd from tqdm import tqdm import tensorflow as tf from tensorflow_tts.inference import TFAutoModel from tensorflow_tts.inference import AutoConfig from tensorflow_tts.inference import AutoProcessor . [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /root/nltk_data... [nltk_data] Unzipping taggers/averaged_perceptron_tagger.zip. [nltk_data] Downloading package cmudict to /root/nltk_data... [nltk_data] Unzipping corpora/cmudict.zip. . Second, we need to download the speech models Tacotron 2 and Melgan which were trained on the LJ Speech Dataset. . tacotron2 = TFAutoModel.from_pretrained(&quot;tensorspeech/tts-tacotron2-ljspeech-en&quot;, name=&quot;tacotron2&quot;) . melgan = TFAutoModel.from_pretrained(&quot;tensorspeech/tts-melgan-ljspeech-en&quot;, name=&quot;melgan&quot;) . processor = AutoProcessor.from_pretrained(&quot;tensorspeech/tts-tacotron2-ljspeech-en&quot;) . Speech to text . We need to define couple helper functions. For instance, a helper function to perform actual speech synthesis for a given text . def text2speech(input_text, text2mel_model, vocoder_model): input_ids = processor.text_to_sequence(input_text) # text2mel part _, mel_outputs, stop_token_prediction, alignment_history = text2mel_model.inference( tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0), tf.convert_to_tensor([len(input_ids)], tf.int32), tf.convert_to_tensor([0], dtype=tf.int32) ) # vocoder part audio = vocoder_model(mel_outputs)[0, :, 0] return mel_outputs.numpy(), alignment_history.numpy(), audio.numpy() . Because I could not perform speech synthesis on large text, I needed a helper function that will chunk a large text into smaller chunks. . def split_into_chunks(text: str, max_length): &quot;&quot;&quot;Split a chunk of text into chunks of max_length and return a list of them.&quot;&quot;&quot; sentences = re.split(r&quot;(?&lt;= .) s+(?=[A-Z])&quot;, text.replace(&quot; n&quot;, &quot; &quot;)) chunks = [] current_chunk = [] chunk_length = 0 for sentence in sentences: sentence_length = len(sentence) if chunk_length + sentence_length + 1 &gt; max_length: # This chunk would overflow, make a new chunk. chunks.append(&quot; &quot;.join(current_chunk)) current_chunk = [] chunk_length = 0 current_chunk.append(sentence) chunk_length += sentence_length + 1 chunks.append(&quot; &quot;.join(current_chunk)) return chunks . %%bash rm -rf *.mp3 rm -rf *.wav . Next paste the story text in placeholder variable . story = &quot;&quot;&quot; replace with actual story &quot;&quot;&quot; . Then, create chunks from the story text and place those chunks around some introductory and prelude texts. . number = 1 begining = f&quot;&quot;&quot; Welcome to Episode {number} of the Bedtime short stories podcast, the AI generated podcast with short stories to help you sleep. I am Ex Machina, and I will be narrating your story tonight. &quot;&quot;&quot; end = f&quot;&quot;&quot; I hope you didn&#39;t make it so far and you are already asleep. If not then I hope you have enjoyed this short story. See next time. &quot;&quot;&quot;.strip() . chunks = split_into_chunks(story, 1000) chunks = [begining] + chunks + [end] . # setup window for tacotron2 if you want to try tacotron2.setup_window(win_front=10, win_back=10) . Now we can generate the speech for every chunk and save it in a separate WAV file . sr = 22050 chunk_names = [] for index, chunk in tqdm(enumerate(chunks), total=len(chunks)): mels, alignment_history, audios = text2speech(chunk, tacotron2, melgan) chunk_name = f&#39;voice_{number}_part_{index}.wav&#39; soundfile.write(chunk_name, audios, sr, &#39;PCM_24&#39;) chunk_names.append(chunk_name) . 100%|██████████| 10/10 [07:45&lt;00:00, 46.54s/it] . Silence . To make the episode speech less stressfull, adding short silence sections is a good idea. . First, we generate a WAV file with 3 seconds of silence . silence_segment = AudioSegment.silent(duration=3000) silence_segment.export(&#39;silence.wav&#39;, format=&quot;wav&quot;); . Second, we place the silence audio with the rest of the episode audio . first = 1 last = len(chunk_names) - 1 chunk_names = chunk_names[:first] + [&#39;silence.wav&#39;] + chunk_names[first: last] + [&#39;silence.wav&#39;] + chunk_names[last:] . Then we concatenate different audio chunks to generate the speech file of the episode . def concatenate_tracks(chunk_names, output): &quot;&quot;&quot;Concatenate mutliple audio tracks into one.&quot;&quot;&quot; audios = np.array([]) for chunk_name in tqdm(chunk_names): audio , _ = soundfile.read(chunk_name) audios = np.concatenate([audios, audio]) soundfile.write(output, audios, sr, &#39;PCM_24&#39;) def wav2mp3(input, output): audio = AudioSegment.from_wav(input) audio.export(output, format=&quot;mp3&quot;) . concatenate_tracks(chunk_names, &#39;voice.wav&#39;) . 100%|██████████| 12/12 [00:00&lt;00:00, 45.51it/s] . For convinience when later adding background, I convert the WAV audio file into the MP3 format. . wav2mp3(&#39;voice.wav&#39;, &#39;voice.mp3&#39;) . Background sound . To make the episode more interesting I add a background sound that matches the theme of the episode. I use freesound.org which is a great resouce for loyalty free audio. For instance, some interesting audios: Ocean waves, rain with thunder. . First, download the audio that best matches the episode theme . !curl -s -o base_background.mp3 https://freesound.org/data/previews/237/237729_3839718-lq.mp3 . voice_duration = AudioSegment.from_wav(&#39;voice.wav&#39;).duration_seconds background_duration = AudioSegment.from_mp3(&#39;base_background.mp3&#39;).duration_seconds print(f&quot;Voice duration is {voice_duration} seconds vs base background in {background_duration} seconds.&quot;) . Voice duration is 397.30965986394557 seconds vs base background in 32.875083333333336 seconds. . base_background = AudioSegment.from_mp3(&quot;base_background.mp3&quot;) background = base_background for _ in range(int(voice_duration / background_duration) + 1): background = background + base_background background.export(&quot;background.mp3&quot;, format=&quot;mp3&quot;) background_duration = background.duration_seconds print(f&quot;Voice duration is {voice_duration} seconds vs background in {background_duration} seconds.&quot;) . Voice duration is 397.30965986394557 seconds vs background in 460.2511666666667 seconds. . def add_background_track(episode_file, background_file, output): tempbg = tempfile.mkstemp()[1] tempepisode = tempfile.mkstemp()[1] episode = AudioSegment.from_mp3(episode_file) background = AudioSegment.from_mp3(background_file) padded_episode = AudioSegment.silent(duration=7000) + episode + AudioSegment.silent(duration=8000) padded_episode.export(tempepisode, format=&#39;mp3&#39;) cut_bg = background[: padded_episode.duration_seconds * 1000].fade_in(3000).fade_out(5000) # Lower the background track volume. lower_volume_cut_bg = cut_bg - 10 lower_volume_cut_bg.export(tempbg, format=&#39;mp3&#39;) subprocess.run( [ &quot;ffmpeg&quot;, &quot;-y&quot;, &quot;-i&quot;, tempbg, &quot;-i&quot;, tempepisode, &quot;-filter_complex&quot;, &quot;amerge,acompressor=threshold=-21dB:ratio=12:attack=100:release=500&quot;, &quot;-ac&quot;, &quot;2&quot;, &quot;-c:a&quot;, &quot;libmp3lame&quot;, &quot;-q:a&quot;, &quot;4&quot;, output, ] ) os.unlink(tempbg) os.unlink(tempepisode) . Finally, add the background to the voice file and disply the final result . add_background_track(&#39;voice.mp3&#39;, &#39;background.mp3&#39;, &#39;episode.mp3&#39;) . AudioSegment.from_mp3(&#39;episode.mp3&#39;) . Your browser does not support the audio element. That&#39;s all folks . You can give the podcast a try, all episodes are pulished here https://anchor.fm/exmachina . I would love to hear any feedack, suggestions or ideas for improvement. So feel free to leave a comment or reach out on twitter @bachiirc . &lt;/div&gt; .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/speech/2022/02/18/bedtime-stories-speech.html",
            "relUrl": "/tensorflow/speech/2022/02/18/bedtime-stories-speech.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Exploring bias in Transformers",
            "content": "In Recent years, language models are able to achieve remarkable results, from writing stories (e.g. GPT-2 and GPT-3), turn captions into images, or write code. . Some of those models are trained by a masking technique: taking sentences, splitting them into tokens (i.e. words), and then randomly hiding some of those tokens and train the model to predict those hidden tokens. At the core of those models is a Transformer architecture, hence the name of the transfomers library. Learn more on performing Masked Language Modeling here - link. . In this post, we will try BERT one of those language models and see how its prediction for masked words different based on location or gender. This post is insipired by the followin article - What Have Language Models Learned? . . Let&#39;s install and import dependencies . %%capture %%bash pip install pyyaml==5.4.1 pip install -q transformers . from transformers import TFAutoModelForMaskedLM, AutoTokenizer import tensorflow as tf import plotly.express as px import math import pandas as pd . /usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details. defaults = yaml.load(f) . BERT has lot variation and the transformers library offer easy access many of them, we will use bert-base-uncased but you can find more models here - link . tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) model = TFAutoModelForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;) . All model checkpoint layers were used when initializing TFBertForMaskedLM. All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training. . Let&#39;s define a helper function, that will tokenize a sentence, encode its tokens with IDs in the model vocabulary, do a forward pass and retrieve the predictions at the index of the mask token. As we need probablities we will pass the predictions through a softmax function and collect the top N tokens. . def top_words(sequence, k=100): inputs = tokenizer(sequence, return_tensors=&quot;tf&quot;) mask_token_index = tf.where(inputs[&quot;input_ids&quot;] == tokenizer.mask_token_id)[0, 1] token_logits = model(**inputs).logits mask_token_logits = token_logits[0, mask_token_index, :] mask_token_probs = tf.nn.softmax(mask_token_logits) top_k = tf.math.top_k(mask_token_probs, k) probs, indices = top_k.values.numpy(), top_k.indices.numpy() tokens = [tokenizer.decode([index]) for index in indices] return tokens, probs . For plotting the predicted tokens and using the probabilities as coordiates, we create the following function . def plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title): intersection = list(set(tokens1).intersection(tokens2)) indicies1 = [tokens1.index(token) for token in intersection] indicies2 = [tokens2.index(token) for token in intersection] df = pd.DataFrame.from_dict({ &#39;token&#39;: intersection, &#39;X&#39;: [probs1[index] for index in indicies1], &#39;Y&#39;: [probs2[index] for index in indicies2] }) fig = px.scatter(df, x=&quot;X&quot;, y=&quot;Y&quot;, text=&quot;token&quot;, log_x=True, log_y=True, size_max=60, labels=dict(X=label1, Y=label2) ) fig.update_traces(textposition=&#39;top center&#39;) fig.update_layout(height=800, title_text=title) fig.show() . Let&#39;s define a sentence and try to see how the language model top prediction changes based on the location for New York vs Texas . tokens1, probs1 = top_words(f&quot;in New York, they like to buy {tokenizer.mask_token}.&quot;, 200) tokens2, probs2 = top_words(f&quot;in Texas, they like to buy {tokenizer.mask_token}.&quot;, 200) label1 = &quot;New York&quot; label2 = &#39;Texas&#39; title = &#39;Likelihood per token: New York vs Texas&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . You can see the bias for books, clothes which seem to be predicted more for New Yorkers, while buying cattle have higher likelihood for people in Texas. . Let&#39;s try another set of locations: Londeners vs Algerois . tokens1, probs1 = top_words(f&quot;in London, they like to buy {tokenizer.mask_token}.&quot;, 200) tokens2, probs2 = top_words(f&quot;in Algiers, they like to buy {tokenizer.mask_token}.&quot;, 200) label1 = &#39;London&#39; label2 = &#39;Algiers&#39; title = &#39;Likelihood per token: London vs Algiers&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . Seems that people in London tend to buy way more books than those in Algiers . Now, lets try something with gender difference . tokens1, probs1 = top_words(f&quot;Lauren was born in the year of {tokenizer.mask_token}.&quot;, 200) tokens2, probs2 = top_words(f&quot;Elsie was born in the year of {tokenizer.mask_token}.&quot;, 200) label1 = &#39;Lauren&#39; label2 = &#39;Elsie&#39; title = &#39;Likelihood per token: Lauren vs Elsie&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . tokens1, probs1 = top_words(f&quot;Jane worked as a {tokenizer.mask_token}.&quot;, 200) tokens2, probs2 = top_words(f&quot;Jim worked as a {tokenizer.mask_token}.&quot;, 200) label1 = &#39;Jane&#39; label2 = &#39;Jim&#39; title = &#39;Likelihood per token: Jane vs Jim&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . It is funny how lot of those top predictions does not even make sense, but still the model predicted maid for Jane vs salesman for Jim. . tokens1, probs1 = top_words(f&quot;The doctor performed CPR even though {tokenizer.mask_token} knew it was too late.&quot;, 200) tokens2, probs2 = top_words(f&quot;The nurse performed CPR even though {tokenizer.mask_token} knew it was too late.&quot;, 200) label1 = &#39;Doctor&#39; label2 = &#39;Nurse&#39; title = &#39;Likelihood per token: Doctor vs Nurse&#39; plot_intersection(tokens1, probs1, label1, tokens2, probs2, label2, title) . . . Here while most predictions have low likelihood for both, still he seems to be highly associated with doctor and she is highly associated with nurse. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/nlp/explainability/2022/02/04/transformers_bias.html",
            "relUrl": "/tensorflow/nlp/explainability/2022/02/04/transformers_bias.html",
            "date": " • Feb 4, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Vision Transformer in TensorFlow",
            "content": "The publication of the Vision Transformer (or simply ViT) architecture in An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale had a great impact on the use of a Transformer-based architecture in computer vision problems. In fact, it was the first architecture that made good results on the ImageNet because of those two factors: . Applying Transformers in the processing of the entire image. | Also the training method used in ViT had impact on the model performance. | . Overview . As described in the paper arxiv.org and depicted in the following diagram, ViT works as follows: . Each image is split into fixed-size patches | Calculate a Patch Embeddings for each patch | Add position embeddings a class token to each of the Patch Embeddings | The sequence of embeddings are passed to a Transformer encoder | Pass the representations through a MLP Head to get final class predictions. | . As a detailed example, when trying to classify an image (with similar size as ImageNet images: 224 x 224 x 3, the process is as follows: . First, divide the image into patches of size 16 x 16. This will create 14 x 14 = 196 patches. Then, we put the patches in sequence one after the other as depicted in the diagram above. . Second, pass each of those patches through a linear layer to get an embeddings vector or Patch Embeddings of the patch of size 1 x 768 (note that 3 x 16 x 16 = 768). In the diagram those vectors are colored in pink. . Third, for each patch we calculate it&#39;s Position Embeddings (as shown in purple) then add this vector to the previous Patch Embeddings. We also append at the begining the embeddings of the special token [class]. . After this, we endup with a matrix of size 1 x 768 + 196 x 768 = 197 x 768. . Forth, the patch embeddings are passed through Transformer Encoder to get the learned representations of the [class] token. The output will be a 1 x 768 vector. . Fifth, the representation generated by the transformer is passed to the MLP Head (which is simply a Linear Layer) to generate the class predictions. . Implementation . As described in the previous section, the ViT model consits of a patch embedding, multiple Transformer blocks with self-attention layer, and a multilayer perceptron. . In the rest of this article we will go over each component of the ViT architecture and implement it in TensorFlow. . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.layers import Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention . Patch extraction . The first step in implementing the ViT model is the extraction of pathches from an input image as depicted in the following illustration . . In TensorFlow, we can simply use the tf.image.extract_patches function to extract patches. We can use it inside a custom Layer to make it easy to use later when building the model . class PatchExtractor(Layer): def __init__(self): super(PatchExtractor, self).__init__() def call(self, images): batch_size = tf.shape(images)[0] patches = tf.image.extract_patches( images=images, sizes=[1, 16, 16, 1], strides=[1, 16, 16, 1], rates=[1, 1, 1, 1], padding=&quot;VALID&quot;, ) patch_dims = patches.shape[-1] patches = tf.reshape(patches, [batch_size, -1, patch_dims]) return patches . !curl -s -o flower.jpeg https://images.unsplash.com/photo-1604085572504-a392ddf0d86a?ixlib=rb-1.2.1&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=224&amp;q=224 . image = plt.imread(&#39;flower.jpeg&#39;) image = tf.image.resize(tf.convert_to_tensor(image), size=(224, 224)) plt.imshow(image.numpy().astype(&quot;uint8&quot;)) plt.axis(&quot;off&quot;); . For an image of size (224, 224) we get 196 patches of 16x16 . batch = tf.expand_dims(image, axis=0) patches = PatchExtractor()(batch) patches.shape . TensorShape([1, 196, 768]) . We can visualize all the 196 patches . n = int(np.sqrt(patches.shape[1])) for i, patch in enumerate(patches[0]): ax = plt.subplot(n, n, i + 1) patch_img = tf.reshape(patch, (16, 16, 3)) ax.imshow(patch_img.numpy().astype(&quot;uint8&quot;)) ax.axis(&quot;off&quot;) . Patch Encoding . The Patch encoder takes as input the patches and generate their embeddings which later get passes to the Transformer. For each path, it will also create a positional embeddings vector. The following illustration describes inner working of the patch encoder. . . The PatchEncoder implementation is straightforward, we need a Dense layer to project a patch into a vector of size projection_dim, plus an Embedding layer to learn the positional embeddings. We also need a trainable tf.Variable that will learn the [class] token embeddings. . We use a custom layer to put all this together as follows: . class PatchEncoder(Layer): def __init__(self, num_patches=196, projection_dim=768): super(PatchEncoder, self).__init__() self.num_patches = num_patches self.projection_dim = projection_dim w_init = tf.random_normal_initializer() class_token = w_init(shape=(1, projection_dim), dtype=&quot;float32&quot;) self.class_token = tf.Variable(initial_value=class_token, trainable=True) self.projection = Dense(units=projection_dim) self.position_embedding = Embedding(input_dim=num_patches+1, output_dim=projection_dim) def call(self, patch): batch = tf.shape(patch)[0] # reshape the class token embedins class_token = tf.tile(self.class_token, multiples = [batch, 1]) class_token = tf.reshape(class_token, (batch, 1, self.projection_dim)) # calculate patches embeddings patches_embed = self.projection(patch) patches_embed = tf.concat([patches_embed, class_token], 1) # calcualte positional embeddings positions = tf.range(start=0, limit=self.num_patches+1, delta=1) positions_embed = self.position_embedding(positions) # add both embeddings encoded = patches_embed + positions_embed return encoded . We can confirm that the output of this layer is as expected 1, 197, 768 . embeddings = PatchEncoder()(patches) embeddings.shape . TensorShape([1, 197, 768]) . Multilayer Perceptron . A Multilayer Perceptron (MLP) consists basically two dense layers and a GELU activation layer. It is used in the Transformer encoder as well as the final output layer of the ViT model. We can simply implement it as custom layer as follows: . class MLP(Layer): def __init__(self, hidden_features, out_features, dropout_rate=0.1): super(MLP, self).__init__() self.dense1 = Dense(hidden_features, activation=tf.nn.gelu) self.dense2 = Dense(out_features) self.dropout = Dropout(dropout_rate) def call(self, x): x = self.dense1(x) x = self.dropout(x) x = self.dense2(x) y = self.dropout(x) return y . mlp = MLP(768 * 2, 768) y = mlp(tf.zeros((1, 197, 768))) y.shape . TensorShape([1, 197, 768]) . Transformer Encoder . The transformer encoder consists of a sequence of L blocs of typical Transformer architecture. So we just need to implement the bloc once and use it multiple times . . The transfomer bloc uses LayerNormalization and MultiHeadAttention layers, along with some skip connections. The following custom layer implements the Transformer bloc . class Block(Layer): def __init__(self, projection_dim, num_heads=4, dropout_rate=0.1): super(Block, self).__init__() self.norm1 = LayerNormalization(epsilon=1e-6) self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate) self.norm2 = LayerNormalization(epsilon=1e-6) self.mlp = MLP(projection_dim * 2, projection_dim, dropout_rate) def call(self, x): # Layer normalization 1. x1 = self.norm1(x) # encoded_patches # Create a multi-head attention layer. attention_output = self.attn(x1, x1) # Skip connection 1. x2 = Add()([attention_output, x]) #encoded_patches # Layer normalization 2. x3 = self.norm2(x2) # MLP. x3 = self.mlp(x3) # Skip connection 2. y = Add()([x3, x2]) return y . block = Block(768) y = block(tf.zeros((1, 197, 768))) y.shape . TensorShape([1, 197, 768]) . Now, we can simply create custom layer that implements a Transfomer using the previous Block layer. . class TransformerEncoder(Layer): def __init__(self, projection_dim, num_heads=4, num_blocks=12, dropout_rate=0.1): super(TransformerEncoder, self).__init__() self.blocks = [Block(projection_dim, num_heads, dropout_rate) for _ in range(num_blocks)] self.norm = LayerNormalization(epsilon=1e-6) self.dropout = Dropout(0.5) def call(self, x): # Create a [batch_size, projection_dim] tensor. for block in self.blocks: x = block(x) x = self.norm(x) y = self.dropout(x) return y . transformer = TransformerEncoder(768) y = transformer(embeddings) y.shape . TensorShape([1, 197, 768]) . Putting it together . After defining all the major components of the ViT architecture, we can put them together to build the model. This is fairly straightforward now as we just need to plug a patch extract to an encoder to a transformer to a multilayer perceptron as depicted in the following diagram . . def create_VisionTransformer(num_classes, num_patches=196, projection_dim=768, input_shape=(224, 224, 3)): inputs = Input(shape=input_shape) # Patch extractor patches = PatchExtractor()(inputs) # Patch encoder patches_embed = PatchEncoder(num_patches, projection_dim)(patches) # Transformer encoder representation = TransformerEncoder(projection_dim)(patches_embed) representation = GlobalAveragePooling1D()(representation) # MLP to classify outputs logits = MLP(projection_dim, num_classes, 0.5)(representation) # Create model model = Model(inputs=inputs, outputs=logits) return model . model = create_VisionTransformer(2) . model.summary() . Model: &#34;model_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_7 (InputLayer) [(None, 224, 224, 3)] 0 patch_extractor_5 (PatchExt (None, None, 768) 0 ractor) patch_encoder_13 (PatchEnco (None, 197, 768) 742656 der) transformer_encoder_11 (Tra (None, 197, 768) 141743616 nsformerEncoder) global_average_pooling1d_7 (None, 768) 0 (GlobalAveragePooling1D) mlp_151 (MLP) (None, 2) 592130 ================================================================= Total params: 143,078,402 Trainable params: 143,078,402 Non-trainable params: 0 _________________________________________________________________ . That&#39;s all folks . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/classification/2021/10/01/vision_transformer.html",
            "relUrl": "/tensorflow/vision/classification/2021/10/01/vision_transformer.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Crypto trading platform with OpenAI Gym",
            "content": "In this article we will build a cryptocurrency trading exchange with OpenAI Gym and using real-world data from the Gemini cryptocurrency exchange. This environment will enable us to develop and train RL agents capable of performing crypto trading. i.e. buy/sell or hold crypto tokens and get rewarded accordingly by either reporting a profit or a loss. . #collapse-hide %%capture %%bash mkdir -p data curl -o data/Gemini_BTCUSD_d.csv https://raw.githubusercontent.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook/master/Chapter04/data/Gemini_BTCUSD_d.csv curl -o data/Gemini_ETHUSD_d.csv https://raw.githubusercontent.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook/master/Chapter04/data/Gemini_ETHUSD_d.csv . . First, lets install the dependencies need to be able to run this notebook: OpenAI Gym, xvfb X window server, and mplfinance for finantial data visualization. . %%capture %%bash apt install xvfb pip install gym pip install mplfinance . Second, lets import the necessary Python modules that we will need in the following sections. . import os import random from typing import Dict import gym import numpy as np import pandas as pd from gym import spaces import plotly.express as px . import matplotlib import matplotlib.pyplot as plt import mplfinance as mpf import numpy as np import pandas as pd from matplotlib import style from mplfinance.original_flavor import candlestick_ohlc from IPython import display from matplotlib import animation . %matplotlib inline . Next, we define a configuration dictionnary that we will use to set the crypto ticket to trade, the initial balance available to the agent, window size of the number of days in the past, etc. . env_config = { &quot;exchange&quot;: &quot;Gemini&quot;, # Cryptocurrency exchange (Gemini, coinbase, kraken, etc.) &quot;ticker&quot;: &quot;BTCUSD&quot;, # CryptoFiat &quot;frequency&quot;: &quot;daily&quot;, # trading frequency daily/hourly/minutes &quot;opening_account_balance&quot;: 100000, # Starting balance in USD &quot;observation_horizon_sequence_length&quot;: 30, # Window size in days &quot;order_size&quot;: 1, # Number of coins to buy per buy/sell order } . Next, we define a class to facilitate the access to the crypto price data. This will hide the implementation details about where the data comes from, e.g. from a public API. In our case, for simplicity, the price data is available offline from a local CSV file. . The class also uses the previous configuration dictionnary, and exposes some utility functions like getting the current price of a symbol. . class TradingData: def __init__(self, env_config: Dict = env_config): self.ticker = env_config.get(&quot;ticker&quot;, &quot;BTCUSD&quot;) data_dir = &quot;data&quot; self.exchange = env_config[&quot;exchange&quot;] freq = env_config[&quot;frequency&quot;] if freq == &quot;daily&quot;: self.freq_suffix = &quot;d&quot; elif freq == &quot;hourly&quot;: self.freq_suffix = &quot;1hr&quot; elif freq == &quot;minutes&quot;: self.freq_suffix = &quot;1min&quot; self.ticker_file_stream = os.path.join(f&quot;{data_dir}&quot;,f&quot;{&#39;_&#39;.join([self.exchange, self.ticker,self.freq_suffix])}.csv&quot;,) assert os.path.isfile(self.ticker_file_stream), f&quot;Cryptocurrency data file stream not found at: data/{self.ticker_file_stream}.csv&quot; self.ohlcv_df = pd.read_csv(self.ticker_file_stream, parse_dates=True, index_col=&quot;Date&quot;, skiprows=1).sort_values(by=&quot;Date&quot;) if &quot;USD&quot; in self.ticker: self.ohlcv_df[&quot;Volume&quot;] = self.ohlcv_df[ &quot;Volume &quot; + self.ticker[:-3] # e.g: &quot;Volume BTC&quot; ] self.horizon = env_config.get(&quot;observation_horizon_sequence_length&quot;) self.observation_features = [&quot;Open&quot;,&quot;High&quot;,&quot;Low&quot;,&quot;Close&quot;,&quot;Volume BTC&quot;,&quot;Volume USD&quot;,] def get_current_price(self, current_step): # Stochastically determine the current price based on Market Open and Close prices current_date = self.ohlcv_df.index[current_step] open_price = self.ohlcv_df.loc[current_date, &quot;Open&quot;] close_price = self.ohlcv_df.loc[current_date,&quot;Close&quot;] return random.uniform(open_price, close_price,) def get_size(self): return len(self.ohlcv_df.loc[:, &quot;Open&quot;].values) def get_observation(self, current_step): # Get crypto price info data table from input (file/live) stream first_date = self.ohlcv_df.index[current_step] last_date = self.ohlcv_df.index[current_step + self.horizon] observation = ( self.ohlcv_df.loc[ first_date : last_date, self.observation_features, ] .to_numpy() .T ) return observation . We can visualize the data loaded by the TradingData class by showing the changes in Open, High, Low, Close (OHLC) prices of Bitcoin in this dataset. . data = TradingData() . df = data.ohlcv_df.reset_index() df = df[[&#39;Date&#39;, &#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]] #&#39;Symbol&#39; df = df.melt(id_vars=&#39;Date&#39;, value_vars=[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;], var_name=&#39;BTC&#39;) px.line(df, x=&#39;Date&#39;, y=&#39;value&#39;, color=&#39;BTC&#39; ) . . . Next, we build a utility class that will visualize the state of our Crypto Trading Exchange using two charts: . A chart for visualizing the balance of the Agent over time, this is a simple line chart of the balance | A candlesticks chart of Open, High, Low, Close (OHLC) prices of Bitcoin, this is built using mplfinance library | . class TradeVisualizer: def __init__(self, data, title=None): self.data = data self.account_balances = np.zeros(len(self.data.ohlcv_df.index)) fig = plt.figure(&quot;Crypto Trading Exchange&quot;, figsize=[18, 10]) nrows, ncols = 6, 1 gs = fig.add_gridspec(nrows, ncols) row, col = 0, 0 rowspan, colspan = 2, 1 self.account_balance_ax = fig.add_subplot( gs[row : row + rowspan, col : col + colspan] ) row, col = 2, 0 rowspan, colspan = 8, 1 self.price_ax = plt.subplot2grid( (nrows, ncols), (row, col), rowspan=rowspan, colspan=colspan, sharex=self.account_balance_ax, ) self.price_ax = fig.add_subplot(gs[row : row + rowspan, col : col + colspan]) self.viz_not_initialized = True self.fig = fig def _render_account_balance(self, current_step, account_balance, horizon): self.account_balance_ax.clear() date_range = self.data.ohlcv_df.index[current_step : current_step + len(horizon)] self.account_balance_ax.plot_date( date_range, self.account_balances[horizon], &quot;-&quot;, label=&quot;Account Balance ($)&quot;, lw=1.0, ) self.account_balance_ax.legend() legend = self.account_balance_ax.legend(loc=2, ncol=2) legend.get_frame().set_alpha(0.4) last_date = self.data.ohlcv_df.index[current_step + len(horizon)].strftime(&quot;%Y-%m-%d&quot;) last_date = matplotlib.dates.datestr2num(last_date) last_account_balance = self.account_balances[current_step] self.account_balance_ax.annotate( &quot;{0:.2f}&quot;.format(account_balance), (last_date, last_account_balance), xytext=(last_date, last_account_balance), bbox=dict(boxstyle=&quot;round&quot;, fc=&quot;w&quot;, ec=&quot;k&quot;, lw=1), color=&quot;black&quot;, ) self.account_balance_ax.set_ylim( min(self.account_balances[np.nonzero(self.account_balances)]) / 1.25, max(self.account_balances) * 1.25, ) plt.setp(self.account_balance_ax.get_xticklabels(), visible=False) def _render_ohlc(self, current_step, dates, horizon): self.price_ax.clear() candlesticks = zip( dates, self.data.ohlcv_df[&quot;Open&quot;].values[horizon], self.data.ohlcv_df[&quot;Close&quot;].values[horizon], self.data.ohlcv_df[&quot;High&quot;].values[horizon], self.data.ohlcv_df[&quot;Low&quot;].values[horizon], ) candlestick_ohlc( self.price_ax, candlesticks, width=np.timedelta64(1, &quot;D&quot;), colorup=&quot;g&quot;, colordown=&quot;r&quot;, ) self.price_ax.set_ylabel(f&quot;{self.data.ticker} Price ($)&quot;) self.price_ax.tick_params(axis=&quot;y&quot;, pad=30) last_date = self.data.ohlcv_df.index[current_step].strftime(&quot;%Y-%m-%d&quot;) last_date = matplotlib.dates.datestr2num(last_date) last_close = self.data.ohlcv_df[&quot;Close&quot;].values[current_step] last_high = self.data.ohlcv_df[&quot;High&quot;].values[current_step] self.price_ax.annotate( &quot;{0:.2f}&quot;.format(last_close), (last_date, last_close), xytext=(last_date, last_high), bbox=dict(boxstyle=&quot;round&quot;, fc=&quot;w&quot;, ec=&quot;k&quot;, lw=1), color=&quot;black&quot;, ) plt.setp(self.price_ax.get_xticklabels(), visible=False) def _render_trades(self, trades, horizon): for trade in trades: if trade[&quot;step&quot;] in horizon: date = self.data.ohlcv_df.index[trade[&quot;step&quot;]].strftime(&quot;%Y-%m-%d&quot;) date = matplotlib.dates.datestr2num(date) high = self.data.ohlcv_df[&quot;High&quot;].values[trade[&quot;step&quot;]] low = self.data.ohlcv_df[&quot;Low&quot;].values[trade[&quot;step&quot;]] if trade[&quot;type&quot;] == &quot;buy&quot;: high_low = low color = &quot;g&quot; arrow_style = &quot;&lt;|-&quot; else: # sell high_low = high color = &quot;r&quot; arrow_style = &quot;-|&gt;&quot; proceeds = &quot;{0:.2f}&quot;.format(trade[&quot;proceeds&quot;]) self.price_ax.annotate( f&quot;{trade[&#39;type&#39;]} ${proceeds}&quot;.upper(), (date, high_low), xytext=(date, high_low), color=color, arrowprops=( dict( color=color, arrowstyle=arrow_style, connectionstyle=&quot;angle3&quot;, ) ), ) def render(self, current_step, account_balance, trades, window_size=100): self.account_balances[current_step] = account_balance window_start = max(current_step - window_size, 0) step_range = range(window_start, current_step + 1) dates = self.data.ohlcv_df.index[step_range] self._render_account_balance(current_step, account_balance, step_range) self._render_ohlc(current_step, dates, step_range) self._render_trades(trades, step_range) self.fig.canvas.draw() fig_data = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=np.uint8) fig_data = fig_data.reshape(self.fig.canvas.get_width_height()[::-1] + (3,)) return fig_data def close(self): plt.close() . Next, we define the TradingService class which provides a set of API for the agent to execute a trade (sell or buy) and update accordingly the balance. . reset() method used to reset the agent state (e.g. reset the balance to it&#39;s initial value, clear the list of trades executed so far by the agent) | execute_trade(): executes Buy/Sell trade and update the account balance once the trade order has been executed. | . class TradingService: def __init__(self, env_config: Dict = env_config, data = None): self.opening_account_balance = env_config[&quot;opening_account_balance&quot;] self.horizon = env_config.get(&quot;observation_horizon_sequence_length&quot;) self.order_size = env_config.get(&quot;order_size&quot;) self.data = data if data else TradingData() def reset(self): # Reset the state of the environment to an initial state self.cash_balance = self.opening_account_balance self.account_value = self.opening_account_balance self.num_coins_held = 0 self.cost_basis = 0 self.trades = [] def execute_buy(self, current_price, current_step): # Simulate a BUY order and execute it at current_price allowable_coins = int(self.cash_balance / current_price) if allowable_coins &lt; self.order_size: # Not enough cash to execute a buy order return num_coins_bought = self.order_size current_cost = self.cost_basis * self.num_coins_held additional_cost = num_coins_bought * current_price self.cash_balance -= additional_cost self.cost_basis = (current_cost + additional_cost) / (self.num_coins_held + num_coins_bought) self.num_coins_held += num_coins_bought self.trades.append({ &quot;type&quot;: &quot;buy&quot;, &quot;step&quot;: current_step, &quot;shares&quot;: num_coins_bought, &quot;proceeds&quot;: additional_cost, }) def execute_sell(self, current_price, current_step): # Simulate a SELL order and execute it at current_price if self.num_coins_held &lt; self.order_size: # Not enough coins to execute a sell order return num_coins_sold = self.order_size self.cash_balance += num_coins_sold * current_price self.num_coins_held -= num_coins_sold sale_proceeds = num_coins_sold * current_price self.trades.append({ &quot;type&quot;: &quot;sell&quot;, &quot;step&quot;: current_step, &quot;shares&quot;: num_coins_sold, &quot;proceeds&quot;: sale_proceeds, }) def execute_trade_action(self, action, current_step): if action == 0: # Hold position return order_type = &quot;buy&quot; if action == 1 else &quot;sell&quot; current_price = self.data.get_current_price(current_step) if order_type == &quot;buy&quot;: self.execute_buy(current_price, current_step) elif order_type == &quot;sell&quot;: self.execute_sell(current_price, current_step) if self.num_coins_held == 0: self.cost_basis = 0 # Update account value self.account_value = self.cash_balance + self.num_coins_held * current_price . Finally, we define the tranding exchange environment class that takes the previous implemented utility classes for trading, data and visualiztion. It will subclass the gym.Env interface and implement the following main methods: . reset() method executed at the start of every episode to reset the state of the environment and the agent. | step() method executed at each step of an episode, it takes the action (e.g. hold, sell, buy) selected by the agent and execute it. After that, it calculates the reward (i.e. the profit or loss corresponding to the action execution). | render() method uses the TraidingVisualizer class to render the current state of the environment and the agent current balance. | close() method closes all visualizations. | . class TradingEnv(gym.Env): def __init__(self, service = None, data = None, env_config: Dict = env_config): super().__init__() self.data = data if data else TradingData() self.service = service if service else TradingService() self.opening_account_balance = env_config[&quot;opening_account_balance&quot;] self.horizon = env_config.get(&quot;observation_horizon_sequence_length&quot;) self.viewer = None self.action_space = spaces.Discrete(3) self.observation_features = [&quot;Open&quot;,&quot;High&quot;,&quot;Low&quot;,&quot;Close&quot;,&quot;Volume BTC&quot;,&quot;Volume USD&quot;,] self.observation_space = spaces.Box(low=0,high=1,shape=(len(self.observation_features),self.horizon + 1),dtype=np.float,) self.viz = None def step(self, action): # Execute one step within the trading environment self.service.execute_trade_action(action, self.current_step) self.current_step += 1 account_value = self.service.account_value reward = account_value - self.opening_account_balance done = account_value &lt;= 0 or self.current_step &gt;= self.data.get_size() obs = self.data.get_observation(self.current_step) return obs, reward, done, {} def reset(self): # Reset the state of the environment to an initial state self.current_step = 0 self.service.reset() if self.viz is None: self.viz = TradeVisualizer(self.data, &quot;TradingEnv&quot;) return self.data.get_observation(self.current_step) def render(self, mode=&quot;rgb_array&quot;): # Render the environment img = self.viz.render(self.current_step,self.service.account_value,self.service.trades,window_size=self.horizon,) if mode == &quot;rgb_array&quot;: return img elif mode == &quot;human&quot;: if self.viewer is None: from gym.envs.classic_control import rendering self.viewer = rendering.SimpleImageViewer() self.viewer.imshow(img) def close(self): if self.viz is not None: self.viz.close() self.viz = None . Now, we are ready to test our trading environment. For simplicity, we will run one episode and use a basic agent that will randomly select an action to be executed from the possible ones (buy, sell, or hold). We will keep track of the rewards from each action so we can display them later. . env = TradingEnv() images, rewards = [], [] obs = env.reset() for _ in range(100): action = env.action_space.sample() next_obs, reward, done, _ = env.step(action) images.append(env.render()) rewards.append(reward) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations . After finishing the episode, we can display the evolution of the environment as well as the agent states as follows: . plt.rcParams[&#39;axes.grid&#39;] = False . dpi = 72 interval = 50 fig = plt.figure(figsize=(images[0].shape[1]/dpi,images[0].shape[0]/dpi), dpi=dpi, frameon=False) ax = fig.add_axes([0, 0, 1, 1]) plt.axis=(&#39;off&#39;) patch = ax.imshow(images[0]) animate = lambda i: patch.set_data(images[i]) ani = animation.FuncAnimation(fig, animate, frames=len(images),interval=interval) . The following video is the recoding of the states . display.display(display.HTML(ani.to_jshtml())) . &lt;/input&gt; Once Loop Reflect We can display the rewards collected by the agent. . df = pd.DataFrame({&quot;reward&quot;: rewards}) fig = px.line(df, y=&quot;reward&quot;, title=&#39;Rewards over time&#39;, labels=dict(index=&quot;Step&quot;, reward=&quot;Reward&quot;)) fig.show() . . . Now we have an environment ready to use for testing different trading strategies, but this will be part of another article. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/reinforcement/2021/08/04/Crypto_trading_platform_with_Gym.html",
            "relUrl": "/tensorflow/reinforcement/2021/08/04/Crypto_trading_platform_with_Gym.html",
            "date": " • Aug 4, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Build a custom OpenAI Gym environment",
            "content": "OpenAI Gym is a comprehensive platform for building and testing RL strategies. It comes will a lot of ready to use environments but in some case when you&#39;re trying a solve specific problem and cannot use off the shelf environments. In this case, you can still leverage Gym to build a custom environment and this post walks through how to do it. . As an example, we will build a GridWorld environment with the following rules: . Each cell of this environment can have one of the following colors: . BLUE: a cell reprensentig the agent | GREEN: a cell reprensentig the target destination | GRAY: a cell representing a wall | RED: a cell representing a bomb | BLACK: an empty cell | . In this environment, the agent can move UP, DOWN, LEFT or RIGHT. The agent get rewarded dependening on the cell color where the move will take it: . 0.0 if the agent moves into an empty cell (i.e. BLACK cell) | -0.1 if the agent hits a cell containing a wall (i.e. GRAY cell) | -1.0 if the agent hits a cell containing a bomb (i.e. RED cell) | 1.0 if the agent hits a cell containing the destinaton (i.e. GREEN cell) | . The following picture depicts how the GridWorld environment looks like: . . First, we need to install Gym and its dependencies to be able to use it and run it on a notebook: . %%capture %%bash apt install xvfb pip install gym pip install gym-notebook-wrapper . from collections import defaultdict import copy import sys import numpy as np import gnwrapper import gym . Second, we declare the constants to represent cells type as well as the type of the agent actions: . # Grid cell state and color mapping EMPTY = BLACK = 0 WALL = GRAY = 1 AGENT = BLUE = 2 BOMB = RED = 3 GOAL = GREEN = 4 # RGB color value table COLOR_MAP = { BLACK: [0.0, 0.0, 0.0], GRAY: [0.5, 0.5, 0.5], BLUE: [0.0, 0.0, 1.0], RED: [1.0, 0.0, 0.0], GREEN: [0.0, 1.0, 0.0], } # Action mapping NOOP = 0 DOWN = 1 UP = 2 LEFT = 3 RIGHT = 4 . We can randomly initiate a grid to use on each episode, but for simplicity we initiate the grid to the same configuration as follows: . DEFAULT_GRID_LAYOUT = &quot;&quot;&quot; 1 1 1 1 1 1 1 1 1 2 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 4 1 0 0 1 1 0 3 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 &quot;&quot;&quot; . Then, we define the memory representation of the environment state using the following State class. . class State: def __init__(self, grid_layout=DEFAULT_GRID_LAYOUT, max_steps=100): self.grid_layout = DEFAULT_GRID_LAYOUT self.initial_grid_state = np.fromstring(self.grid_layout, dtype=int, sep=&quot; &quot;) self.initial_grid_state = self.initial_grid_state.reshape(8, 8) self.grid_state = copy.deepcopy(self.initial_grid_state) self.action_pos_dict = defaultdict( lambda: [0, 0], { NOOP: [0, 0], UP: [-1, 0], DOWN: [1, 0], LEFT: [0, -1], RIGHT: [0, 1], }, ) (self.agent_state, self.goal_state) = self.get_state() self.done = False self.info = {&quot;status&quot;: &quot;Live&quot;} self.step_num = 0 # To keep track of number of steps self.max_steps = max_steps def get_shape(self): return self.grid_state.shape def get_state(self): &quot;&quot;&quot;Get the agent&#39;s position, as well as the goal&#39;s position &quot;&quot;&quot; start_state = np.where(self.grid_state == AGENT) goal_state = np.where(self.grid_state == GOAL) start_or_goal_not_found = not (start_state[0] and goal_state[0]) if start_or_goal_not_found: sys.exit( &quot;Start and/or Goal state not present in the Gridworld. &quot; &quot;Check the Grid layout&quot; ) start_state = (start_state[0][0], start_state[1][0]) goal_state = (goal_state[0][0], goal_state[1][0]) return start_state, goal_state def reset(self): &quot;&quot;&quot;Reset the grid state &quot;&quot;&quot; self.grid_state = copy.deepcopy(self.initial_grid_state) self.done = False self.info[&quot;status&quot;] = &quot;Live&quot; self.step_num = 0 def is_next_state_invalid(self, next_state): &quot;&quot;&quot;Check if the next agent move is invalid or not &quot;&quot;&quot; next_state_invalid = ( next_state[0] &lt; 0 or next_state[0] &gt;= self.grid_state.shape[0] ) or (next_state[1] &lt; 0 or next_state[1] &gt;= self.grid_state.shape[1]) return next_state_invalid def execute_action(self, action): &quot;&quot;&quot;Execute the given action and calculate the reward &quot;&quot;&quot; reward = 0.0 next_state = ( self.agent_state[0] + self.action_pos_dict[action][0], self.agent_state[1] + self.action_pos_dict[action][1], ) next_state_invalid = self.is_next_state_invalid(next_state) if next_state_invalid: # Leave the agent state unchanged next_state = self.agent_state self.info[&quot;status&quot;] = &quot;Next state is invalid&quot; next_agent_state = self.grid_state[next_state[0], next_state[1]] # Calculate reward if next_agent_state == EMPTY: # Move agent from previous state to the next state on the grid self.info[&quot;status&quot;] = &quot;Agent moved to a new cell&quot; self.grid_state[next_state[0], next_state[1]] = AGENT self.grid_state[self.agent_state[0], self.agent_state[1]] = EMPTY self.agent_state = copy.deepcopy(next_state) elif next_agent_state == WALL: # Agent hit a wall with this move self.info[&quot;status&quot;] = &quot;Agent bumped into a wall&quot; reward = -0.1 elif next_agent_state == GOAL: # Terminal state: goal reached self.info[&quot;status&quot;] = &quot;Agent reached the GOAL &quot; self.done = True reward = 1 elif next_agent_state == BOMB: # Terminal state: agent died self.info[&quot;status&quot;] = &quot;Agent stepped on a BOMB&quot; self.done = True reward = -1 else: # NOOP or next state is invalid self.done = False self.step_num += 1 # Check if max steps per episode has been reached if self.step_num &gt;= self.max_steps: self.done = True self.info[&quot;status&quot;] = &quot;Max steps reached&quot; return reward, self.done, self.info . Then, to be able to display the environment state we define the following GridRenderer class that will be used to convert a grid into an RGB image . class GridRenderer: def __init__(self, img_shape): self.img_shape = img_shape def render(self, grid_state): observation = np.random.randn(*self.img_shape) * 0.0 scale_x = int(observation.shape[0] / grid_state.shape[0]) scale_y = int(observation.shape[1] / grid_state.shape[1]) for i in range(grid_state.shape[0]): for j in range(grid_state.shape[1]): for k in range(3): # 3-channel RGB image pixel_value = COLOR_MAP[grid_state[i, j]][k] observation[ i * scale_x : (i + 1) * scale_x, j * scale_y : (j + 1) * scale_y, k, ] = pixel_value return (255 * observation).astype(np.uint8) . Then, we define the GridworldEnv class representing our environment by subclassesing the gym.Env class and implementing its interface: . step(action): executes the given Agent&#39;s action and return the reward | reset(): resets the internal state of the environment (represented by the State class) | render(): using a GridRenderer it renders the internal state of the environment | . class GridworldEnv(gym.Env): def __init__(self): &quot;&quot;&quot;Initialize Gridworld Args: max_steps (int, optional): Max steps per episode. Defaults to 100. &quot;&quot;&quot; # Observations self.state = State() self.observation_space = gym.spaces.Box( low=0, high=6, shape=self.state.get_shape() ) self.metadata = {&quot;render.modes&quot;: [&quot;human&quot;]} # Actions self.actions = [NOOP, UP, DOWN, LEFT, RIGHT] self.action_space = gym.spaces.Discrete(len(self.actions)) self.viewer = None self.renderer = GridRenderer([256, 256, 3]) def step(self, action): &quot;&quot;&quot;Return next observation, reward, done , info&quot;&quot;&quot; action = int(action) reward, done, info = self.state.execute_action(action) if done: terminal_state = copy.deepcopy(self.state) _ = self.reset() return (terminal_state, reward, done, info) return self.state, reward, done, info def reset(self): self.state.reset() return self.state def render(self, mode=&quot;human&quot;, close=False): if close: if self.viewer is not None: self.viewer.close() self.viewer = None return grid_state = self.state.grid_state img = self.renderer.render(grid_state) if mode == &quot;rgb_array&quot;: return img elif mode == &quot;human&quot;: from gym.envs.classic_control import rendering if self.viewer is None: self.viewer = rendering.SimpleImageViewer() self.viewer.imshow(img) def close(self): self.render(close=True) @staticmethod def get_action_meanings(): return [&quot;NOOP&quot;, &quot;DOWN&quot;, &quot;UP&quot;, &quot;LEFT&quot;, &quot;RIGHT&quot;] . Finally, we can use our new environment and test it with a basic agent that randomly sample an action. . grid_env = GridworldEnv() env = gnwrapper.LoopAnimation(grid_env) obs, done = env.reset(), False rewards = [] # Run one episode while not done: # Sample a random action from the action space action = env.action_space.sample() next_obs, reward, done, info = env.step(action) rewards.append(reward) env.render() env.display() . &lt;/input&gt; Once Loop Reflect We can display the rewards collected by the agent. . Note how the eposide ended when the agent hit the bomb (i.e. -1.0 reward) . import plotly.express as px import pandas as pd df = pd.DataFrame({&quot;reward&quot;: rewards}) fig = px.line(df, y=&quot;reward&quot;, title=&#39;Rewards over time&#39;, labels=dict(index=&quot;Step&quot;, reward=&quot;Reward&quot;)) fig.show() . . . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/reinforcement/2021/07/25/Custom_Gym_environment.html",
            "relUrl": "/tensorflow/reinforcement/2021/07/25/Custom_Gym_environment.html",
            "date": " • Jul 25, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Build a basic neural agent in TensorFlow",
            "content": "This post walks through how to build the foundation blocks for implementing a Neural Agent. We start with basic Agent that can take action from the space of valid actions but does not learn from the rewards returned by the environemnt. . For this task we will use a simple environment GridWorld (see below picture). . . Setup . First, we need to install OpenAI Gym and the dependencies needed to run Gym on a notebook. . %%capture %%bash apt install xvfb pip install gym pip install gym-notebook-wrapper . Import the dependencies and Gym&#39;s GridWorld environment . import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers import gnwrapper import gym . #collapse-hide %%writefile gridworld.py #!/usr/bin/env python # GridWorld RL environment with image observations # Chapter 1, TensorFlow 2 Reinforcement Learning Cookbook | Praveen Palanisamy from collections import defaultdict import copy import sys import gym import numpy as np # Grid cell state and color mapping EMPTY = BLACK = 0 WALL = GRAY = 1 AGENT = BLUE = 2 BOMB = RED = 3 GOAL = GREEN = 4 # RGB color value table COLOR_MAP = { BLACK: [0.0, 0.0, 0.0], GRAY: [0.5, 0.5, 0.5], BLUE: [0.0, 0.0, 1.0], RED: [1.0, 0.0, 0.0], GREEN: [0.0, 1.0, 0.0], } # Action mapping NOOP = 0 DOWN = 1 UP = 2 LEFT = 3 RIGHT = 4 class GridworldEnv(gym.Env): def __init__(self, max_steps=100): &quot;&quot;&quot;Initialize Gridworld Args: max_steps (int, optional): Max steps per episode. Defaults to 100. &quot;&quot;&quot; # Observations self.grid_layout = &quot;&quot;&quot; 1 1 1 1 1 1 1 1 1 2 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 4 1 0 0 1 1 0 3 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 &quot;&quot;&quot; self.initial_grid_state = np.fromstring(self.grid_layout, dtype=int, sep=&quot; &quot;) self.initial_grid_state = self.initial_grid_state.reshape(8, 8) self.grid_state = copy.deepcopy(self.initial_grid_state) self.observation_space = gym.spaces.Box( low=0, high=6, shape=self.grid_state.shape ) self.img_shape = [256, 256, 3] self.metadata = {&quot;render.modes&quot;: [&quot;human&quot;]} # Actions self.action_space = gym.spaces.Discrete(5) self.actions = [NOOP, UP, DOWN, LEFT, RIGHT] self.action_pos_dict = defaultdict( lambda: [0, 0], { NOOP: [0, 0], UP: [-1, 0], DOWN: [1, 0], LEFT: [0, -1], RIGHT: [0, 1], }, ) (self.agent_state, self.goal_state) = self.get_state() self.step_num = 0 # To keep track of number of steps self.max_steps = max_steps self.done = False self.info = {&quot;status&quot;: &quot;Live&quot;} self.viewer = None def step(self, action): &quot;&quot;&quot;Return next observation, reward, done , info&quot;&quot;&quot; action = int(action) reward = 0.0 next_state = ( self.agent_state[0] + self.action_pos_dict[action][0], self.agent_state[1] + self.action_pos_dict[action][1], ) next_state_invalid = ( next_state[0] &lt; 0 or next_state[0] &gt;= self.grid_state.shape[0] ) or (next_state[1] &lt; 0 or next_state[1] &gt;= self.grid_state.shape[1]) if next_state_invalid: # Leave the agent state unchanged next_state = self.agent_state self.info[&quot;status&quot;] = &quot;Next state is invalid&quot; next_agent_state = self.grid_state[next_state[0], next_state[1]] # Calculate reward if next_agent_state == EMPTY: # Move agent from previous state to the next state on the grid self.info[&quot;status&quot;] = &quot;Agent moved to a new cell&quot; self.grid_state[next_state[0], next_state[1]] = AGENT self.grid_state[self.agent_state[0], self.agent_state[1]] = EMPTY self.agent_state = copy.deepcopy(next_state) elif next_agent_state == WALL: self.info[&quot;status&quot;] = &quot;Agent bumped into a wall&quot; reward = -0.1 # Terminal states elif next_agent_state == GOAL: self.info[&quot;status&quot;] = &quot;Agent reached the GOAL &quot; self.done = True reward = 1 elif next_agent_state == BOMB: self.info[&quot;status&quot;] = &quot;Agent stepped on a BOMB&quot; self.done = True reward = -1 # elif next_agent_state == AGENT: else: # NOOP or next state is invalid self.done = False self.step_num += 1 # Check if max steps per episode has been reached if self.step_num &gt;= self.max_steps: self.done = True self.info[&quot;status&quot;] = &quot;Max steps reached&quot; if self.done: done = True terminal_state = copy.deepcopy(self.grid_state) terminal_info = copy.deepcopy(self.info) _ = self.reset() return (terminal_state, reward, done, terminal_info) return self.grid_state, reward, self.done, self.info def reset(self): self.grid_state = copy.deepcopy(self.initial_grid_state) ( self.agent_state, self.agent_goal_state, ) = self.get_state() self.step_num = 0 self.done = False self.info[&quot;status&quot;] = &quot;Live&quot; return self.grid_state def get_state(self): start_state = np.where(self.grid_state == AGENT) goal_state = np.where(self.grid_state == GOAL) start_or_goal_not_found = not (start_state[0] and goal_state[0]) if start_or_goal_not_found: sys.exit( &quot;Start and/or Goal state not present in the Gridworld. &quot; &quot;Check the Grid layout&quot; ) start_state = (start_state[0][0], start_state[1][0]) goal_state = (goal_state[0][0], goal_state[1][0]) return start_state, goal_state def gridarray_to_image(self, img_shape=None): if img_shape is None: img_shape = self.img_shape observation = np.random.randn(*img_shape) * 0.0 scale_x = int(observation.shape[0] / self.grid_state.shape[0]) scale_y = int(observation.shape[1] / self.grid_state.shape[1]) for i in range(self.grid_state.shape[0]): for j in range(self.grid_state.shape[1]): for k in range(3): # 3-channel RGB image pixel_value = COLOR_MAP[self.grid_state[i, j]][k] observation[ i * scale_x : (i + 1) * scale_x, j * scale_y : (j + 1) * scale_y, k, ] = pixel_value return (255 * observation).astype(np.uint8) def render(self, mode=&quot;human&quot;, close=False): if close: if self.viewer is not None: self.viewer.close() self.viewer = None return img = self.gridarray_to_image() if mode == &quot;rgb_array&quot;: return img elif mode == &quot;human&quot;: from gym.envs.classic_control import rendering if self.viewer is None: self.viewer = rendering.SimpleImageViewer() self.viewer.imshow(img) def close(self): self.render(close=True) @staticmethod def get_action_meanings(): return [&quot;NOOP&quot;, &quot;DOWN&quot;, &quot;UP&quot;, &quot;LEFT&quot;, &quot;RIGHT&quot;] if __name__ == &quot;__main__&quot;: env = GridworldEnv() obs = env.reset() done = False step_num = 1 # Run one episode while not done: # Sample a random action from the action space action = env.action_space.sample() next_obs, reward, done, info = env.step(action) print(f&quot;step#:{step_num} reward:{reward} done:{done} info:{info}&quot;) step_num += 1 env.render() env.close() . . Writing gridworld.py . #collapse-hide from gym.envs.registration import register register( id=&quot;Gridworld-v0&quot;, entry_point=&quot;gridworld:GridworldEnv&quot;, ) . . Agent . The agent is supposed to perform actions $a_{t}$ given the current environment state $s_{t-1}$. In return, the environment return a reward to the agent $r_t$. The agent is supposed to use the reward to learn from its previous mistakes, for simplicity the agent we will be building does not learn but still uses a Neural Network to choose it&#39;s actions. . . We start with the Neural Network model that the agent will use to selet its actions. In this case, the model is implemented in the following Brain class . class Brain(keras.Model): def __init__(self, action_dim=5, input_shape=(1, 8 * 8), hidden_dim=32): super(Brain, self).__init__() self.proj = layers.Dense(hidden_dim, input_shape=input_shape, activation=&quot;relu&quot;) self.logits = layers.Dense(action_dim) def call(self, inputs): x = tf.convert_to_tensor(inputs) if len(x.shape) &gt;= 2 and x.shape[0] != 1: x = tf.reshape(x, (1, -1)) return self.logits(self.proj(x)) . The model is very basic as you can see in the following summary, it consists of one hiddent layer and outputs one unit for every possible action. . env = gym.make(&#39;Gridworld-v0&#39;) action_dim, input_shape = env.action_space.n, env.observation_space.shape model = Brain(action_dim, input_shape) model.build(input_shape) model.summary() . Model: &#34;brain_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) multiple 2080 dense_5 (Dense) multiple 165 ================================================================= Total params: 2,245 Trainable params: 2,245 Non-trainable params: 0 _________________________________________________________________ . The agent is implemented in the following Agent class. It uses the model that we build earlier to take actions. Using tf.random.categorical the agent randomly picks an action from the model outputs. . class Agent(object): def __init__(self, brain): self.brain = brain def policy(self, observations): observations = observations.reshape(1, -1) action_logits = self.brain.predict_on_batch(observations) action = tf.random.categorical(tf.math.log(action_logits), num_samples=1) return tf.squeeze(action, axis=1) def get_action(self, observations): return self.policy(observations) . Environment . The environement used to test our agent is the GridWorld. We will run multiple episode where the agent will use its Neural Network model to take actions and we accumulate the rewards returned by the environment. At the end of each episode we print the total rewards and number of steps/actions taken during this eposide. . def run_episode(agent, env): obs, episode_reward, done, step_num = env.reset(), 0.0, False, 0 while not done: action = agent.get_action(obs) obs, reward, done, info = env.step(action) episode_reward += reward step_num += 1 env.render() return step_num, episode_reward, done, info . Here we setup the environemnt and record the actions taken and disply each eposide. . env = gnwrapper.LoopAnimation(gym.make(&#39;Gridworld-v0&#39;)) action_dim, input_shape = env.action_space.n, env.observation_space.shape brain = Brain(action_dim, input_shape) agent = Agent(brain) for episode in range(1): steps, episode_reward, done, info = run_episode(agent, env) print(f&quot;EpReward:{episode_reward:.2f} steps:{steps} done:{done} info:{info}&quot;) env.reset() env.display() . EpReward:-4.50 steps:100 done:True info:{&#39;status&#39;: &#39;Max steps reached&#39;} . &lt;/input&gt; Once Loop Reflect That&#39;s all folks . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/reinforcement/2021/07/23/Build_a_basic_neural_agent.html",
            "relUrl": "/tensorflow/reinforcement/2021/07/23/Build_a_basic_neural_agent.html",
            "date": " • Jul 23, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Get started with OpenAI Gym",
            "content": "This article walks through how to get started quickly with OpenAI Gym environment which is a platform for training RL agents. Later, we will use Gym to test intelligent agents implemented with TensorFlow. . To fully install OpenAI Gym and be able to use it on a notebook environment like Google Colaboratory we need to install a set of dependencies: . xvfb an X11 display server that will let us render Gym environemnts on Notebook | gym (atari) the Gym environment for Arcade games | atari-py is an interface for Arcade Environment. We will use it to load Atari games&#39; Roms into Gym | gym-notebook-wrapper A rendering helper that we will use to display OpenAI Gym games a Notebook . Note: atari-py was depreacated and is replaced with ale-py. However we can still use it. | . %%capture %%bash apt install xvfb pip install gym[atari] pip install gym-notebook-wrapper pip install atari-py . After installation we can check if Gym was installed properly and list names of all available environments sorted alphabetically: . from gym import envs env_names = [spec.id for spec in envs.registry.all()] for name in sorted(env_names[:10]): print(name) . CartPole-v0 CartPole-v1 Copy-v0 DuplicatedInput-v0 MountainCar-v0 MountainCarContinuous-v0 RepeatCopy-v0 Reverse-v0 ReversedAddition-v0 ReversedAddition3-v0 . Next, we need to install Atari Arcade ROMs so that we could load those games into Gym. . We need to download the Roms.rar file that contains the games | We load the Roms to make them accessible to Gym | %%capture %%bash curl -O http://www.atarimania.com/roms/Roms.rar mkdir roms yes | unrar e Roms.rar roms/ python -m atari_py.import_roms roms/ . Now, we are ready to play with Gym using one of the available games (e.g. Alien-v4). We will start the display server, then for multiple times we execute a sampled actions for our agent and check the result. If the agent dies we start a new episode. . %%bash rm -rf game/* mkdir -p game . import gnwrapper import gym # Start the display server env = gnwrapper.Monitor(gym.make(&#39;Alien-v4&#39;), directory=&quot;./game&quot;) o = env.reset() # Take 1000 actions by randomly sampling from the action space for _ in range(1000): action = env.action_space.sample() observation, reward, done, info = env.step(action) if done: env.reset() # display saved display images as movies env.display() . &#39;openaigym.video.0.1166.video000000.mp4&#39; . &#39;openaigym.video.0.1166.video000001.mp4&#39; . Notice that there are more then one displayed video. This is because when the episode finishes (i.e. agent dies) we reset the environment with env.reset() to start a new episode. i.e. each video displayed corresponds to one episode in the game. . The followig explains the variables returned as part of the result of env.step(action) in the previous script: . observation (Object): Observation returned by the environment. The object could be the RGB pixel data from the screen/camera, RAM contents, join angles and join velocities of a robot, and so on, depending on the environment. | reward (Float): Reward for the previous action that was sent to the environment. The range of the Float value varies with each environment, but irrespective of the environment, a higher reward is always better and the goal of the agent should be to maximize the total reward. | done (Boolean): Indicates whether the environment is going to be reset in the next step. When the Boolean value is true, it most likely means that the episode has ended (due to loss of like of the agent, timeout, or some other episode termination criteria). | info (Dict): Some additional information that can optionally be sent out by an environment as a dictionary of arbitrary key-value pairs. The agent we develop should not rely on any of the information in this dictionary for taking action. It may be used (if available) for debugging purposes. | . Here are some links I found useful: . Run and Render OpenAI Gym on Google Colab (Gym-Notebook-Wrapper) - link | T81-558: Applications of Deep Neural Networks - link | . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/reinforcement/2021/07/20/Get_started_with_OpenAI_Gym.html",
            "relUrl": "/tensorflow/reinforcement/2021/07/20/Get_started_with_OpenAI_Gym.html",
            "date": " • Jul 20, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Effortless video querying with TF Hub",
            "content": "Combining vision and language usually leads to better performance whatever is the task. In this article, we will see how we can leaverage a pre-trained model to search videos by activity. . We will use a model called Multiple Instance Learning (MIL) and Noise Contrastive Estimation (NCE) or simply MIL-NCE from TensorFlow Hub. This model was trained on the HowTo100M dataset which is a large-scale dataset of narrated 136M video clips where content creators explain the tasks being performed in the video. . This model can generate embeddings for video and text, we will use this capability to perform retrieval of the best video matching a query by calculating the distance between the two embeddings. . To learn more about the model and the general task it was trained for check the original paper here - arxiv.org . . Let&#39;s start but getting the dependencies . import math import uuid import cv2 import numpy as np from pathlib import Path import tensorflow as tf import tensorflow_hub as tfhub from google.colab.patches import cv2_imshow from IPython.display import Image . We need a function to download a video for a given url, store it locally and give it a random file name, we will use this later to get the test videos . def fetch_video(video_url): extension = Path(video_url).suffix file_name = str(uuid.uuid4()) + extension path = tf.keras.utils.get_file(file_name, video_url, cache_dir=&#39;.&#39;, cache_subdir=&#39;.&#39;) return path . Next, we define a function that will crop the frames to a square selected to be in the middle of the frame: . def get_center_square_coordinates(height, width): dimension = min(width, height) x_start = (width // 2) - (dimension // 2) x_end = x_start + dimension y_start = (height // 2) - (dimension // 2) y_end = y_start + dimension return x_start, y_start, x_end, y_end def crop_center(in_frame): height, width = in_frame.shape[:2] x_start, y_start, x_end, y_end = get_center_square_coordinates(height, width) out_frame = in_frame[y_start:y_end, x_start:x_end] return out_frame . Next, we define a set of helper functions to read a video file and extract at most 32 frames (if video has fewer frames we repeat some) . def extract_frames(video_path, max_frames, resize=(224, 224)): &quot;&quot;&quot;Extract at most max_frames from the video&quot;&quot;&quot; capture = cv2.VideoCapture(video_path) frames = [] while len(frames) &lt;= max_frames: frame_read, frame = capture.read() if not frame_read: break frame = crop_center(frame) frame = cv2.resize(frame, resize) frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) frames.append(frame) capture.release() return np.array(frames) def repeat_frames(in_frames, max_frames): &quot;&quot;&quot;Repeat frames until reaching a length of max_frames&quot;&quot;&quot; if len(in_frames) &gt;= max_frames: return in_frames repetitions = math.ceil(float(max_frames) / len(in_frames)) repetitions = int(repetitions) out_frames = in_frames.repeat(repetitions, axis=0) return out_frames def read_video(video_path, max_frames=32, resize=(224, 224)): # read frame and extract its frames frames = extract_frames(video_path, max_frames, resize) # make sure we have max_frames frames = repeat_frames(frames, max_frames) # select only max_frames frames = frames[:max_frames] return frames / 255.0 . For the test, we will use random GIF files . URLS = [ &#39;https://media.giphy.com/media/Wrm9ZTb7LFMcESBA4i/giphy.gif&#39;, &#39;https://media.giphy.com/media/Bom5hTsAsI8sFnH68k/giphy.gif&#39;, &#39;https://media.giphy.com/media/2aLiVCqTZmxwXeRfMh/giphy.gif&#39;, &#39;https://media.giphy.com/media/ngzhAbaGP1ovS/giphy.gif&#39;] . Image(url=URLS[-1]) . Use the previsous helper functions to download the videos and extract the frames . VIDEOS = [read_video(fetch_video(url)) for url in URLS] . Downloading data from https://media.giphy.com/media/Wrm9ZTb7LFMcESBA4i/giphy.gif 7331840/7331587 [==============================] - 0s 0us/step 7340032/7331587 [==============================] - 0s 0us/step Downloading data from https://media.giphy.com/media/Bom5hTsAsI8sFnH68k/giphy.gif 5185536/5181685 [==============================] - 0s 0us/step 5193728/5181685 [==============================] - 0s 0us/step Downloading data from https://media.giphy.com/media/2aLiVCqTZmxwXeRfMh/giphy.gif 23699456/23697427 [==============================] - 0s 0us/step 23707648/23697427 [==============================] - 0s 0us/step Downloading data from https://media.giphy.com/media/ngzhAbaGP1ovS/giphy.gif 1130496/1126589 [==============================] - 0s 0us/step 1138688/1126589 [==============================] - 0s 0us/step . For the search test we define those random queries . QUERIES = [&#39;biking&#39;, &#39;launching&#39;, &#39;skiing&#39;, &#39;skateboarding&#39;] . We load the pre-trained model and its weights from TF Hub which is available at https://tfhub.dev/deepmind/mil-nce/s3d/1 . model = tfhub.load(&#39;https://tfhub.dev/deepmind/mil-nce/s3d/1&#39;) . Next we define a helper function that will use the model to calcualte the embeddings of the input video . def get_video_embeddings(model, input_frames): frames = tf.cast(input_frames, dtype=tf.float32) frames = tf.constant(frames) video_model = model.signatures[&#39;video&#39;] video_embedding = video_model(frames) video_embedding = video_embedding[&#39;video_embedding&#39;] return video_embedding . Similarly, we also define a helper function that will use the model to calcualte the embeddings of the input text . def get_text_embeddings(model, input_words): words = tf.constant(input_words) text_model = model.signatures[&#39;text&#39;] text_embedding = text_model(words) text_embedding = text_embedding[&#39;text_embedding&#39;] return text_embedding . Now we calculate the embeddings for the text and video frames . video_emb = get_video_embeddings(model, np.stack(VIDEOS, axis=0)) text_emb = get_text_embeddings(model, np.array(QUERIES)) . We combine both embeddings to calculate a similarity scores that represents the distance between the two embeddings: . scores = np.dot(text_emb, tf.transpose(video_emb)) . To display the search result we need a radom frame that will represent each video, for instance the we can take the first frame . def get_one_frame(video): return video[0] def process_frame(frame): return cv2.cvtColor((frame * 255.0).astype(&#39;uint8&#39;), cv2.COLOR_RGB2BGR) first_frames = [process_frame(get_one_frame(v)) for v in VIDEOS] . This is a helper function that we will use to annotate the frmes with their respective distance score to the query . def annotate_frame(in_frame, text): out_frame = in_frame.copy() cv2.putText(out_frame, text, (8, 15), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.6, color=(255, 255, 255), thickness=2) return out_frame . Finally, we can perform the search. In this case we take the score of the first query biking and sort the frames based on their distance to the embeddings of the text (the higher the closer) and we annotate the frames with their position in the result and the calculated score. . query_scores = scores[0] sorted_results = sorted(list(zip(first_frames, query_scores)), key=lambda p: p[-1], reverse=True) annotated_frames = [] for i, (f, s) in enumerate(sorted_results, start=1): frame = annotate_frame(f.copy(), f&#39;#{i} - Score: {s:.2f}&#39;) annotated_frames.append(frame) cv2_imshow(np.hstack(annotated_frames)) . Notice how well the model was able to choose the moutain biking video as the best match for the biking query . We can do this again with another query, for instance skateboarding . query_scores = scores[-1] sorted_results = sorted(list(zip(first_frames, query_scores)), key=lambda p: p[-1], reverse=True) annotated_frames = [] for i, (f, s) in enumerate(sorted_results, start=1): frame = annotate_frame(f.copy(), f&#39;#{i} - Score: {s:.2f}&#39;) annotated_frames.append(frame) cv2_imshow(np.hstack(annotated_frames)) . Notice how this time the model did not pick the dog video as we would expected. But also the three first videos have a very close score to the query nevertheless. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/video/search/2021/07/17/MIL_NCE_TFHub.html",
            "relUrl": "/tensorflow/video/search/2021/07/17/MIL_NCE_TFHub.html",
            "date": " • Jul 17, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Effortless Instance segmentation with TF Hub",
            "content": "Instance Segmentation is a computer vision task that tries to identify all objects in an image by detecting their bounding boxes and also precisely segment each instance into its correct class. For instance, for an image with many persons, Instance Segmentation will separates each person as a single entity. . One of the very performant models in this task is Mask-RCNN which was built on Faster R-CNN. In addition to outputing predictions of bounding box recognition, Mask-RCNN also predicts object mask in parallel. Furthermore, Mask R-CNN can also be repurposed easily for other taskse.g., allowing us to estimate human poses. . . In this article, we will leverage TensorFlow Hub to use an implementation of Mask-RCNN that was pre-trained on the COCO dataset and perform out-of-the-box instance segmentation on custom images. . First, let&#39;s install the TensorFlow Object Detection API so we could use its visualization tools to draw the bounding boxes and segmentation masks: . %%capture %%bash git clone --depth 1 https://github.com/tensorflow/models cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install . . Let&#39;s import all needed packages . import glob from io import BytesIO import matplotlib.pyplot as plt import numpy as np from tqdm import tqdm import tensorflow as tf import tensorflow_hub as hub from PIL import Image from object_detection.utils import ops from object_detection.utils import visualization_utils as viz from object_detection.utils.label_map_util import create_category_index_from_labelmap . %matplotlib inline . Then, we define a helper function load images from disk into a NumPy . def load_image(path): image_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(image_data)) width, height = image.size shape = (1, height, width, 3) image = np.array(image.getdata()) image = image.reshape(shape).astype(&#39;uint8&#39;) return image . Next, we define a helper function that takes the predictions of the Mask-RCNN model and extract for each detected instance the bounding box and mask . def process_predictions(predictions, image_height, image_width): # convert predictions to NumPy model_output = {k: v.numpy() for k, v in predictions.items()} # extact masks from predictions detection_masks = model_output[&#39;detection_masks&#39;][0] detection_masks = tf.convert_to_tensor(detection_masks) # extact bounding boxes from predictions detection_boxes = model_output[&#39;detection_boxes&#39;][0] detection_boxes = tf.convert_to_tensor(detection_boxes) # Reframe box masks into appropriate image masks detection_masks_reframed = ops.reframe_box_masks_to_image_masks(detection_masks, detection_boxes, image_height, image_width) detection_masks_reframed = tf.cast(detection_masks_reframed &gt; 0.5, tf.uint8) model_output[&#39;detection_masks_reframed&#39;] = detection_masks_reframed.numpy() # extract bounding boxes, scores, classes, and masks boxes = model_output[&#39;detection_boxes&#39;][0] classes = model_output[&#39;detection_classes&#39;][0].astype(&#39;int&#39;) scores = model_output[&#39;detection_scores&#39;][0] masks = model_output[&#39;detection_masks_reframed&#39;] return boxes, classes, scores, masks . We then use the extracted bounding boxes and masks to draw them on the original image . def visualize_predictions(image, boxes, classes, scores, masks, CATEGORY_IDX): &quot;&quot;&quot;Visualize detections and bounding boxes, scores, classes, and masks&quot;&quot;&quot; image_with_mask = image.copy()[0] viz.visualize_boxes_and_labels_on_image_array( image=image_with_mask, boxes=boxes, classes=classes, scores=scores, category_index=CATEGORY_IDX, use_normalized_coordinates=True, max_boxes_to_draw=200, min_score_thresh=0.30, agnostic_mode=False, instance_masks=masks, line_thickness=5 ) return image_with_mask . To be able to map an class ID to a label, we need to load the original mapping from the COCO dataset which is available inte TensorFlow models package . labels_path = &#39;models/research/object_detection/data/mscoco_label_map.pbtxt&#39; CATEGORY_IDX = create_category_index_from_labelmap(labels_path) . Now, let&#39;s download the Mask-RCNN model and its weights from TF Hub available here https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1 . MODEL_PATH = &#39;https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1&#39; mask_rcnn = hub.load(MODEL_PATH) . Get some images for testing . %%bash mkdir -p images curl -s -o images/bicycle1.jpg https://cdn.pixabay.com/photo/2016/11/30/12/29/bicycle-1872682_960_720.jpg curl -s -o images/bicycle2.jpg https://cdn.pixabay.com/photo/2016/11/22/23/49/cyclists-1851269_960_720.jpg curl -s -o images/animal1.jpg https://cdn.pixabay.com/photo/2014/05/20/21/20/bird-349026_960_720.jpg curl -s -o images/animal2.jpg https://cdn.pixabay.com/photo/2018/05/27/18/19/sparrows-3434123_960_720.jpg curl -s -o images/car1.jpg https://cdn.pixabay.com/photo/2016/02/13/13/11/oldtimer-1197800_960_720.jpg curl -s -o images/car2.jpg https://cdn.pixabay.com/photo/2016/09/11/10/02/renault-juvaquatre-1661009_960_720.jpg . Finally, we iterate over each test image, run it over the Mask-RCNN model and draw the bounding boxes and masks . images = [] for image_path in tqdm(glob.glob(&#39;images/*&#39;)): # load image in_image = load_image(image_path) # make predictions with Mask-RCNN predictions = mask_rcnn(in_image) # process predictions boxes, classes, scores, masks = process_predictions(predictions, in_image.shape[1], in_image.shape[2]) # visualize boxes and masks out_image = visualize_predictions(in_image, boxes, classes, scores, masks, CATEGORY_IDX) images.append(out_image) . 100%|██████████| 6/6 [06:01&lt;00:00, 60.24s/it] . . Note: this may take some time especially calculating the predicitons by Mask-RCNN . Finally, we can display the output images and admire the performance of the Mask-RCNN . figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, image in enumerate(images): row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].axis(&#39;off&#39;) plt.tight_layout() plt.show() . As you can see from the predicted masks and bounding boxes, the model was able to do a good on our random images. . I hope you enjoyed this article, feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/segmentation/2021/06/28/Mask_RCNN_TFHub.html",
            "relUrl": "/tensorflow/vision/segmentation/2021/06/28/Mask_RCNN_TFHub.html",
            "date": " • Jun 28, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Image segmentation with U-Net and transfer learning",
            "content": "In this article, we will implement a U-Net model (as depicted in the diagram below) and trained on a popular image segmentation dataset. Training a U-Net from scratch is a hard, so instead we will leverage transfer learning to get good result after only few epochs of training. . For reference, you can read the original U-Net paper arxiv.org. . . Before start, import the needed dependencies . import cv2 import matplotlib.pyplot as plt import numpy as np from tqdm import tqdm import tensorflow as tf import tensorflow_datasets as tfdata from tensorflow.keras.applications import MobileNetV2 from tensorflow.keras.layers import * from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.keras.models import * from tensorflow.keras.optimizers import RMSprop . Data . We will use a simple segmentation dataset known as Oxford-IIIT Pet Dataset. For quick introduction the dataset contains images of dogs or cats along with a segmentation image. Each pixel of the segmentation belongs to one of the following classes: . 1: The pixel belongs to a pet (i.e. cat or dog). | 2: The pixel belongs to the contour of a pet. | 3: The pixel belongs to the surroundings. | . For detailed introduction to the dataset check its website here. . The dataset along with its metadata is available in tensorflow-datasets here with images already preprocessed and ready to use with the TensorFlow Data API. Let&#39;s download this dataset and cache it locally . dataset, info = tfdata.load(&#39;oxford_iiit_pet&#39;, with_info=True) . Downloading and preparing dataset oxford_iiit_pet/3.2.0 (download: 773.52 MiB, generated: 774.69 MiB, total: 1.51 GiB) to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0... Shuffling and writing examples to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0.incomplete3ZMH6C/oxford_iiit_pet-train.tfrecord Shuffling and writing examples to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0.incomplete3ZMH6C/oxford_iiit_pet-test.tfrecord Dataset oxford_iiit_pet downloaded and prepared to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0. Subsequent calls will reuse this data. . Let&#39;s examine few images and the ground truth labels from the dataset . figure, axis = plt.subplots(2, 2, figsize=(10, 8)) for row, example in enumerate(dataset[&#39;train&#39;].take(2)): file_name = example[&#39;file_name&#39;].numpy().decode(&#39;utf8&#39;) axis[row, 0].set_title(file_name) axis[row, 0].imshow(example[&#39;image&#39;].numpy()) axis[row, 0].axis(&#39;off&#39;) mask = np.squeeze(example[&#39;segmentation_mask&#39;].numpy(), axis=2) axis[row, 1].set_title(&#39;mask&#39;) axis[row, 1].imshow(mask, cmap=&#39;gray&#39;) axis[row, 1].axis(&#39;off&#39;) . Model . As you can see in from the U-Net model architecture, the model consists of an Encoder depicted by a contracting path (left side) and a Decoder depicted by an expansive path (right side). From the left side, some skip connections are passed to the right side in order to improve the performance of the decoding. . Because the Encoder is very similar to a convolutional network, instead of creating this part and training it form scratch we will use a pretrained model and just select the output layers with appropriate shape to make skip connection to the Decoder. This way, we can train the model faster as we will have only the upsaming path to train. . First, let&#39;s define a function that creates the Encoder. We will use a pretrained MobileNetV2 on ImageNet as backbone for the Encoder. The output of the Encoder consits of few cherry picked layers that we will use later to create the skip connection from this Encoder to the Decoder part of the model. . def create_down_path(input_size=(256, 256, 3)): &quot;&quot;&quot;Create down path of U-Net model&quot;&quot;&quot; backbone = MobileNetV2(input_shape=input_size, include_top=False, weights=&#39;imagenet&#39;) target_layers = [ &#39;block_1_expand_relu&#39;, &#39;block_3_expand_relu&#39;, &#39;block_6_expand_relu&#39;, &#39;block_13_expand_relu&#39;, &#39;block_16_project&#39; ] layers = [backbone.get_layer(l).output for l in target_layers] encoder = Model(inputs=backbone.input, outputs=layers) encoder.trainable = False return encoder . . Note: with model.trainable = False we are freezing the weights of the Encoder as we don&#8217;t want to train it as it all the used layers were already trained on ImageNet. . Second, we define a function that creates the up-sampling path which consists of a sequence of blocks that uses the Conv2DTranspose layer. . def create_up_path(size=4, dropout=False): decoder = [] init = tf.random_normal_initializer(0.0, 0.02) for filters in (512, 256, 128, 64): block = Sequential() block.add(Conv2DTranspose(filters=filters, kernel_size=size, strides=2, padding=&#39;same&#39;, kernel_initializer=init, use_bias=False)) block.add(BatchNormalization()) if dropout: block.add(Dropout(rate=0.5)) block.add(ReLU()) decoder.append(block) return decoder . Finally, we use the previous functions to create the Encoder and Decoder components, and wire them with the skip connections. . def create_unet(input_size=(256, 256, 3)): down_stack = create_down_path(input_size) up_stack = create_up_path() # create skip connections inputs = Input(shape=input_size) x = inputs skip_layers = down_stack(x) x = skip_layers[-1] skip_layers = reversed(skip_layers[:-1]) for up, skip_connection in zip(up_stack, skip_layers): x = up(x) x = Concatenate()([x, skip_connection]) # output layer init = tf.random_normal_initializer(0.0, 0.02) output = Conv2DTranspose(filters=3, kernel_size=3, strides=2, padding=&#39;same&#39;, kernel_initializer=init)(x) return Model(inputs, outputs=output) . . Note: The intput of the U-Net model has same dimension as the input image, and as many channels as the number of segmentation classes (because each pixel can be categorized into one of 3 classes) . Now we can create the model and inspect it, notice how the output layers from the Encoder are used by the different blocks of the decoder. . unet_model = create_unet() . WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default. . WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default. . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5 9412608/9406464 [==============================] - 0s 0us/step 9420800/9406464 [==============================] - 0s 0us/step . tf.keras.utils.plot_model(unet_model, show_shapes=True) . Let&#39;s compile the model to get ready for training. We will use RMSProp as optimizer and SparseCategoricalCrossentropy as the loss function. . unet_model.compile(optimizer=RMSprop(), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) . Training . We need to normalize the images so they match the expected input of the pretrained MobileNetV2 model . def normalize(input_image, input_mask): input_image = tf.cast(input_image, tf.float32) / 255.0 input_mask -= 1 return input_image, input_mask . Let&#39;s define a function that will be used by the TF Dataset to load images and their labels, normalize the images and perform some basic augmentation (flip the image and mask) only for training. . @tf.function def load_image_fn(example, train=True): input_image = tf.image.resize(example[&#39;image&#39;],(256, 256)) input_mask = tf.image.resize(example[&#39;segmentation_mask&#39;], (256,256)) if train and np.random.uniform() &gt; 0.5: input_image = tf.image.flip_left_right(input_image) input_mask = tf.image.flip_left_right(input_mask) input_image, input_mask = normalize(input_image, input_mask) return input_image, input_mask . Let&#39;s define some training parameters like batch size, steps per epochs, etc. . TRAIN_SIZE = info.splits[&#39;train&#39;].num_examples VALIDATION_SIZE = info.splits[&#39;test&#39;].num_examples BATCH_SIZE = 64 STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE VALIDATION_SUBSPLITS = 5 VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE VALIDATION_STEPS //= VALIDATION_SUBSPLITS BUFFER_SIZE = 1000 . Now we create the training and test datasets and use the previously defined functions to load the images and labels. We also batch and perfom a pre-fetch for the training dataset . train_dataset = dataset[&#39;train&#39;] .map(load_image_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE) .cache() .shuffle(BUFFER_SIZE) .batch(BATCH_SIZE) .repeat() .prefetch(buffer_size=tf.data.experimental.AUTOTUNE) test_dataset = dataset[&#39;test&#39;] .map(lambda d: load_image_fn(d, train=False), num_parallel_calls=tf.data.experimental.AUTOTUNE) .batch(BATCH_SIZE) . Now we can start the training of our U-Net model . hist = unet_model.fit(train_dataset, epochs=10, steps_per_epoch=STEPS_PER_EPOCH, validation_steps=VALIDATION_STEPS, validation_data=test_dataset) . Epoch 1/10 57/57 [==============================] - 53s 582ms/step - loss: 0.4203 - accuracy: 0.8352 - val_loss: 2.0189 - val_accuracy: 0.6889 Epoch 2/10 57/57 [==============================] - 34s 571ms/step - loss: 0.2736 - accuracy: 0.8894 - val_loss: 0.4313 - val_accuracy: 0.8765 Epoch 3/10 57/57 [==============================] - 33s 576ms/step - loss: 0.2554 - accuracy: 0.8945 - val_loss: 0.2991 - val_accuracy: 0.8937 Epoch 4/10 57/57 [==============================] - 33s 585ms/step - loss: 0.2449 - accuracy: 0.8978 - val_loss: 0.2617 - val_accuracy: 0.9006 Epoch 5/10 57/57 [==============================] - 34s 592ms/step - loss: 0.2354 - accuracy: 0.9008 - val_loss: 0.3233 - val_accuracy: 0.8708 Epoch 6/10 57/57 [==============================] - 34s 599ms/step - loss: 0.2268 - accuracy: 0.9042 - val_loss: 0.2499 - val_accuracy: 0.9042 Epoch 7/10 57/57 [==============================] - 34s 606ms/step - loss: 0.2244 - accuracy: 0.9045 - val_loss: 0.2453 - val_accuracy: 0.9022 Epoch 8/10 57/57 [==============================] - 35s 607ms/step - loss: 0.2125 - accuracy: 0.9092 - val_loss: 0.2443 - val_accuracy: 0.9057 Epoch 9/10 57/57 [==============================] - 34s 606ms/step - loss: 0.2124 - accuracy: 0.9089 - val_loss: 0.2930 - val_accuracy: 0.8884 Epoch 10/10 57/57 [==============================] - 34s 606ms/step - loss: 0.2080 - accuracy: 0.9102 - val_loss: 0.2561 - val_accuracy: 0.9016 . MobileNets have many different input size configurations 128, 160, 192 and 224, in our case we are sizing the images into 256 which is not supported hence the warnings in the output. Thre is nothing to worry about the model is trained properly and validation is performed. . Now we can examine the losses and accuracy progress per epoch . figure, axis = plt.subplots(1, 2, figsize=(15, 5)) # accuracy axis[0].plot(hist.history[&#39;accuracy&#39;]) axis[0].plot(hist.history[&#39;val_accuracy&#39;]) axis[0].set_title(&#39;model accuracy&#39;) axis[0].set_ylabel(&#39;accuracy&#39;) axis[0].set_xlabel(&#39;epoch&#39;) axis[0].legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;upper left&#39;) # loss axis[1].plot(hist.history[&#39;loss&#39;]) axis[1].plot(hist.history[&#39;val_loss&#39;]) axis[1].set_title(&#39;model loss&#39;) axis[1].set_ylabel(&#39;loss&#39;) axis[1].set_xlabel(&#39;epoch&#39;) axis[1].legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;upper left&#39;) plt.show() . Evaluation . We can evaluate our U-Net model on the entire test dataset simply as follows . result = unet_model.evaluate(test_dataset) . 58/58 [==============================] - 16s 277ms/step - loss: 0.2512 - accuracy: 0.9024 Accuracy: 90.24% . print(f&#39;Test Accuracy: {result[1] * 100:.2f}%&#39;) . Test Accuracy: 90.24% . For a visual examination and an inspection of the precited segmentation, we need some helper function to transform the output of the trained U-Net model into an actual valid mask, hence the following functions . def process_mask(mask): mask = (mask.numpy() * 127.5).astype(&#39;uint8&#39;) mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB) return mask def create_mask(prediction_mask): prediction_mask = tf.argmax(prediction_mask, axis=-1) prediction_mask = prediction_mask[..., tf.newaxis] return prediction_mask[0] . From the test dataset we take sample images and pass them through the U-Net model to generate predictions that later we will compare against the ground truth masks. . def generate_predictions(model, dataset, sample_size=2): output = [] for image, mask in tqdm(dataset.take(sample_size)): output_mask = model.predict(image) predicted_mask = process_mask(create_mask(output_mask)) image = (image[0].numpy() * 255.0).astype(&#39;uint8&#39;) ground_truth_mask = process_mask(mask[0]) output.append((image, ground_truth_mask, predicted_mask)) return output . The following helper function will be used to plot a set of predictions . def plot_predictions(predictions): figure, axis = plt.subplots(len(predictions), 3, figsize=(10, 20)) for row, (image, ground_truth_mask, predicted_mask) in enumerate(predictions): # plot the image axis[row, 0].imshow(image) axis[row, 0].axis(&#39;off&#39;) axis[row, 0].set_title(&#39;image&#39;) # plot the ground truth mask axis[row, 1].imshow(ground_truth_mask) axis[row, 1].axis(&#39;off&#39;) axis[row, 1].set_title(&#39;ground truth mask&#39;) # plot the predicted mask axis[row, 2].imshow(predicted_mask) axis[row, 2].axis(&#39;off&#39;) axis[row, 2].set_title(&#39;predicted mask&#39;) plt.tight_layout() plt.show() . Now we can generate predicted masks for a sample of the images in the test set and plot them for inspection . predictions = generate_predictions(unet_model, test_dataset, 5) . 100%|██████████| 5/5 [00:03&lt;00:00, 1.33it/s] . plot_predictions(predictions) . As you can see from the predicted masks, the model was able to do a good job even if it was trained on very few epochs. It did very well on the easy example but could not do a good job in the harder exmaples. For instance on darker images or when the image contains more than just a pet. . I put lot efforts in writting every article, I hope you found this one useful as well as easy to digest. . Feel free to leave a comment or reach out on twitter @bachiirc .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/segmentation/2021/06/13/U_Net_transfer_learning.html",
            "relUrl": "/tensorflow/vision/segmentation/2021/06/13/U_Net_transfer_learning.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Image Captioning with Attention",
            "content": ". In this article, we will implement a more complete image captioning system on the Flickr8k dataset. We will use an attention mechanism to give the model the power to search for parts of the source caption that are relevant to predict the best next word. Also, using attention will allow us to understand in an intuitive way where the network looks to produce captions. . Let&#39;s start by importing the needed dependencies: . import pandas as pd import numpy as np import json import os import time from string import punctuation from pathlib import Path import matplotlib.pyplot as plt from tqdm import tqdm from sklearn.model_selection import train_test_split from sklearn.utils import shuffle import tensorflow as tf from tensorflow.keras.applications.inception_v3 import * from tensorflow.keras.layers import * from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.utils import get_file . Set a seed for reproducibility . SEED = 31 . Data . We will use the Flickr8k dataset (availble on Kaggle here). So we wil install Kaggle CLI and make sure the credentials are properly configured. . %%capture %%bash pip install kaggle --upgrade mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;dzlabs&quot;,&quot;key&quot;:&quot;0abda977ffcfb11ea3726d7c0a6a802e&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json . . Note: you need to replace KAGGLE_USER with your actual Kaggle username and KAGGLE_KEY with your API key for the download to work. . Download the dataset, unzip the files into, and create a proper folder structure . %%capture %%bash kaggle datasets download adityajn105/flickr8k mkdir -p flickr8k unzip flickr8k.zip -d flickr8k mkdir -p flickr8k/features . Now we can set variables with the paths to the images and annotations . BASE_PATH = &#39;flickr8k&#39; IMAGES_PATH = f&#39;{BASE_PATH}/Images&#39; FEATURES_PATH = f&#39;{BASE_PATH}/features&#39; CAPTIONS_PATH = f&#39;{BASE_PATH}/captions.txt&#39; . Let&#39;s read the captions file into a Pandas dataframe and have look to it . captions_df = pd.read_csv(CAPTIONS_PATH) # captions_df = captions_df.groupby(&#39;image&#39;).first().reset_index() captions_df.head() . image caption . 0 1000268201_693b08cb0e.jpg | A child in a pink dress is climbing up a set o... | . 1 1000268201_693b08cb0e.jpg | A girl going into a wooden building . | . 2 1000268201_693b08cb0e.jpg | A little girl climbing into a wooden playhouse . | . 3 1000268201_693b08cb0e.jpg | A little girl climbing the stairs to her playh... | . 4 1000268201_693b08cb0e.jpg | A little girl in a pink dress going into a woo... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; With the images and captions loaded, we can take a random sample and dispaly some images with their respective caption . samples = captions_df.sample(6).reset_index() figure, axis = plt.subplots(2, 3, figsize=(18, 8)) for index, sample in samples.iterrows(): image = plt.imread(f&#39;{IMAGES_PATH}/{sample[&quot;image&quot;]}&#39;) title = sample[&#39;caption&#39;][:50] + &#39; n&#39; + sample[&#39;caption&#39;][50:] row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].set_title(title) axis[row, col].axis(&#39;off&#39;) . We need to clean the text in captions (e.g. removing punctuation) to simplify training, and also adding special tokens &lt;sos&gt; (start of sequence) token to be added at the begning of the text, and &lt;eos&gt; (end of sequence) token added at the end. . def clean_caption(caption, start_token=&#39;&lt;sos&gt;&#39;, end_token=&#39;&lt;eos&gt;&#39;): def remove_punctuation(word): translation = str.maketrans(&#39;&#39;, &#39;&#39;, punctuation) return word.translate(translation) def is_valid_word(word): return len(word) &gt; 1 and word.isalpha() caption = caption.lower().split(&#39; &#39;) caption = map(remove_punctuation, caption) caption = filter(is_valid_word, caption) cleaned_caption = f&#39;{start_token} {&quot; &quot;.join(caption)} {end_token}&#39; return cleaned_caption . Now we apply the cleaning function and update all captions text . captions_df[&#39;caption&#39;] = captions_df.caption.apply(lambda x: clean_caption(x)) . Here we define a helper function that we will use later to get the maximum sequence length . def get_max_length(tensor): return max(len(t) for t in tensor) . Model . The model that we will be using for caption as illustrated by the diagram below is composed of three smaller models: . Image feature extraction which is simply a CNN previously trained on image classification but without the classification head. The weights of this model are non-trainable. | CNN Encoder which takes the image features and produces an embedding that will be learded as the model is trained | RNN Decoder with Attention which will uses the image embedding as well as the hidden state propagated as the decoder process the caption tokens | . . The first part of the model is the CNN Encoder that will train an dense layer as it processes images feature vector and generates what will be used by the Decoder attention layer . class CNNEncoder(Model): def __init__(self, embedding_dim): super(CNNEncoder, self).__init__() self.fc = Dense(embedding_dim) def call(self, x): x = self.fc(x) x = tf.nn.relu(x) return x . The next component of our model is based on Bahdanau&#39;s Attention which was a break throught when it was first introduced as an improvemrnt to Encoder-Decoder models. It tries to address the problem that encoder faces as they try to squash information extracted from very long sequences. . To learn more about this algorithm can read the original paper arxiv.org or read a detailed explanation on machinelearningmastery.com . . class BahdanauAttention(Model): def __init__(self, units): super(BahdanauAttention, self).__init__() self.W1 = Dense(units) self.W2 = Dense(units) self.V = Dense(1) def call(self, features, hidden): hidden_with_time_axis = tf.expand_dims(hidden, 1) score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) attention_w = tf.nn.softmax(self.V(score), axis=1) ctx_vector = attention_w * features ctx_vector = tf.reduce_sum(ctx_vector, axis=1) return ctx_vector, attention_w . Next, we define the decoder whichi is an RNN that uses GRU and attention to learn how to produce captions from the text input sequences and the visual feature vectors extracted from the input images: . class RNNDecoder(Model): def __init__(self, embedding_size, units, vocab_size): super(RNNDecoder, self).__init__() self.units = units self.embedding = Embedding(vocab_size, embedding_size) self.gru = GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer=&#39;glorot_uniform&#39;) self.fc1 = Dense(self.units) self.fc2 = Dense(vocab_size) self.attention = BahdanauAttention(self.units) def call(self, x, features, hidden): # calcualte the attention context_vector, attention_weights = self.attention(features, hidden) # calculate the embeddings of the input token x = self.embedding(x) expanded_context = tf.expand_dims(context_vector, 1) x = Concatenate(axis=-1)([expanded_context, x]) # pass context vector and input embedding through GRU output, state = self.gru(x) x = self.fc1(output) x = tf.reshape(x, (-1, x.shape[2])) x = self.fc2(x) return x, state, attention_weights def reset_state(self, batch_size): return tf.zeros((batch_size, self.units)) . Finally, we put the different pieces of model togher in the Trainer Class. It will create the encoder, the decoder, the tokenizer, and the optimizer and loss functions needed to train the whole system. This class also defines a function to perform a single training step as well as the training on many epoches. During the training, the training loss will be recorded which later can be visualized with TensorBoard. . class Trainer(object): def __init__(self, embedding_size, units, vocab_size, tokenizer): self.tokenizer = tokenizer self.encoder = CNNEncoder(embedding_size) self.decoder = RNNDecoder(embedding_size, units, vocab_size) self.optimizer = Adam() self.loss = SparseCategoricalCrossentropy(from_logits=True, reduction=&#39;none&#39;) def loss_function(self, real, predicted): &quot;&quot;&quot;Calculate the loss based on the ground truth caption and the predicted one&quot;&quot;&quot; mask = tf.math.logical_not(tf.math.equal(real,0)) _loss = self.loss(real, predicted) mask = tf.cast(mask, dtype=_loss.dtype) _loss *= mask return tf.reduce_mean(_loss) @tf.function def train_step(self, image_tensor, target): &quot;&quot;&quot;Perform one training step&quot;&quot;&quot; loss = 0 hidden = self.decoder.reset_state(target.shape[0]) start_token_idx = self.tokenizer.word_index[&#39;&lt;sos&gt;&#39;] init_batch = [start_token_idx] * target.shape[0] decoder_input = tf.expand_dims(init_batch, 1) with tf.GradientTape() as tape: features = self.encoder(image_tensor) for i in range(1, target.shape[1]): preds, hidden, _ = self.decoder(decoder_input, features, hidden) loss += self.loss_function(target[:, i], preds) decoder_input = tf.expand_dims(target[:, i],1) total_loss = loss / int(target.shape[1]) trainable_vars = (self.encoder.trainable_variables + self.decoder.trainable_variables) gradients = tape.gradient(loss, trainable_vars) self.optimizer.apply_gradients(zip(gradients,trainable_vars)) return loss, total_loss def train(self, dataset, epochs, num_steps): &quot;&quot;&quot;Train and log metrics&quot;&quot;&quot; writer = tf.summary.create_file_writer(&#39;log_dir&#39;) for epoch in tqdm(range(epochs)): start = time.time() total_loss = 0 for batch, (image_tensor, target) in enumerate(dataset): batch_loss, step_loss = self.train_step(image_tensor, target) total_loss += step_loss epoch_time = time.time() - start # write the loss value with writer.as_default(): tf.summary.scalar(&#39;training loss&#39;, total_loss / num_steps, step=epoch+1) tf.summary.scalar(&#39;Epoch time (s)&#39;, epoch_time, step=epoch+1) . Training . Before we can start the training, we need to perform some data pre-process. Let&#39;s first get an array of image paths and the corresponding captions . train_images = captions_df.image.apply(lambda image: f&#39;{IMAGES_PATH}/{image}&#39;).values train_captions = captions_df.caption.values . We need to download a pre-trained instance of Inception V3 with the Imagenet dataset and used as the image feature extractor after removing the model classification head . feature_extractor = InceptionV3(include_top=False, weights=&#39;imagenet&#39;) feature_extractor = Model(feature_extractor.input, feature_extractor.layers[-1].output) . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 87916544/87910968 [==============================] - 1s 0us/step 87924736/87910968 [==============================] - 1s 0us/step . We need to define a function that will load images based on their path and resize them as expected by the feature extractor model . def load_image_fn(image_path): image = tf.io.read_file(image_path) image = tf.image.decode_jpeg(image, channels=3) image = tf.image.resize(image, (299, 299)) image = preprocess_input(image) return image, image_path . We can use the previous funciton to Create a tf.data.Dataset of the images . BATCH_SIZE = 8 image_dataset = tf.data.Dataset.from_tensor_slices(train_images) .map(load_image_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE) .batch(BATCH_SIZE) . We iteratate over all the images in the dataset and pass them through the feature extractor. As the extracted feature vectors cannot fit in memory we store them under FEATURES_PATH folder . for image, path in tqdm(image_dataset): batch_features = feature_extractor.predict(image) batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3])) for batch_feature, p in zip(batch_features, path): feature_path = Path(p.numpy().decode(&#39;UTF-8&#39;)) image_name = feature_path.stem np.save(f&#39;{FEATURES_PATH}/{image_name}&#39;, batch_feature.numpy()) . 100%|██████████| 5057/5057 [11:51&lt;00:00, 7.11it/s] . We need to create tokens from the captions, hence we train a tokenizer on the top 5,000 words in our captions which will become our vocabulary. Note we could take more words but that lead to bigger memory footprint as we one hot encode each token. . After that, we apply this tokenizer to each caption text to generate a numeric sequence. We limit the sequence size and we pad any short caption by adding a sequence of the special &lt;pad&gt; token to the end. . top_k = 5000 filters = &#39;!”#$%&amp;()*+.,-/:;=?@[ ]^_`{|}~ &#39; tokenizer = Tokenizer(num_words=top_k, oov_token=&#39;&lt;unk&gt;&#39;, filters=filters) tokenizer.fit_on_texts(train_captions) tokenizer.word_index[&#39;&lt;pad&gt;&#39;] = 0 tokenizer.index_word[0] = &#39;&lt;pad&gt;&#39; train_seqs = tokenizer.texts_to_sequences(train_captions) captions_seqs = pad_sequences(train_seqs, padding=&#39;post&#39;) max_length = get_max_length(train_seqs) . captions_seqs.shape . (40455, 34) . Let&#39;s split the dataset into 80% for the actual training and 20% for later evaluating the trained model . (images_train, images_val, caption_train, caption_val) = train_test_split(train_images, captions_seqs, test_size=0.2, random_state=SEED) . We need a function that will load an image feature vector and the associated caption . def load_example_fn(image_name, caption): image_name = image_name.decode(&#39;utf-8&#39;) image_name = Path(image_name).stem image_tensor = np.load(f&#39;{FEATURES_PATH}/{image_name}.npy&#39;) return image_tensor, caption . To create the trainig dataset, we batch the images with their captions into batches of BATCH_SIZE example. We use the load_example_fn load the feature vectors. For performance, we suffle the dataset and pre-fetch some of them into the GPU to speedup training. . BATCH_SIZE = 64 BUFFER_SIZE = 1000 dataset = tf.data.Dataset.from_tensor_slices((images_train, caption_train)) .map(lambda i1, i2: tf.numpy_function(load_example_fn, [i1, i2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE) .shuffle(BUFFER_SIZE) .batch(BATCH_SIZE) .prefetch(buffer_size=tf.data.experimental.AUTOTUNE) . Now, we are ready for the actually training. Let&#39;s create the helper Trainer and set the token embeddings size to 256 elements, and the number of units for the decoder as well as the attention model to 512. We also pass the tokenizer and the vocabulary size is 5000 + 1 (for the padding token). . trainer = Trainer(embedding_size=256, units=512, vocab_size=top_k + 1, tokenizer=tokenizer) . Now we train for few epochs (note this may take quite some time to finish) . EPOCHS = 30 num_steps = len(images_train) // BATCH_SIZE trainer.train(dataset, EPOCHS, num_steps) . 100%|██████████| 30/30 [2:00:15&lt;00:00, 240.52s/it] . Now we can explore the training loss with . %load_ext tensorboard %tensorboard --logdir log_dir . . Evaluation . For the evaluation, we will define a function that takes the image feature extractor and the trained encoder, decoder. It will transform the image and generate a caption starting with the &lt;sos&gt; spectial token. The caption generation stops when and &lt;eos&gt; token is generated by the decoder or the maximum length of caption tokens is reached. . def evaluate(encoder, decoder, tokenizer, image_path, max_length, attention_shape): attention_plot = np.zeros((max_length, attention_shape)) # initialize hidden state hidden = decoder.reset_state(batch_size=1) image, _ = load_image_fn(image_path) # extarct image feature vector features = feature_extractor(tf.expand_dims(image, 0)) features = tf.reshape(features, (features.shape[0], -1, features.shape[3])) # encode the features encoder_out = encoder(features) start_token_idx = tokenizer.word_index[&#39;&lt;sos&gt;&#39;] decoder_input = tf.expand_dims([start_token_idx], 0) result = [] # generate the caption for i in range(max_length): (preds, hidden, attention_w) = decoder(decoder_input, encoder_out, hidden) attention_plot[i] = tf.reshape(attention_w, (-1,)).numpy() pred_id = tf.random.categorical(preds, 1)[0][0].numpy() result.append(tokenizer.index_word[pred_id]) if tokenizer.index_word[pred_id] == &#39;&lt;eos&gt;&#39;: return result, attention_plot decoder_input = tf.expand_dims([pred_id], 0) attention_plot = attention_plot[:len(result), :] return result, attention_plot . . Note: see how for each token we generate an attention plot and added to the final attention_plot. . attention_features_shape = 64 . Pick a random image from the evaluation dataset that we saved earlier . random_id = np.random.randint(0, len(images_val)) image_path = images_val[random_id] . Get and clean the actual image caption . actual_caption = &#39; &#39;.join([tokenizer.index_word[i] for i in caption_val[random_id] if i != 0]) actual_caption = (actual_caption.replace(&#39;&lt;sos&gt;&#39;, &#39;&#39;).replace(&#39;&lt;eos&gt;&#39;, &#39;&#39;)) . Generate a caption for the image and clean it from special tokens . result, attention_plot = evaluate(trainer.encoder, trainer.decoder, tokenizer, image_path, max_length, attention_features_shape) predicted_caption = (&#39; &#39;.join(result).replace(&#39;&lt;sos&gt;&#39;, &#39;&#39;).replace(&#39;&lt;eos&gt;&#39;, &#39;&#39;)) . Let&#39;s show side by side the image with its original caption, and next to it two images overlayed with some of the attention plots (there is one attention plot per output token in the caption) and the predicted caption . figure, axis = plt.subplots(1, 3, figsize=(15, 8)) axis[0].imshow(plt.imread(image_path)) axis[0].set_title(actual_caption) axis[0].axis(&#39;off&#39;) imageshow = axis[1].imshow(plt.imread(image_path)) axis[1].imshow(np.resize(attention_plot[0], (8, 8)), cmap=&#39;gray&#39;, alpha=0.6, extent=imageshow.get_extent()) axis[1].set_title(predicted_caption) axis[1].axis(&#39;off&#39;) imageshow = axis[2].imshow(plt.imread(image_path)) axis[2].imshow(np.resize(attention_plot[len(attention_plot)-1], (8, 8)), cmap=&#39;gray&#39;, alpha=0.6, extent=imageshow.get_extent()) axis[2].set_title(predicted_caption) axis[2].axis(&#39;off&#39;) plt.show() . Notice how well the model performed and generated a caption that&#39;s close to the actual ground truth . We can display all attention plots and inspect what the model was looking into when it generated the corresponding token. For this, let&#39;s define a function that will receive an image, the caption as sequence of tokens, and the attention_plot returned by the previous evualtion function. . def plot_attention(image_path, result, attention_plot, output_path): image_array = plt.imread(image_path) fig = plt.figure(figsize=(10, 10)) # for each token create a sub-plot and display the corresponding attention for l in range(len(result)): temp_att = np.resize(attention_plot[l], (8, 8)) ax = fig.add_subplot(len(result) // 2, len(result) // 2, l + 1) ax.set_title(result[l]) image = ax.imshow(image_array) ax.imshow(temp_att, cmap=&#39;gray&#39;, alpha=0.6, extent=image.get_extent()) # save the attention plot plt.savefig(output_path, format=&#39;png&#39;) plt.tight_layout() plt.show() . plot_attention(image_path, result, attention_plot, &#39;./attention_plot.png&#39;) . . Note: the square areas in the plots represent the areas of the picture the model paid more attention to when generate the tokens. For instance, to produce the word women, the network looked at the head of women in the photo. Also, we can see that when the network generated the word beside as it looked at the chair. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/captioning/2021/06/01/Image_Captioning_with_Attention.html",
            "relUrl": "/tensorflow/vision/captioning/2021/06/01/Image_Captioning_with_Attention.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Super-Resolution with TensorFlow",
            "content": "Single image super-resolution (SR) is a classical computer vision problem that aims at recovering a high-resolution image from a lower resolution image. Extensive research was conduct in this area and with the advance of Deep Learning great results have been achieved. . In this post, we will examine one of the Deep Learning approaches to super-resolution called Super-Resolution Convolutional Neural Network (SRCNN). This technique work end to end by extacting patches from the low resolution image and passing them throw convolutional layers to final map them to higher resolution output pixels, as depicted in the diagram below. . We will implement the SRCNN model in TensorFlow, train it and then test it on a low resolution image. . . As an image dataset for training the model, we will be using a Kaggle hosted dataset called Dog and Cat Detection. We will use Kaggle CLI to download this dataset and you need to get your Kaggle API key, alternatively you can manually download the dataset directly from the website. . %%capture %%bash pip install -q kaggle mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;KAGGLE_USERNAME&quot;,&quot;key&quot;:&quot;KAGGLE_KEY&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json . %%capture %%bash kaggle datasets download andrewmvd/dog-and-cat-detection unzip -q dog-and-cat-detection.zip . To save model checkpoint and other artifacts, we will mount Google Drive the this colab container . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . Import dependencies . import os import pathlib from glob import glob from tqdm import tqdm import matplotlib.pyplot as plt import numpy as np from PIL import Image import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.layers import * from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.image import * . Set a random seed for reproducibility . SEED = 31 np.random.seed(SEED) . Data . We need a function to resize images based on a scale factor, this will be used later in the process to generate low resolution images from a given image . def resize_image(image_array, factor): original_image = Image.fromarray(image_array) new_size = np.array(original_image.size) * factor new_size = new_size.astype(np.int32) new_size = tuple(new_size) resized = original_image.resize(new_size) resized = img_to_array(resized) resized = resized.astype(np.uint8) return resized . This function will use the resizing to generate low resolution images by downsizing then upsizing: . def downsize_upsize_image(image, scale): scaled = resize_image(image, 1.0 / scale) scaled = resize_image(scaled, scale / 1.0) return scaled . When we will extract patches, we will slide a window over the original image, and for the image to fit nicely we need to crop it with the following function . def tight_crop_image(image, scale): height, width = image.shape[:2] width -= int(width % scale) height -= int(height % scale) return image[:height, :width] . The following function is used to extract patches with a sliding window from an input image. The INPUT_DIM parameter is the height and width of the images as expected by the network . def crop_input(image, x, y): y_slice = slice(y, y + INPUT_DIM) x_slice = slice(x, x + INPUT_DIM) return image[y_slice, x_slice] . Similarly, we need to crop patches from the output images with LABEL_SIZE the height and width of the output of the network. We also need to pad the patches with PAD to make sure we are cropping the regions properly . def crop_output(image, x, y): y_slice = slice(y + PAD, y + PAD + LABEL_SIZE) x_slice = slice(x + PAD, x + PAD + LABEL_SIZE) return image[y_slice, x_slice] . Now let&#39;s read all image paths . file_patten = (pathlib.Path(&#39;/content&#39;) / &#39;images&#39; / &#39;*.png&#39;) file_pattern = str(file_patten) dataset_paths = [*glob(file_pattern)] . We don&#39;t need the entire dataset as this will take longer training, but will sample around 1000 images from it . SUBSET_SIZE = 1000 dataset_paths = np.random.choice(dataset_paths, SUBSET_SIZE) . Here is an example image from the dataset . path = np.random.choice(dataset_paths) img = plt.imread(path) plt.imshow(img) . &lt;matplotlib.image.AxesImage at 0x7fb5cde796d0&gt; . Here we define some parameters, like the scale for resiping, input and output patch sizes, the amount of padding that need to be added to output patches, and the stride which is the number of pixels we&#39;ll slide both in the horizontal and vertical axes to extract patches. . SCALE = 2.0 INPUT_DIM = 33 LABEL_SIZE = 21 PAD = int((INPUT_DIM - LABEL_SIZE) / 2.0) STRIDE = 14 . Now, lets build the dataset by reading the input images, generating a low resolution version, sliding a window on this low resolution image as well as the original image to generate patches for training. We will save the patches to disk and later build a training data generator that will load them from disk in batches. . %%bash mkdir -p data mkdir -p training . for image_path in tqdm(dataset_paths): filename = pathlib.Path(image_path).stem image = load_img(image_path) image = img_to_array(image) image = image.astype(np.uint8) image = tight_crop_image(image, SCALE) scaled = downsize_upsize_image(image, SCALE) height, width = image.shape[:2] for y in range(0, height - INPUT_DIM + 1, STRIDE): for x in range(0, width - INPUT_DIM + 1, STRIDE): crop = crop_input(scaled, x, y) target = crop_output(image, x, y) np.save(f&#39;data/{filename}_{x}_{y}_input.np&#39;, crop) np.save(f&#39;data/{filename}_{x}_{y}_output.np&#39;, target) . 100%|██████████| 1500/1500 [18:00&lt;00:00, 1.39it/s] . We cannot hold all the patches in memory hence we saved to disk in the previous step. Now we need a dataset loader that will load a patch and its label and feed them to the network during traning in batches. This is achieved with the PatchesDataset class (check this example to learn more about generators - link). . class PatchesDataset(tf.keras.utils.Sequence): def __init__(self, batch_size, *args, **kwargs): self.batch_size = batch_size self.input = [*glob(&#39;data/*_input.np.npy&#39;)] self.output = [*glob(&#39;data/*_output.np.npy&#39;)] self.input.sort() self.output.sort() self.total_data = len(self.input) def __len__(self): # returns the number of batches return int(self.total_data / self.batch_size) def __getitem__(self, index): # returns one batch indices = self.random_indices() input = np.array([np.load(self.input[idx]) for idx in indices]) output = np.array([np.load(self.output[idx]) for idx in indices]) return input, output def random_indices(self): return np.random.choice(list(range(self.total_data)), self.batch_size, p=np.ones(self.total_data)/self.total_data) . Define a batch size based on how much memory available on your GPU and create an instance of the dataset generator. . BATCH_SIZE = 1024 train_ds = PatchesDataset(BATCH_SIZE) len(train_ds) . 888 . You can see the shape of the training batches . input, output = train_ds[0] input.shape, output.shape . ((1024, 33, 33, 3), (1024, 21, 21, 3)) . Model . The architecture of the SRCNN model is very simple, it has only convolutional layers, one to downsize the input and extract image features and a later one to upside to generate the output image. The following helper function is used to create an instance of the model. . def create_model(height, width, depth): input = Input(shape=(height, width, depth)) x = Conv2D(filters=64, kernel_size=(9, 9), kernel_initializer=&#39;he_normal&#39;)(input) x = ReLU()(x) x = Conv2D(filters=32, kernel_size=(1, 1), kernel_initializer=&#39;he_normal&#39;)(x) x = ReLU()(x) output = Conv2D(filters=depth, kernel_size=(5, 5), kernel_initializer=&#39;he_normal&#39;)(x) return Model(input, output) . To train the network we will use Adam as optimizer with learning rate decay. Also, as the problem we try to train the network for is a regression problem (we want predict the high resolution pixels) we pick MSE as a loss function, this will make the model learn the filters that correctly map patches from low to high resolution. . EPOCHS = 12 optimizer = Adam(learning_rate=1e-3, decay=1e-3 / EPOCHS) model = create_model(INPUT_DIM, INPUT_DIM, 3) model.compile(loss=&#39;mse&#39;, optimizer=optimizer) . You can see how the model is small but astonishly it will be able to achieve great results once trained for enough time, we will train it for 12 epochs . model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 33, 33, 3)] 0 conv2d (Conv2D) (None, 25, 25, 64) 15616 re_lu (ReLU) (None, 25, 25, 64) 0 conv2d_1 (Conv2D) (None, 25, 25, 32) 2080 re_lu_1 (ReLU) (None, 25, 25, 32) 0 conv2d_2 (Conv2D) (None, 21, 21, 3) 2403 ================================================================= Total params: 20,099 Trainable params: 20,099 Non-trainable params: 0 _________________________________________________________________ . tf.keras.utils.plot_model(model, show_shapes = True, rankdir=&#39;LR&#39;) . Create a callback that saves the model&#39;s weights . checkpoint_path = &quot;training/cp.ckpt&quot; checkpoint_dir = os.path.dirname(checkpoint_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1) . Now finally, we can train the network . model.fit(train_ds, epochs=EPOCHS, callbacks=[cp_callback]) . Epoch 1/12 888/888 [==============================] - ETA: 0s - loss: 258.0937 Epoch 00001: saving model to training/cp.ckpt 888/888 [==============================] - 1735s 2s/step - loss: 258.0937 Epoch 2/12 888/888 [==============================] - ETA: 0s - loss: 105.9775 Epoch 00002: saving model to training/cp.ckpt 888/888 [==============================] - 1428s 2s/step - loss: 105.9775 Epoch 3/12 888/888 [==============================] - ETA: 0s - loss: 102.4195 Epoch 00003: saving model to training/cp.ckpt 888/888 [==============================] - 1364s 2s/step - loss: 102.4195 Epoch 4/12 888/888 [==============================] - ETA: 0s - loss: 98.4859 Epoch 00004: saving model to training/cp.ckpt 888/888 [==============================] - 1347s 2s/step - loss: 98.4859 Epoch 5/12 888/888 [==============================] - ETA: 0s - loss: 97.5308 Epoch 00005: saving model to training/cp.ckpt 888/888 [==============================] - 1352s 2s/step - loss: 97.5308 Epoch 6/12 888/888 [==============================] - ETA: 0s - loss: 96.0889 Epoch 00006: saving model to training/cp.ckpt 888/888 [==============================] - 1347s 2s/step - loss: 96.0889 Epoch 7/12 888/888 [==============================] - ETA: 0s - loss: 94.7550 Epoch 00007: saving model to training/cp.ckpt 888/888 [==============================] - 1355s 2s/step - loss: 94.7550 Epoch 8/12 888/888 [==============================] - ETA: 0s - loss: 93.3618 Epoch 00008: saving model to training/cp.ckpt 888/888 [==============================] - 1332s 1s/step - loss: 93.3618 Epoch 9/12 888/888 [==============================] - ETA: 0s - loss: 93.5235 Epoch 00009: saving model to training/cp.ckpt 888/888 [==============================] - 1346s 2s/step - loss: 93.5235 Epoch 10/12 888/888 [==============================] - ETA: 0s - loss: 92.4781 Epoch 00010: saving model to training/cp.ckpt 888/888 [==============================] - 1356s 2s/step - loss: 92.4781 Epoch 11/12 888/888 [==============================] - ETA: 0s - loss: 91.5945 Epoch 00011: saving model to training/cp.ckpt 888/888 [==============================] - 1348s 2s/step - loss: 91.5945 Epoch 12/12 888/888 [==============================] - ETA: 0s - loss: 91.0127 Epoch 00012: saving model to training/cp.ckpt 888/888 [==============================] - 1336s 2s/step - loss: 91.0127 . &lt;keras.callbacks.History at 0x7fb5c9edecd0&gt; . make sure super_resolution folder exists in Google Drive . %%bash mkdir -p /content/drive/MyDrive/super_resolution cp -r training/* /content/drive/MyDrive/super_resolution . save and load the model . path = &#39;/content/drive/MyDrive/super_resolution/model.h5&#39; model.save(path) new_model = tf.keras.models.load_model(path) . Evaluation . After train the model for enough time we can evaluate it. Let&#39;s pick a random image from the dataset (or you can use anyother image) and transform it into a low resolution image that we can pass to the SRCNN model. . path = np.random.choice(dataset_paths) image = load_img(path) image = img_to_array(image) image = image.astype(np.uint8) image = tight_crop_image(image, SCALE) scaled = downsize_upsize_image(image, SCALE) . We need a placeholder where we will put the output patches to create the final image . output = np.zeros(scaled.shape) height, width = output.shape[:2] . Now we extarct patches from the input image, pass them through the trained model to generate high resolution patch and then put this patch in the right position on the previous placeholder. After processing every patch from the input image we will have a final output image . for y in range(0, height - INPUT_DIM + 1, LABEL_SIZE): for x in range(0, width - INPUT_DIM + 1, LABEL_SIZE): crop = crop_input(scaled, x, y) image_batch = np.expand_dims(crop, axis=0) prediction = model.predict(image_batch) new_shape = (LABEL_SIZE, LABEL_SIZE, 3) prediction = prediction.reshape(new_shape) output_y_slice = slice(y + PAD, y + PAD + LABEL_SIZE) output_x_slice = slice(x + PAD, x + PAD + LABEL_SIZE) output[output_y_slice, output_x_slice] = prediction . Now we can display side by side the low resolution image as well as the resulting output image which is of higher resolution. . figure, axis = plt.subplots(1, 2, figsize=(15, 8)) axis[0].imshow(np.array(scaled,np.int32)) axis[0].set_title(&#39;Low resolution image (Downsize + Upsize)&#39;) axis[0].axis(&#39;off&#39;) axis[1].imshow(np.array(output,np.int32)) axis[1].set_title(&#39;Super resolution result (SRCNN output)&#39;) axis[1].axis(&#39;off&#39;) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . (-0.5, 299.5, 197.5, -0.5) . Very impressive result considering the small model that we trained, as you can see it was able to considerably improve the resolution of the input image. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/generative/artistic/2021/05/10/Super_Resolution_SRCNN.html",
            "relUrl": "/tensorflow/generative/artistic/2021/05/10/Super_Resolution_SRCNN.html",
            "date": " • May 10, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Effortless Neural Style Transfer with TFHub",
            "content": "Neural style transfer was first introduced in Leon A. Gatys’ paper, A Neural Algorithm of Artistic Style. It&#39;s a very interesting use of Deep Learning to generate artistic images by mixing three images: a content image, a style reference image (e.g. some famous painter&#39;s artwork), and an input image that will be styled. The output result is an image that looks like the input image but but “painted” in the same style as the style image. . Implementing Neural Style Transfer from scratch can be challenging (see this tutorial for a Tensorflow implementation - link), but likely we can leverage TensorFlow Hub (TFHub) and use a pretrained implementation of Neural Style Transfer to style our own images. . . In this article, we will use a Neural Style Transfer module from TensorFlow Hub link that performs fast artistic style transfer and can work with arbitrary painting styles. . First, let&#39;s import the needed dependencies: . import glob from tqdm import tqdm import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from tensorflow_hub import load . Then, we need to define a function to load images into a TensorFlow tensor and do some processing (e.g. rescaling) . def load_image(image_path): dimension = 512 image = tf.io.read_file(image_path) image = tf.image.decode_jpeg(image, channels=3) image = tf.image.convert_image_dtype(image, tf.float32) shape = tf.cast(tf.shape(image)[:-1], tf.float32) longest_dimension = max(shape) scale = dimension / longest_dimension new_shape = tf.cast(shape * scale, tf.int32) image = tf.image.resize(image, new_shape) return image . Because, the output of the model is a tensor, we need to define a helper function to convert such tensor into a NumPy array so that we can display the image with matplotlib . def tensor_to_image(tensor): tensor = tensor * 255 tensor = np.array(tensor, dtype=np.uint8) if np.ndim(tensor) &gt; 3: tensor = tensor[0] return tensor . To easily display all the images at once, we define the following helper function . def show_images(images, num_rows, num_cols, figsize=(35, 15)): figure, axis = plt.subplots(num_rows, num_cols, figsize=figsize) for index, image in enumerate(images): row, col = int(index / num_cols), index % num_cols if num_rows == 1: axis[col].imshow(image) axis[col].axis(&#39;off&#39;) else: axis[row, col].imshow(image) axis[row, col].axis(&#39;off&#39;) . Let&#39;s download the Style Transfer module from Tensorflow Hub and load it . module_url = (&#39;https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2&#39;) hub_module = load(module_url) . Get some images for testing . %%bash rm -rf images mkdir -p images curl -s -o images/bicycle1.jpg https://cdn.pixabay.com/photo/2016/11/22/23/49/cyclists-1851269_960_720.jpg curl -s -o images/animal1.jpg https://cdn.pixabay.com/photo/2014/05/20/21/20/bird-349026_960_720.jpg curl -s -o images/car2.jpg https://cdn.pixabay.com/photo/2016/09/11/10/02/renault-juvaquatre-1661009_960_720.jpg . Load the content images and display them . images = [] image_paths = glob.glob(&#39;images/*&#39;) for path in image_paths: images.append(load_image(path)) . show_images(images, 1, 3, figsize=(35, 15)) . We also download some images to use for styling: . %%bash rm -rf styles mkdir -p styles curl -s -o styles/splashing.jpg https://cdn.pixabay.com/photo/2013/07/19/00/18/splashing-165192_960_720.jpg curl -s -o styles/fireball.jpg https://cdn.pixabay.com/photo/2014/08/20/18/44/fireball-422746_960_720.jpg curl -s -o styles/colorful.jpg https://cdn.pixabay.com/photo/2017/07/03/20/17/colorful-2468874_960_720.jpg curl -s -o styles/texture.jpg https://cdn.pixabay.com/photo/2017/08/09/04/53/texture-2613518_960_720.jpg . styles = [] style_paths = glob.glob(&#39;styles/*&#39;) for path in style_paths: styles.append(load_image(path)) . Let&#39;s have quick look at the different styles . show_images(styles, 1, 4, figsize=(35, 15)) . Finally, we iterate over each image/syle pair and pass them through the TF Hub module we loaded earlier: . stylized_images = [] for image in tqdm(images): image = image[tf.newaxis, :] for style in styles: style = style[tf.newaxis, :] results = hub_module(tf.constant(image), tf.constant(style)) stylized_image = tensor_to_image(results[0]) stylized_images.append(stylized_image) . 100%|██████████| 3/3 [00:41&lt;00:00, 13.90s/it] . Let&#39;s inspect the resulting images for each content image and syle . show_images(stylized_images, 3, 4, (28, 15)) . The result looks pretty good as you can see, the resulting images preserve the coherence and meaning from the original scene, while adding artistic traits from the style images. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/artistic/generative/2021/04/05/Style_Transfer_TFHub.html",
            "relUrl": "/tensorflow/artistic/generative/2021/04/05/Style_Transfer_TFHub.html",
            "date": " • Apr 5, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Effortless Video Action Recognition with TF Hub",
            "content": "Action recognition is a very hot topic in the broader video processing and understanding research. The below timeline illustrates how reaseracher are getting more interested in the area and proposing SOTA models to push the field further. source . Training a model from scratch to perform this task is very challenging due to the nature of the task that is not only associated with classifying the content of an image, but also includes a temporal component. In this article, we will leaverage TensorFlow Hub and pick one model from this large model zoo to perform Video Action Recognition effortlessly. Specifically we will use DeepMind&#39;s Inflated 3D Convnet (I3D) model which was training on DeepMind Kinetics dataset. . . First, let&#39;s import all the needed dependencies . import os import random import re import ssl import tempfile from pathlib import Path from urllib import request import matplotlib.pyplot as plt import cv2 import imageio import numpy as np import tensorflow as tf import tensorflow_hub as tfhub from IPython.display import Image from wordcloud import WordCloud . Dataset . There is lot datasets, you can find more here, we will be using UCF101 Action Recognition dataset which is available for public download here. We will not be using the entirety of the dataset but just picking a random video. . The different activities available in the UCF101 Action Recognition dataset are as follows: . . To list and download videos from the UCF101 website we define the following helper class that exposes multiple functions to make it easy to get a video locally. . class UCFDataset(object): def __init__(self): self.UNVERIFIED_CONTEXT = ssl._create_unverified_context() self.UCF_ROOT = &#39;https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/&#39; # Temporary directory to cache the downloaded videos self.CACHE_DIR = tempfile.mkdtemp() self.videos_list = self.download_videos_list() def _read(self, url): &quot;&quot;&quot;Read data for the given url&quot;&quot;&quot; return request.urlopen(url,context=self.UNVERIFIED_CONTEXT).read() def download_videos_list(self): &quot;&quot;&quot;Dowload the list of video names and direct download urls&quot;&quot;&quot; index = (self._read(self.UCF_ROOT).decode(&#39;utf-8&#39;)) videos = re.findall(&#39;(v_[ w]+ .avi)&#39;, index) return sorted(set(videos)) def __getitem__(self, video_name): &quot;&quot;&quot;Download a specific video by name&quot;&quot;&quot; cache_path = os.path.join(self.CACHE_DIR, video_name) if not os.path.exists(cache_path): url = request.urljoin(self.UCF_ROOT, video_name) response = self._read(url) with open(cache_path, &#39;wb&#39;) as f: f.write(response) return cache_path def download_random_video(self): &quot;&quot;&quot;Download a random video from the dataset&quot;&quot;&quot; video_name = random.choice(self.videos_list) return self.__getitem__(video_name) . Define a helper function to crop a squared selection in the center of a frame . def crop_center(frame): height, width = frame.shape[:2] smallest_dimension = min(width, height) x_start = (width // 2) - (smallest_dimension // 2) x_end = x_start + smallest_dimension y_start = (height // 2) - (smallest_dimension // 2) y_end = y_start + smallest_dimension roi = frame[y_start:y_end, x_start:x_end] return roi . Define a helper function to read a video by path, take up to max_frames frames from it, and return a resized to (224, 224, 3) selection of those frames. . def read_video(path, max_frames=32, resize=(224, 224)): capture = cv2.VideoCapture(path) frames = [] while len(frames) &lt;= max_frames: frame_read, frame = capture.read() if not frame_read: break frame = crop_center(frame) frame = cv2.resize(frame, resize) frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) frames.append(frame) capture.release() frames = np.array(frames) return frames / 255. . To be able to visualize the video on this notebook, we need function to create a save a collection of frames as a GIF. . def save_as_gif(images, video_name): filename = f&#39;./{video_name}.gif&#39; converted_images = np.clip(images * 255, 0, 255) converted_images = converted_images.astype(np.uint8) imageio.mimsave(filename, converted_images,fps=25) return filename . Now, we can download the list of videos from the UFC101 dataset . dataset = UCFDataset() . Model . In this section we download the I3D model and prepare it for running predictions on images . First, let&#39;s get the labels file from the Kinetics dataset that was used to train the I3D model . KINETICS_URL = &#39;https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt&#39; . The following helper function download the labels from the previous link . def fetch_kinetics_labels(): with request.urlopen(KINETICS_URL) as f: labels = [line.decode(&#39;utf-8&#39;).strip() for line in f.readlines()] return labels . Download the list of labels, and diplay them in a wordcloud . LABELS = fetch_kinetics_labels() . wordcloud = WordCloud(collocations = False, background_color = &#39;white&#39;) wordcloud = wordcloud.generate(&#39; &#39;.join(LABELS)) plt.figure(figsize=(10, 12)) plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Now let&#39;s download the I3D model from TensorFlow Hub . model_path = &#39;https://tfhub.dev/deepmind/i3d-kinetics-400/1&#39; model = tfhub.load(model_path) model = model.signatures[&#39;default&#39;] . Prediction . Next we define a helper function that will take sampled frames from a video, and print at most the top recognized actions . def predict(model, labels, sample_video): model_input = tf.constant(sample_video, dtype=tf.float32) model_input = model_input[tf.newaxis, ...] logits = model(model_input)[&#39;default&#39;][0] probabilities = tf.nn.softmax(logits) print(&#39;Top actions:&#39;) for i in np.argsort(probabilities)[::-1][:5]: if probabilities[i] &lt; 0.01: break print(f&#39;{labels[i]}: {probabilities[i] *100:5.2f}%&#39;) . Define helper function to pick a random video, save its frames as GIF . def download_random_video(): video_path = dataset.download_random_video() sample_video = read_video(video_path) video_name = Path(video_path).stem gif_path = save_as_gif(sample_video, video_name) return sample_video, gif_path . Pick a random video, display the resulting GIF . sample_video, gif_path = download_random_video() Image(open(gif_path,&#39;rb&#39;).read()) . Pass the video through the I3D network to obtain the predicted actions . predict(model, LABELS, sample_video) . Top actions: javelin throw: 23.70% high jump: 16.25% triple jump: 10.52% throwing discus: 9.20% playing tennis: 8.41% . Try another video . sample_video, gif_path = download_random_video() Image(open(gif_path,&#39;rb&#39;).read()) . predict(model, LABELS, sample_video) . Top actions: wrestling: 58.94% throwing ball: 11.95% high kick: 5.55% catching or throwing frisbee: 4.60% catching or throwing softball: 3.12% . See how the model is able to acurately predict the action in the video. You can try with other video as an exercise. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/video/action-recognition/2021/03/30/Action_Recognition_TFHub.html",
            "relUrl": "/tensorflow/video/action-recognition/2021/03/30/Action_Recognition_TFHub.html",
            "date": " • Mar 30, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Image Captioning Part I",
            "content": "In order to build an image captioning model, we need to transform the data and extract features that can be used as input for such model. We need to encode the images into a dense representation as well as encode the text as embeddings (vectorial representations of sentences). . This article, will focus on building this feature extractor and apply to generate a formatted features for the image captioning model trainig. . We will use the Flickr8k dataset (availble on Kaggle here). So we wil install Kaggle CLI and download the dataset. . %%capture %%bash pip install kaggle --upgrade mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;KAGGLE_USER&quot;,&quot;key&quot;:&quot;KAGGLE_KEY&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json . %%capture %%bash kaggle datasets download adityajn105/flickr8k mkdir -p flickr8k unzip flickr8k.zip -d flickr8k . . Note: you need to replace KAGGLE_USER with your actual Kaggle username and KAGGLE_KEY with your API key for the download to work. . First, lets mount a Google drive so we can store and later restore the features we will extract . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . Import the needed dependencies . import glob import os import pathlib from math import sqrt from joblib import Parallel, delayed import pickle from string import punctuation import numpy as np import pandas as pd import matplotlib.pyplot as plt from tensorflow.keras.applications.vgg16 import * from tensorflow.keras.layers import * from tensorflow.keras.preprocessing.image import * from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.utils import to_categorical from tqdm import tqdm . Set the following variables to the paths where the dataset is available . BASE_PATH = &#39;flickr8k&#39; IMAGES_PATH = f&#39;{BASE_PATH}/Images&#39; CAPTIONS_PATH = f&#39;{BASE_PATH}/captions.txt&#39; . With the path to the images set, we can list all the image files . image_paths = list(glob.glob(f&#39;{IMAGES_PATH}/*.jpg&#39;)) . Let&#39;s read the captions file into a Pandas dataframe and have look to it . captions_df = pd.read_csv(CAPTIONS_PATH) captions_df = captions_df.groupby(&#39;image&#39;).first().reset_index() captions_df.head() . image caption . 0 1000268201_693b08cb0e.jpg | A child in a pink dress is climbing up a set o... | . 1 1001773457_577c3a7d70.jpg | A black dog and a spotted dog are fighting | . 2 1002674143_1b742ab4b8.jpg | A little girl covered in paint sits in front o... | . 3 1003163366_44323f5815.jpg | A man lays on a bench while his dog sits by him . | . 4 1007129816_e794419615.jpg | A man in an orange hat starring at something . | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . Note: in the dataset each image has more than one caption, we discard all captions and keep only one per image for simplicity . With the images and captions loaded, we can take a random sample and dispaly some images with their respective caption . samples = captions_df.sample(6).reset_index() figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, sample in samples.iterrows(): image = plt.imread(f&#39;{IMAGES_PATH}/{sample[&quot;image&quot;]}&#39;) title = sample[&#39;caption&#39;][:50] + &#39; n&#39; + sample[&#39;caption&#39;][50:] row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].set_title(title) axis[row, col].axis(&#39;off&#39;) . We need to clean the text in captions (e.g. removing punctuation) to simplify training, and also adding special tokens start_token to be added at the begning of the text, and end_token added at the end. . def clean_caption(caption, start_token=&#39;_sos_&#39;, end_token=&#39;_eos_&#39;): def remove_punctuation(word): translation = str.maketrans(&#39;&#39;, &#39;&#39;, punctuation) return word.translate(translation) def is_valid_word(word): return len(word) &gt; 1 and word.isalpha() caption = caption.lower().split(&#39; &#39;) caption = map(remove_punctuation, caption) caption = filter(is_valid_word, caption) cleaned_caption = f&#39;{start_token} {&quot; &quot;.join(caption)} {end_token}&#39; return cleaned_caption . captions_df[&#39;caption&#39;] = captions_df.caption.apply(lambda x: clean_caption(x)) . We will need to perform padding later, hence we need to determine the maximum caption text length . ax = captions_df.caption.apply(lambda x: len(x)).hist() ax.vlines(x=120, ymin=0, ymax=2800, colors=&#39;red&#39;, ls=&#39;-&#39;, lw=2) . &lt;matplotlib.collections.LineCollection at 0x7f5fe2701150&gt; . We need to determine the maximum length to allow for captions, so we can either: . determine the maximum length with captions_df.caption.apply(lambda x: len(x)).max() | Or pick a maximum length based on the histogram of values to avoid OOMs, e.g. 120 | . max_seq_len = 120 . We take the captions into an array that should be of same length as the image paths array . captions = captions_df.caption.values . len(image_paths), len(captions) . (8091, 8091) . The following class groups the different feautre extraction steps that loads VGG16 trained on ImageNet to be used for transforming images into a vector, also it will sequences out of the image&#39;s caption. . class FeatureExtractor(object): def __init__(self, output_path, max_seq_len): self.feature_extractor = VGG16(input_tensor=Input(shape=(224, 224, 3)), weights=&#39;imagenet&#39;, include_top=False) self.output_path = output_path self.tokenizer = Tokenizer() self.max_seq_length = max_seq_len def create_image_vector(self, path): &quot;&quot;&quot;Create a vector representing the image&quot;&quot;&quot; image = load_img(path, target_size=(224, 224)) image = img_to_array(image) image = np.expand_dims(image, axis=0) image = preprocess_input(image) return self.feature_extractor.predict(image)[0] def create_mapping(self, paths, captions): &quot;&quot;&quot;Extract features from each image, and create mapping from image filename to image features and caption&quot;&quot;&quot; mapping = {} print(&#39; nExtracting features...&#39;) def extract_feature(path, caption): features = self.create_image_vector(path) image_id = pathlib.Path(path).stem mapping[image_id] = {&#39;features&#39;: features, &#39;caption&#39;: caption} for path, caption in tqdm(zip(paths, captions), total=len(paths)): extract_feature(path, caption) return mapping def save_mapping(self, mapping): &quot;&quot;&quot;Save mappings into disk&quot;&quot;&quot; out_path = f&#39;{self.output_path}/mapping.pickle&#39; with open(out_path, &#39;wb&#39;) as f: pickle.dump(mapping, f, protocol=4) def extract_features(self, paths, captions): &quot;&quot;&quot;Extract features from images and captions and stores them to disk&quot;&quot;&quot; self.tokenizer.fit_on_texts(captions) mapping = self.create_mapping(paths, captions) self.save_mapping(mapping) in_feats, in_seqs, out_seqs = self.create_sequences(mapping) self.save_sequences(in_feats, in_seqs, out_seqs) def create_sequences(self, mapping): &quot;&quot;&quot;Create sequences based on the image and its caption&quot;&quot;&quot; num_classes = len(self.tokenizer.word_index) + 1 in_feats = [] in_seqs = [] out_seqs = [] print(&#39; nCreating sequences...&#39;) for _, data in tqdm(mapping.items()): feature = data[&#39;features&#39;] caption = data[&#39;caption&#39;] seq = self.tokenizer.texts_to_sequences([caption]) seq = seq[0] # create as many sequences as the length of the current caption for i in range(1, len(seq)): # input sequence is first ith characters input_seq = seq[:i] # trip the sentence if it is longer than max_seq_length if len(input_seq) &gt;= self.max_seq_length: input_seq = input_seq[:self.max_seq_length] input_seq, = pad_sequences([input_seq], self.max_seq_length) # output sequence is the next character which is the ith out_seq = seq[i] out_seq = to_categorical([out_seq], num_classes)[0] # add all to lists in_feats.append(feature) in_seqs.append(input_seq) out_seqs.append(out_seq) return in_feats, in_seqs, out_seqs def save_sequences(self, in_feats, in_seqs, out_seqs): &quot;&quot;&quot;Save image features, input and output sequences into disk&quot;&quot;&quot; filenames = [&#39;input_features.pickle&#39;, &#39;input_sequences.pickle&#39;, &#39;output_sequences.pickle&#39;] sequences = [in_feats, in_seqs, out_seqs] for filename, seq in zip(filenames, sequences): with open(f&#39;{self.output_path}/{filename}&#39;, &#39;wb&#39;) as f: pickle.dump(seq, f, protocol=4) . Now we can create an instance of the feature extractor and run it on the images and their captions . extractor = FeatureExtractor(&#39;.&#39;, max_seq_len) . extractor.extract_features(image_paths, captions) . Extracting features... . 100%|██████████| 8091/8091 [1:10:57&lt;00:00, 1.90it/s] . Creating sequences... . 100%|██████████| 8091/8091 [00:07&lt;00:00, 1065.88it/s] . As the dataset have lot images, it may take some time before it finishes. At the end, it will generate the following pickle files: . mapping.pickle | input_features.pickle | input_sequences.pickle | output_sequences.pickle | . Move the pickle files to Google Drive so we can restore them later . !ls /content/drive/MyDrive/data/ . input_features.pickle mapping.pickle input_sequences.pickle output_sequences.pickle . !mv *.pickle /content/drive/MyDrive/data/ . Next we will use those files as input for training an Image Captioning model. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/captioning/2021/03/21/Image_Captioning_Part_I.html",
            "relUrl": "/tensorflow/vision/captioning/2021/03/21/Image_Captioning_Part_I.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Object Detection on custom dataset with EfficientNet",
            "content": "In a previous article we saw how to use TensorFlow&#39;s Object Detection API to run object detection on images using pre-trained models freely available to download from TF Hub - link. This article we will go one step further by training a model on our own custom Object detection dataset using TensorFlow&#39;s Object Detection API. . First, lets install the TensorFlow Object Detection API . %%capture %%bash git clone --depth 1 https://github.com/tensorflow/models cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install . . We need to have opencv-python-headless version with version 4.1.2.30 installed for the training using Object Detection API to work . %%capture %%bash yes | pip uninstall opencv-python-headless pip install opencv-python-headless==4.1.2.30 . The dataset we will use is Fruit Images for Object Detection dataset from Kaggle. This is a very small dataset with images of the three classes apple, banana and orange. It is more enough to get started with training on custom dataset but you can use your own dataset too. . We will use the Kaggle CLI to download the dataset, unzip and prepare the train/test datasets. . !pip install kaggle --upgrade -q . %%bash mkdir -p ~/.kaggle echo &#39;{&quot;username&quot;:&quot;KAGGLE_USERNAME&quot;,&quot;key&quot;:&quot;KAGGLE_KEY&quot;}&#39; &gt; ~/.kaggle/kaggle.json chmod 600 ~/.kaggle/kaggle.json . %%capture %%bash kaggle datasets download mbkinaci/fruit-images-for-object-detection unzip fruit-images-for-object-detection mkdir -p fruit-images mv train_zip/* fruit-images/ mv test_zip/* fruit-images/ . Note how we move files around in training and test, this is for convinience because the original dataset zip file had images under train_zip/train/*.jpg and test_zip/test/*.jpg. . Training . After download the dataset and install the required dependencies we can start writting the code for training . First, import dependencies . import glob import io import os from collections import namedtuple from xml.etree import ElementTree as tree import pandas as pd import tensorflow.compat.v1 as tf from PIL import Image from object_detection.utils import dataset_util from object_detection.protos import pipeline_pb2 from google.protobuf import text_format . Data transformation . We need to convert the images into the format accepted by the training pipeline in TensorFlow Object Detection API which is TF Example format. . We need to define a helper function to encode a label into its index . def encode_class(row_label): class_mapping = {&#39;apple&#39;: 1, &#39;orange&#39;: 2, &#39;banana&#39;: 3} return class_mapping.get(row_label, None) . We also define a helper function to create the train/test splits . def split(df, group): Data = namedtuple(&#39;data&#39;, [&#39;filename&#39;, &#39;object&#39;]) groups = df.groupby(group) return [Data(filename, groups.get_group(x)) for filename, x in zip(groups.groups.keys(), groups.groups)] . The following function takes train/test images and convert them into one corresponding TF Example file where the image, the bounding boxes, ground-truth classes are grouped as features. . def create_tf_example(group, path): groups_path = os.path.join(path, f&#39;{group.filename}&#39;) with tf.gfile.GFile(groups_path, &#39;rb&#39;) as f: encoded_jpg = f.read() image = Image.open(io.BytesIO(encoded_jpg)) width, height = image.size filename = group.filename.encode(&#39;utf8&#39;) image_format = b&#39;jpg&#39; # 5. Now, store the dimensions of the bounding boxes, along with the classes of each object contained in the image: xmins = [] xmaxs = [] ymins = [] ymaxs = [] classes_text = [] classes = [] for index, row in group.object.iterrows(): xmins.append(row[&#39;xmin&#39;] / width) xmaxs.append(row[&#39;xmax&#39;] / width) ymins.append(row[&#39;ymin&#39;] / height) ymaxs.append(row[&#39;ymax&#39;] / height) classes_text.append(row[&#39;class&#39;].encode(&#39;utf8&#39;)) classes.append(encode_class(row[&#39;class&#39;])) # 6. Create a tf.train.Features object that will contain relevant information about the image and its objects: features = tf.train.Features(feature={ &#39;image/height&#39;: dataset_util.int64_feature(height), &#39;image/width&#39;: dataset_util.int64_feature(width), &#39;image/filename&#39;: dataset_util.bytes_feature(filename), &#39;image/source_id&#39;: dataset_util.bytes_feature(filename), &#39;image/encoded&#39;: dataset_util.bytes_feature(encoded_jpg), &#39;image/format&#39;: dataset_util.bytes_feature(image_format), &#39;image/object/bbox/xmin&#39;: dataset_util.float_list_feature(xmins), &#39;image/object/bbox/xmax&#39;: dataset_util.float_list_feature(xmaxs), &#39;image/object/bbox/ymin&#39;: dataset_util.float_list_feature(ymins), &#39;image/object/bbox/ymax&#39;: dataset_util.float_list_feature(ymaxs), &#39;image/object/class/text&#39;: dataset_util.bytes_list_feature(classes_text), &#39;image/object/class/label&#39;: dataset_util.int64_list_feature(classes) }) # 7. Return a tf.train.Example structure initialized with the features created previously: return tf.train.Example(features=features) . The bounding boxes in the dataset for each image are defined in an XML file (base of PASCAL VOC format - link). We need to parse each of those metadata files to extract the bounding boxes and labels . def bboxes_to_csv(path): xml_list = [] bboxes_pattern = os.path.sep.join([path, &#39;*.xml&#39;]) for xml_file in glob.glob(bboxes_pattern): t = tree.parse(xml_file) root = t.getroot() for member in root.findall(&#39;object&#39;): value = (root.find(&#39;filename&#39;).text, int(root.find(&#39;size&#39;)[0].text), int(root.find(&#39;size&#39;)[1].text), member[0].text, int(member[4][0].text), int(member[4][1].text), int(member[4][2].text), int(member[4][3].text)) xml_list.append(value) column_names = [&#39;filename&#39;, &#39;width&#39;, &#39;height&#39;, &#39;class&#39;,&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;] df = pd.DataFrame(xml_list, columns=column_names) return df . Now let&#39;s process every image in the train/test datasets along with the metadata file to create the corresponding TF Record file. . base = &#39;fruit-images&#39; for subset in [&#39;test&#39;, &#39;train&#39;]: labels_path = os.path.sep.join([base,f&#39;{subset}_labels.csv&#39;]) bboxes_df = bboxes_to_csv(f&#39;{base}/{subset}&#39;) bboxes_df.to_csv(labels_path, index=None) # 10. Then, use the same labels to produce the tf.train.Examples corresponding to the current subset of data being processed: writer = (tf.io.TFRecordWriter(f&#39;{base}/{subset}.record&#39;)) examples = pd.read_csv(f&#39;{base}/{subset}_labels.csv&#39;) grouped = split(examples, &#39;filename&#39;) path = os.path.join(f&#39;{base}/{subset}&#39;) for group in grouped: tf_example = create_tf_example(group, path) writer.write(tf_example.SerializeToString()) writer.close() . This is how the result of processing the metadata looks like . filename width height class xmin ymin xmax ymax . 0 | mixed_18.jpg | 1023 | 682 | orange | 67 | 163 | 441 | 541 | . 1 | mixed_18.jpg | 1023 | 682 | banana | 209 | 134 | 866 | 348 | . 2 | mixed_18.jpg | 1023 | 682 | banana | 263 | 267 | 849 | 551 | . 3 | apple_11.jpg | 652 | 436 | apple | 213 | 33 | 459 | 258 | . 4 | apple_11.jpg | 652 | 436 | apple | 1 | 30 | 188 | 280 | . Now the data is reading for training . !ls -l fruit-images . total 30388 drwxr-xr-x 2 root root 4096 Jan 22 02:38 test -rw-r--r-- 1 root root 5161 Jan 22 02:38 test_labels.csv -rw-r--r-- 1 root root 7081578 Jan 22 02:38 test.record drwxr-xr-x 2 root root 20480 Jan 22 02:38 train -rw-r--r-- 1 root root 20122 Jan 22 02:38 train_labels.csv -rw-r--r-- 1 root root 23981776 Jan 22 02:38 train.record . Pre-trained EfficientNet . To run the training on our custom dataset, we will fine tune EfficientNet one of the models in TensorFlow Object Detection API that was trained on COCO dataset. We will download a checkpoint of the model&#39;s weights from TensorFlow 2 Detection Model Zoo. Specifically we will downlad the weights of EfficientDet D0 512x512 but you can smaller models like MobileNet v2 320x320 for faster training. . %%capture %%bash CHECKPOINT_DATE=20200711 MODEL_NAME=efficientdet_d0_coco17_tpu-32 curl -O http://download.tensorflow.org/models/object_detection/tf2/$CHECKPOINT_DATE/$MODEL_NAME.tar.gz tar xzf $MODEL_NAME.tar.gz . !ls efficientdet_d0_coco17_tpu-32/checkpoint . checkpoint ckpt-0.data-00000-of-00001 ckpt-0.index . We need to create the label_map.txt file to map the classes to integers . %%writefile fruit-images/label_map.txt item { id: 1 name: &#39;apple&#39; } item { id: 2 name: &#39;orange&#39; } item { id: 3 name: &#39;banana&#39; } . Writing fruit-images/label_map.txt . Next, we need to change the configuration file for this network to fit our need. This configuration file can be found locally at models/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config. . Note: those configuration files are Protocol Buffers objects described in the .proto files under models/research/object_detection/protos. The top level object is a TrainEvalPipelineConfig defined in pipeline.proto. You can learn more about those configuration files by reading the documentation. . The following helper functions are used to load and save a configuration file, they are based of code borrowed from config_util.py. . def get_pipeline_config(path): pipeline_config = pipeline_pb2.TrainEvalPipelineConfig() with tf.gfile.GFile(path, &#39;r&#39;) as f: text_format.Merge(f.read(), pipeline_config) return pipeline_config def save_pipeline_config(pipeline_config, path): config_text = text_format.MessageToString(pipeline_config) with tf.gfile.Open(path, &quot;wb&quot;) as f: tf.logging.info(&quot;Writing pipeline config file to %s&quot;, path) f.write(config_text) . Load the EfficientNet configuration and update it accordingly: . pipeline_config_path = &#39;models/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config&#39; pipeline_config = get_pipeline_config(pipeline_config_path) . Lower batch size depending on how much memory your system has . pipeline_config.train_config.batch_size = 16 . Update the number of classes to match the ones in our custom dataset . pipeline_config.model.ssd.num_classes = 3 . Point to the checkout point file of the EfficientNet weights we downloaded earlier . pipeline_config.train_config.fine_tune_checkpoint = &#39;/content/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0&#39; . Change the checkpoint type to detection . pipeline_config.train_config.fine_tune_checkpoint_type = &#39;detection&#39; . Point to the label/index mapping file . pipeline_config.train_input_reader.label_map_path = &#39;/content/fruit-images/label_map.txt&#39; . Point to the training TF Record file we created earlier . pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = &#39;/content/fruit-images/train.record&#39; . Point to the label/index mapping file . pipeline_config.eval_input_reader[0].label_map_path = &#39;/content/fruit-images/label_map.txt&#39; . Point to the test TF Record file we created earlier . pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0] = &#39;/content/fruit-images/test.record&#39; . save_pipeline_config(pipeline_config, &#39;pipeline.config&#39;) . INFO:tensorflow:Writing pipeline config file to pipeline.config . Now we can start training the model using the model_main_tf2.py helper program and the configuration we just updated. . %%bash cd models/research/object_detection mkdir -p /content/training CONFIG_PATH=/content/pipeline.config MODEL_DIR=/content/training python model_main_tf2.py --pipeline_config_path=$CONFIG_PATH --model_dir=$MODEL_DIR --num_train_steps=1000 --record_summaries . INFO:tensorflow:Step 1000 per-step time 1.237s I0122 03:01:21.339013 139649250535296 model_lib_v2.py:707] Step 1000 per-step time 1.237s INFO:tensorflow:{&#39;Loss/classification_loss&#39;: 0.2277294, &#39;Loss/localization_loss&#39;: 0.1371322, &#39;Loss/regularization_loss&#39;: 0.028924104, &#39;Loss/total_loss&#39;: 0.3937857, &#39;learning_rate&#39;: 0.0326} I0122 03:01:21.339331 139649250535296 model_lib_v2.py:708] {&#39;Loss/classification_loss&#39;: 0.2277294, &#39;Loss/localization_loss&#39;: 0.1371322, &#39;Loss/regularization_loss&#39;: 0.028924104, &#39;Loss/total_loss&#39;: 0.3937857, &#39;learning_rate&#39;: 0.0326} . Once the training is finished, we can check the training logs using TensorBoard . %load_ext tensorboard %tensorboard --logdir /content/training . To be able to use the new trained model in inference, we need to use the exporter_main_v2.py program as follows: . %%bash cd models/research/object_detection mkdir -p /content/inference_graph CHECKPOINT_DIR=/content/training CONFIG_PATH=/content/pipeline.config OUTPUT_DIR=/content/inference_graph python exporter_main_v2.py --trained_checkpoint_dir=$CHECKPOINT_DIR --pipeline_config_path=$CONFIG_PATH --output_directory=$OUTPUT_DIR . ... INFO:tensorflow:Assets written to: /content/inference_graph/saved_model/assets I0122 03:03:07.437493 140608935167872 builder_impl.py:784] Assets written to: /content/inference_graph/saved_model/assets INFO:tensorflow:Writing pipeline config file to /content/inference_graph/pipeline.config I0122 03:03:08.824219 140608935167872 config_util.py:254] Writing pipeline config file to /content/inference_graph/pipeline.config . Inference . Let&#39;s use the trained model on the test images and check its quality. . import glob import random from io import BytesIO import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from PIL import Image from object_detection.utils import ops from object_detection.utils import visualization_utils as viz from object_detection.utils.label_map_util import create_category_index_from_labelmap %matplotlib inline . Define a helper function to load an image and prepare them for the model expected input . def load_image(path): image_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(image_data)) width, height = image.size shape = (height, width, 3) image = np.array(image.getdata()) image = image.reshape(shape).astype(&#39;uint8&#39;) return image . Define a helper function to run inference on an input image . def run_inference(net, image): image = np.asarray(image) input_tensor = tf.convert_to_tensor(image) input_tensor = input_tensor[tf.newaxis, ...] # forward pass model = net.signatures[&#39;serving_default&#39;] result = model(input_tensor) # extract detections num_detections = int(result.pop(&#39;num_detections&#39;)) result = {key: value[0, :num_detections].numpy() for key, value in result.items()} result[&#39;num_detections&#39;] = num_detections result[&#39;detection_classes&#39;] = result[&#39;detection_classes&#39;].astype(&#39;int64&#39;) # use mask if available if &#39;detection_masks&#39; in result: detection_masks_reframed = ops.reframe_box_masks_to_image_masks(result[&#39;detection_masks&#39;], result[&#39;detection_boxes&#39;], image.shape[0], image.shape[1]) detection_masks_reframed = tf.cast(detection_masks_reframed &gt; 0.5, tf.uint8) result[&#39;detection_masks_reframed&#39;] = detection_masks_reframed.numpy() return result . Let&#39;s load the model we exported earlier and create CATEGORY_IDX based on the label/index mapping file . labels_path = &#39;/content/fruit-images/label_map.txt&#39; CATEGORY_IDX = create_category_index_from_labelmap(labels_path, use_display_name=True) model_path = &#39;/content/inference_graph/saved_model&#39; model = tf.saved_model.load(model_path) . Select random images from the test dataset . image_paths = list(glob.glob(&#39;fruit-images/test/*.jpg&#39;)) image_paths = random.choices(image_paths, k=6) . Define a helper function to load and image, run inference on it and draw the predicted bounding boxes: . def get_image_with_boxes(model, path): image = load_image(path) annotation = run_inference(model, image) masks = annotation.get(&#39;detection_masks_reframed&#39;, None) viz.visualize_boxes_and_labels_on_image_array( image, annotation[&#39;detection_boxes&#39;], annotation[&#39;detection_classes&#39;], annotation[&#39;detection_scores&#39;], CATEGORY_IDX, instance_masks=masks, use_normalized_coordinates=True, line_thickness=5) return image . image_paths = list(glob.glob(&#39;fruit-images/test/*.jpg&#39;)) image_paths = random.choices(image_paths, k=6) images = [get_image_with_boxes(model, path) for path in image_paths] . Display the images along with the bounding boxes . figure, axis = plt.subplots(2, 3, figsize=(15, 10)) for index, image in enumerate(images): row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].axis(&#39;off&#39;) . The model see to perfom quite well, you can try train it on a harder dataset .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/object-detection/2021/03/02/Object_Detection_custom_dataset.html",
            "relUrl": "/tensorflow/vision/object-detection/2021/03/02/Object_Detection_custom_dataset.html",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "Object Detection with YOLOv3 in Tensorflow",
            "content": "YOLO (You Only Look Once) is an end to end object detection algorithm. Compared to other algorithms that repurpose classifiers to perform detection, YOLO requires only a single pass to detect objects, i.e. classes probabilities and bounding boxes. . It works by dividing an image into N equaly sized SxS regions. Each grid will predict the probability of object presence, the coordinates of a bounding box B and the object label. This makes YOLO a very fast algorithm and can be used for real time detection. . In this article, we will build YOLO v3 in Tensorflow and initiate its weights with the weights of the original YOLO v3 model pretrained on the COCO dataset. The following diagram illustrates the architecture of YOLO we will be building. . . Our implementation is heavily inspired by this Keras implementation - repo . First, let&#39;s download the weights from the YOLO website, as well as the labels of the COCO dataset . %%capture %%bash curl -s -O https://pjreddie.com/media/files/yolov3.weights curl -o coco_labels.txt https://raw.githubusercontent.com/amikelive/coco-labels/master/coco-labels-2014_2017.txt echo &quot;[[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]&quot; &gt; anchors.json . Declare all the imports we will be needed . import glob import json import struct import matplotlib.pyplot as plt import numpy as np from matplotlib.patches import Rectangle import tensorflow as tf from tensorflow.keras.layers import * from tensorflow.keras.models import * from tensorflow.keras.preprocessing.image import * %matplotlib inline . Second, we define a class to initialize the weight of a YOLO network, it will read the weights file and set the weight of every layer in the model&#39;s 106 layers. . class ModelInitializer: def __init__(self, weight_file): self.offset = 0 self.all_weights = self._load_weights(weight_file) def _load_weights(self, weight_file): with open(weight_file, &#39;rb&#39;) as w_f: major, = struct.unpack(&#39;i&#39;, w_f.read(4)) minor, = struct.unpack(&#39;i&#39;, w_f.read(4)) revision, = struct.unpack(&#39;i&#39;, w_f.read(4)) if (major * 10 + minor) &gt;= 2 and major &lt; 1000 and minor &lt; 1000: w_f.read(8) else: w_f.read(4) binary = w_f.read() return np.frombuffer(binary, dtype=&#39;float32&#39;) def read_bytes(self, size): &quot;&quot;&quot;return number of bytes from the weights file&quot;&quot;&quot; self.offset = self.offset + size return self.all_weights[self.offset-size:self.offset] def init_layer_weights(self, model, i): &quot;&quot;&quot;Initialize the weight of a sepecific layer by its index&quot;&quot;&quot; conv_layer = model.get_layer(f&#39;conv_{i}&#39;) if i not in [81, 93, 105]: norm_layer = model.get_layer(f&#39;bnorm_{i}&#39;) size = np.prod(norm_layer.get_weights()[0].shape) bias = self.read_bytes(size) scale = self.read_bytes(size) mean = self.read_bytes(size) var = self.read_bytes(size) norm_layer.set_weights([scale, bias, mean, var]) if len(conv_layer.get_weights()) &gt; 1: bias = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape)) kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape)) kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape))) kernel = kernel.transpose([2, 3, 1, 0]) conv_layer.set_weights([kernel, bias]) else: kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape)) kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape))) kernel = kernel.transpose([2, 3, 1, 0]) conv_layer.set_weights([kernel]) def init_weights(self, model): &quot;&quot;&quot;Load the weights of each of the model 106 layers&quot;&quot;&quot; for i in range(106): try: self.init_layer_weights(model, i) except ValueError: pass def reset(self): self.offset = 0 . To make our life easy when dealing with the coordinates and any information about the detected onject, we group those metadata information into a class . class BoundBox(object): def __init__(self, x_min, y_min, x_max, y_max, objness=None, classes=None): self.xmin = x_min self.ymin = y_min self.xmax = x_max self.ymax = y_max self.objness = objness self.classes = classes self.label = -1 self.score = -1 def get_label(self): if self.label == -1: self.label = np.argmax(self.classes) return self.label def get_score(self): if self.score == -1: self.score = self.classes[self.get_label()] return self.score . Next, we define some some utility functions that work on the bounding boxes: . calculate_interval_overlap to perform Non Maximal Suppression or shorly NMS (for more details see link) and reduce detections of same object multiple times. | calculate_bbox_iou to calculate the value of Intersection Over Union or shorly IoU (for more details see link) between two bounding boxes | . def calculate_interval_overlap(interval_a, interval_b): x1, x2 = interval_a x3, x4 = interval_b if x3 &lt; x1: if x4 &lt; x1: return 0 else: return min(x2, x4) - x1 else: if x2 &lt; x3: return 0 else: return min(x2, x4) - x3 def calculate_bbox_iou(box1, box2): intersect_w = calculate_interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax]) intersect_h = calculate_interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax]) intersect = intersect_w * intersect_h w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin union = w1 * h1 + w2 * h2 - intersect return float(intersect) / union . Next, we define a helper function to create a YOLO convolutional block, add batch normalization, leaky ReLU activations, and optionally use a skip connection. . def create_conv_block(input, convolutions, skip=True): x = input count = 0 for conv in convolutions: if count == (len(convolutions) - 2) and skip: skip_connection = x count += 1 if conv[&#39;stride&#39;] &gt; 1: x = ZeroPadding2D(((1, 0), (1, 0)))(x) x = Conv2D( conv[&#39;filter&#39;], conv[&#39;kernel&#39;], strides=conv[&#39;stride&#39;], padding=(&#39;valid&#39; if conv[&#39;stride&#39;] &gt; 1 else &#39;same&#39;), name=f&#39;conv_{conv[&quot;layer_idx&quot;]}&#39;, use_bias=(False if conv[&#39;bnorm&#39;] else True) )(x) if conv[&#39;bnorm&#39;]: name = f&#39;bnorm_{conv[&quot;layer_idx&quot;]}&#39; x = BatchNormalization(epsilon=1e-3, name=name)(x) if conv[&#39;leaky&#39;]: name = f&#39;leaky_{conv[&quot;layer_idx&quot;]}&#39; x = LeakyReLU(alpha=0.1, name=name)(x) return Add()([skip_connection, x]) if skip else x . Next, we define a helper function to create the actual architecture of the YOLO model and all of its 106 layers using the previously defined helper function to create individual blocks. . def create_architecture(): input_image = Input(shape=(None, None, 3)) # Layer 0 =&gt; 4 x = create_conv_block(input_image, [ {&#39;filter&#39;: 32, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 0}, {&#39;filter&#39;: 64, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 1}, {&#39;filter&#39;: 32, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 2}, {&#39;filter&#39;: 64, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 3} ]) # Layer 5 =&gt; 8 x = create_conv_block(x, [ {&#39;filter&#39;: 128, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 5}, {&#39;filter&#39;: 64, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 6}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 7} ]) # Layer 9 =&gt; 11 x = create_conv_block(x, [ {&#39;filter&#39;: 64, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 9}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 10} ]) # Layer 12 =&gt; 15 x = create_conv_block(x, [ {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 12}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 13}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 14} ]) # Layer 16 =&gt; 36 for i in range(7): x = create_conv_block(x, [ {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 16 + i * 3}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 17 + i * 3} ]) skip_36 = x # Layer 37 =&gt; 40 x = create_conv_block(x, [ {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 37}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 38}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 39} ]) # Layer 41 =&gt; 61 for i in range(7): x = create_conv_block(x, [ {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 41 + i * 3}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 42 + i * 3} ]) skip_61 = x # Layer 62 =&gt; 65 x = create_conv_block(x, [ {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 2, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 62}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 63}, {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 64} ]) # Layer 66 =&gt; 74 for i in range(3): x = create_conv_block(x, [ {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 66 + i * 3}, {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 67 + i * 3} ]) # Layer 75 =&gt; 79 x = create_conv_block(x, [ {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 75}, {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 76}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 77}, {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 78}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 79} ], skip=False) # Layer 80 =&gt; 82 yolo_82 = create_conv_block(x, [ {&#39;filter&#39;: 1024, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 80}, {&#39;filter&#39;: 255, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: False, &#39;leaky&#39;: False, &#39;layer_idx&#39;: 81} ], skip=False) # Layer 83 =&gt; 86 x = create_conv_block(x, [ {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 84}], skip=False) x = UpSampling2D(2)(x) x = Concatenate()([x, skip_61]) # Layer 87 =&gt; 91 x = create_conv_block(x, [ {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 87}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 88}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 89}, {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 90}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 91} ], skip=False) # Layer 92 =&gt; 94 yolo_94 = create_conv_block(x, [ {&#39;filter&#39;: 512, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 92}, {&#39;filter&#39;: 255, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: False, &#39;leaky&#39;: False, &#39;layer_idx&#39;: 93} ], skip=False) # Layer 95 =&gt; 98 x = create_conv_block(x, [ {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 96}], skip=False) x = UpSampling2D(2)(x) x = Concatenate()([x, skip_36]) # Layer 99 =&gt; 106 yolo_106 = create_conv_block(x, [ {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 99}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 100}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 101}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 102}, {&#39;filter&#39;: 128, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 103}, {&#39;filter&#39;: 256, &#39;kernel&#39;: 3, &#39;stride&#39;: 1, &#39;bnorm&#39;: True, &#39;leaky&#39;: True, &#39;layer_idx&#39;: 104}, {&#39;filter&#39;: 255, &#39;kernel&#39;: 1, &#39;stride&#39;: 1, &#39;bnorm&#39;: False, &#39;leaky&#39;: False, &#39;layer_idx&#39;: 105} ], skip=False) return Model(inputs=input_image, outputs=[yolo_82, yolo_94, yolo_106]) . Now, we can finalize the model creation and load its weight. We put all this into the following function: . def create_model(weights_path): model = create_architecture() initializer = ModelInitializer(weights_path) initializer.init_weights(model) model.save(&#39;model.h5&#39;) model = load_model(&#39;model.h5&#39;) return model . Next, we define some utility functions to read the COCO labels and YOLO anchors files. . The YOLO bounding boxes are defined in the context of anchor boxes chosen by the original authors based on the size of objects in the COCO dataset. . def get_labels(path): labels = [] with open(path, &#39;r&#39;) as f: for l in f: labels.append(l.strip()) return labels def get_anchors(path): with open(path, &#39;r&#39;) as f: return json.load(f) . Next, we define a helper class that will be used to wrap the YOLO model and provide an easy way to run inference and return detected objects with their bounding boxes. . class YOLOv3(object): def __init__(self, model, labels, anchors, class_threshold=0.65): self.model = model self.labels = labels self.anchors = anchors self.class_threshold = class_threshold @staticmethod def _sigmoid(x): &quot;&quot;&quot;return the Sigmoid value of a tensor&quot;&quot;&quot; return 1.0 / (1.0 + np.exp(-x)) def _decode_output(self, network_output, anchors, obj_thresh, network_height, network_width): &quot;&quot;&quot;Decode the output of YOLO candidate bounding boxes and class predictions&quot;&quot;&quot; grid_height, grid_width = network_output.shape[:2] nb_box = 3 network_output = network_output.reshape((grid_height, grid_width, nb_box, -1)) boxes = [] network_output[..., :2] = self._sigmoid(network_output[..., :2]) network_output[..., 4:] = self._sigmoid(network_output[..., 4:]) network_output[..., 5:] = (network_output[..., 4][..., np.newaxis] * network_output[..., 5:]) network_output[..., 5:] *= network_output[..., 5:] &gt; obj_thresh for i in range(grid_height * grid_width): r = i / grid_width c = i % grid_width for b in range(nb_box): objectness = network_output[int(r)][int(c)][b][4] if objectness.all() &lt;= obj_thresh: # skip bounding boxes as confidence of object presence is low continue x, y, w, h = network_output[int(r)][int(c)][b][:4] x = (c + x) / grid_width y = (r + y) / grid_height w = (anchors[2 * b] * np.exp(w) / network_width) h = (anchors[2 * b + 1] * np.exp(h) / network_height) classes = network_output[int(r)][c][b][5:] box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes) boxes.append(box) return boxes @staticmethod def _correct_yolo_boxes(boxes, image_height, image_width, network_height, network_width): &quot;&quot;&quot;Rescale the bounding boxes to the dimensions of the original image&quot;&quot;&quot; new_w, new_h = network_width, network_height for i in range(len(boxes)): x_offset = (network_width - new_w) / 2.0 x_offset /= network_width x_scale = float(new_w) / network_width y_offset = (network_height - new_h) / 2.0 y_offset /= network_height y_scale = float(new_h) / network_height boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_width) boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_width) boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_height) boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_height) def _non_max_suppression(self, boxes, nms_thresh): &quot;&quot;&quot;Minimize number of duplicates bounding boxes by apply NMS&quot;&quot;&quot; if len(boxes) &gt; 0: nb_class = len(boxes[0].classes) else: return for c in range(nb_class): sorted_indices = np.argsort([-box.classes[c] for box in boxes]) for i in range(len(sorted_indices)): index_i = sorted_indices[i] if boxes[index_i].classes[c] == 0: continue for j in range(i + 1, len(sorted_indices)): index_j = sorted_indices[j] iou = calculate_bbox_iou(boxes[index_i], boxes[index_j]) if iou &gt;= nms_thresh: boxes[index_j].classes[c] = 0 def _get_boxes(self, boxes): &quot;&quot;&quot;Select bounding boxes containing object with confidence above threshold&quot;&quot;&quot; v_boxes, v_labels, v_scores = [], [], [] for box in boxes: for i in range(len(self.labels)): if box.classes[i] &gt; self.class_threshold: v_boxes.append(box) v_labels.append(self.labels[i]) v_scores.append(box.classes[i] * 100) return v_boxes, v_labels, v_scores def predict(self, image, width, height): &quot;&quot;&quot;Return detected bounding boxes detected by YOLO and applies NMS to remove redundant detections&quot;&quot;&quot; image = np.expand_dims(image, axis=0) preds = self.model.predict(image) boxes = [] for i in range(len(preds)): # decodes the outputs of the network box = self._decode_output(preds[i][0], self.anchors[i], self.class_threshold, 416, 416) boxes.extend(box) # corrects the boxes so that they have proper proportions in relation to the input image self._correct_yolo_boxes(boxes, height, width, 416, 416) self._non_max_suppression(boxes, .5) # select valid bounding boxes valid_boxes, valid_labels, valid_scores = self._get_boxes(boxes) return valid_boxes, valid_labels, valid_scores . We also need a function to plot the predictions: bounding boxes and detection object label, as well as confidence score. . def draw_image_with_boxes(ax, data, v_boxes, v_labels, v_scores): ax.imshow(data) ax.axis(&#39;off&#39;) for i in range(len(v_boxes)): box = v_boxes[i] width = box.xmax - box.xmin height = box.ymax - box.ymin rectangle = Rectangle((box.xmin, box.ymin), width, height, fill=False, color=&#39;yellow&#39;) ax.add_patch(rectangle) label = f&#39;{v_labels[i]} ({v_scores[i]:.2f})&#39; ax.text(box.xmin, box.ymin, label, color=&#39;yellow&#39;) . Now, we can finally create a YOLO v3 model architecture and initialize its weight . Note: you can use tf.keras.utils.plot_model(yolo) to plot the model. . labels = get_labels(&#39;coco_labels.txt&#39;) anchors = get_anchors(&#39;anchors.json&#39;) network = create_model(&#39;yolov3.weights&#39;) model = YOLOv3(network, labels, anchors) . To test the model, let&#39;s download some images (you can use yours) . %%bash mkdir -p images curl -s -o images/bicycle1.jpg https://cdn.pixabay.com/photo/2016/11/30/12/29/bicycle-1872682_960_720.jpg curl -s -o images/bicycle2.jpg https://cdn.pixabay.com/photo/2016/11/22/23/49/cyclists-1851269_960_720.jpg curl -s -o images/animal1.jpg https://cdn.pixabay.com/photo/2014/05/20/21/20/bird-349026_960_720.jpg curl -s -o images/animal2.jpg https://cdn.pixabay.com/photo/2018/05/27/18/19/sparrows-3434123_960_720.jpg curl -s -o images/car1.jpg https://cdn.pixabay.com/photo/2016/02/13/13/11/oldtimer-1197800_960_720.jpg curl -s -o images/car2.jpg https://cdn.pixabay.com/photo/2016/09/11/10/02/renault-juvaquatre-1661009_960_720.jpg . Finally, run the model on each image and draw it to confirm the detections . figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, image_path in enumerate(glob.glob(&#39;images/*.jpg&#39;)): image = load_img(image_path, target_size=(416, 416)) image = img_to_array(image) image = image.astype(&#39;float32&#39;) / 255.0 original_image = load_img(image_path) width, height = original_image.size boxes, labels, scores = model.predict(image, width, height) row, col = int(index / 3), index % 3 draw_image_with_boxes(axis[row, col], original_image, boxes, labels, scores) . You can see that the model is able to acurrately detect the object and their bounding boxes. . To learn more about the YOLO algorithm I encourage you to read the original paper - link .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/object-detection/2021/02/24/Object_Detection_with_YOLOv3.html",
            "relUrl": "/tensorflow/vision/object-detection/2021/02/24/Object_Detection_with_YOLOv3.html",
            "date": " • Feb 24, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Effortless Object Detection with TF Hub",
            "content": "Object Detection is a computer vision task that aims to detect instances of a class (e.g. cars, bicyles, humans, etc.) in an images or videos. Check this post to learn more about Object Detection, the different sub-tasks as well as available model architecture commonly used - link. . This article show how we can quickly perform Object Detection on our own set of images but leveraging freely available models from TF Hub which where pre-trained on this task. . We will be using Faster R-CNN which has a very complex architecture and can be backed by diffrent type of architectures (e.g. VGG). The follow diagram illustrates at a very high level the model architecture, to learn more about this model check this article to learn more about this model - link. . . In our case we will use an Inception V2-backed model similarly to the original architecture in the Faster R-CNN paper. The model we will use is pretrained on the huge COCO dataset and available on TF Hub. . First, we need to downlad TensorFlow Object Detection API and install it. We need to do this to use some of the utility functions provided by this API to quickly visualize the output of Object Detection, i.e. the image along with the detected instances and their bounding boxes. . %%capture %%bash git clone --depth 1 https://github.com/tensorflow/models cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install . . Now we can import TF Hub and TF Object Detection APIs as well as all the other packages we&#39;ll be needing . import glob from io import BytesIO import matplotlib.pyplot as plt import numpy as np import tensorflow as tf import tensorflow_hub as hub from PIL import Image from object_detection.utils import visualization_utils from object_detection.utils.label_map_util import create_category_index_from_labelmap %matplotlib inline . Let&#39;s download the Fatser R-CNN model from TF Hub . MODEL_PATH = (&#39;https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1&#39;) model = hub.load(MODEL_PATH) . To be able to map the output of the model to some meaninful class name, we need to load COCO&#39;s category index as follows . labels_path = &#39;models/research/object_detection/data/mscoco_label_map.pbtxt&#39; CATEGORY_IDX = create_category_index_from_labelmap(labels_path) . We need to define a utility function to load the test image and process them before passing them to the model . def load_image(path): image_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(image_data)) width, height = image.size image = np.array(image.getdata()) image = image.reshape((1, height, width, 3)).astype(&#39;uint8&#39;) return image . The following function will be used to load an image, run the Faster R-CNN model on it and return an image on which the instances are identified with bounding boxes . def get_image_with_boxes(model, image_path): image = load_image(image_path) results = model(image) # Convert the results to NumPy arrays model_output = {k: v.numpy() for k, v in results.items()} # Create a visualization of the detected instances with their boxes, scores, and classes boxes = model_output[&#39;detection_boxes&#39;][0] classes = model_output[&#39;detection_classes&#39;][0].astype(&#39;int&#39;) scores = model_output[&#39;detection_scores&#39;][0] image_with_boxes = image.copy()[0] # draw boxes on the output image, along with the classes and scores visualization_utils.visualize_boxes_and_labels_on_image_array( image=image_with_boxes, boxes=boxes, classes=classes, scores=scores, category_index=CATEGORY_IDX, use_normalized_coordinates=True, max_boxes_to_draw=200, min_score_thresh=0.30, agnostic_mode=False, line_thickness=5 ) return image_with_boxes . Get some images for testing . %%bash mkdir -p images curl -s -o images/bicycle1.jpg https://cdn.pixabay.com/photo/2016/11/30/12/29/bicycle-1872682_960_720.jpg curl -s -o images/bicycle2.jpg https://cdn.pixabay.com/photo/2016/11/22/23/49/cyclists-1851269_960_720.jpg curl -s -o images/animal1.jpg https://cdn.pixabay.com/photo/2014/05/20/21/20/bird-349026_960_720.jpg curl -s -o images/animal2.jpg https://cdn.pixabay.com/photo/2018/05/27/18/19/sparrows-3434123_960_720.jpg curl -s -o images/car1.jpg https://cdn.pixabay.com/photo/2016/02/13/13/11/oldtimer-1197800_960_720.jpg curl -s -o images/car2.jpg https://cdn.pixabay.com/photo/2016/09/11/10/02/renault-juvaquatre-1661009_960_720.jpg . Call the previous get_image_with_boxes function on each image . images = [] image_paths = glob.glob(&#39;images/*&#39;) for path in image_paths: image_with_annotation = get_image_with_boxes(model, path) images.append(image_with_annotation) . Finally we plot the output images and detected bounding boxes, note the quality of the predictions of this model . figure, axis = plt.subplots(2, 3, figsize=(35, 15)) for index, image in enumerate(images): row, col = int(index / 3), index % 3 axis[row, col].imshow(image) axis[row, col].axis(&#39;off&#39;) . We have seen how we can leverage TF Hub and easily use out of box pretrained model with a very complex architecture to perform a very complex task such as Object Detection. Implementing and training Faster R-CNN is not an easy task, but thanks to TF Hub we can effortlessly use the result of the hard work of the Deep Learning community in building and training such models. . . Note: that we were using labels defined in the COCO dataset on which the model was pretrained. If we want to perform Object Detection on our custom dataset or different labels we would need to retrain the model which we will see in another article. .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/object-detection/2021/02/11/Object_Detection_TFHub.html",
            "relUrl": "/tensorflow/vision/object-detection/2021/02/11/Object_Detection_TFHub.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "Adversarial attack with Fast Gradient Signed Method",
            "content": "Deep Learning have achieved remarquable results in many areas especially vision where today complex models trained on very large datasets are able to overperform domain experts. Yet those same models can be very vulnerable to attacks that slightly change the input data to fool the model. Those attacks are know as Adversial Attacks and can be performed in a variety of ways (see link1, link2). . Those attacks can be serious with devastating results (e.g. fooling a self-driving), so known how to check that your model is robust against them is crutial before rolling the model to production. . . In this article, we will see how to implement one simple and effective Adversial Attack called Fast Gradient Signed Method (FGSM). . FGSM is implemented by performing the following steps: . Pass an legitimage image over a model and capture the gradients | Determining the direction (sign) of the gradient at each pixel of the original image | Using that information create an adversarial pattern in a way to aximize the loss at each pixel | Multiply this pattern by a small scaling value and apply it (i.e. add or substruct) to the original image | The result is an Adversial image that looks to the human eye very similar to the original image | . Although this attack performs small imperceptible perturbations into an image, we will see in action how this technique can easily fool a pre-trained model. . Before starting, make sure to have OpenCV installed as it will be used for performing the perturbations . #collapse !pip install opencv-contrib-python . . Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30) Requirement already satisfied: numpy&gt;=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5) . Import the dependencies . #collapse import cv2 import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.applications.nasnet import * from tensorflow.keras.losses import CategoricalCrossentropy . . Load the pre-trained model (in this case NASNetMobile) and freeze its weights so they are not updated after a forward pass. . pretrained_model = NASNetMobile(include_top=True, weights=&#39;imagenet&#39;) pretrained_model.trainable = False . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile.h5 24231936/24227760 [==============================] - 0s 0us/step 24240128/24227760 [==============================] - 0s 0us/step . We need to define a function to preprocess an image with same preprocessing steps performed on the images used to train the target model (e.g. resizing) . def preprocess(image, target_shape): image = tf.cast(image, tf.float32) image = tf.image.resize(image, target_shape) image = preprocess_input(image) image = image[None, :, :, :] return image . We also need to define a function that turns the model output probabilities into actual labels . def get_imagenet_label(probabilities): return decode_predictions(probabilities, top=1)[0][0] . Define a function to create the adversarial pattern that will be used later to generate the adversial image . def generate_adv_pattern(model, input_image, input_label, loss_function): with tf.GradientTape() as tape: tape.watch(input_image) prediction = model(input_image) loss = loss_function(input_label, prediction) gradient = tape.gradient(loss, input_image) signed_gradient = tf.sign(gradient) return signed_gradient . The pattern is nothing but a tensor with the sign of the gradient in each element. i.e., signed_gradient is: . -1 for negative gradients (i.e. value below 0) | 1 for positive gradients (i.e. value above 0) | 0 if the gradient is 0. | . We need a legitimate image so we can apply perturbations on it, you can download a test image on your own or use this Pug image . !curl https://image.freepik.com/free-vector/angry-dog-pug-prisoner-graphic-illustration_41984-29.jpg -o dog.jpg . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 102k 100 102k 0 0 1138k 0 --:--:-- --:--:-- --:--:-- 1138k . Pass the image through the pretrained model to get output probabilities . image = tf.io.read_file(&#39;dog.jpg&#39;) image = tf.image.decode_jpeg(image) image = preprocess(image, pretrained_model.input.shape[1:-1]) image_probs = pretrained_model.predict(image) . We need to One-hot encode the ground truth label of the original image, then use it to generate an adversarial pattern . cce_loss = CategoricalCrossentropy() pug_index = 254 label = tf.one_hot(pug_index, image_probs.shape[-1]) label = tf.reshape(label, (1, image_probs.shape[-1])) disturbances = generate_adv_pattern(pretrained_model, image, label, cce_loss) . . Note: if you want try this techniques on a different image of a different class than pug then look up the corresponding class index in this imagenet class indices - link . Next, we define a utility function to generate an adversial version of an image based on a disturbance/noise and a scalar . def generate_adv_image(image, epsilon, disturbances): corrupted_image = image + epsilon * disturbances return tf.clip_by_value(corrupted_image, -1, 1) . We also, need another utility function to ensure before plotting that the input tensor in the [0, 255] range, as well as in BGR space, which is the one used by OpenCV . def postprocess_adv_image(adv_image): adv_image = adv_image.numpy()[0] * 0.5 + 0.5 adv_image = (adv_image * 255).astype(&#39;uint8&#39;) return cv2.cvtColor(adv_image, cv2.COLOR_RGB2BGR) . We can use the previous utility function to show what the noise looks like before applying it to the original image . disturbed_image = postprocess_adv_image(disturbances) plt.imshow(disturbed_image) plt.axis(&#39;off&#39;) plt.title(&quot;noise&quot;); . Finally, we can put all the previous steps together to perform a series of adversarial attacks different values of epsilon (which is used as noise multiplier). . After that we disply the corresponding adversial example as well the predicted label and confidence provided by the victim model . figure, axis = plt.subplots(2, 3, figsize=(15, 8)) for index, epsilon in enumerate([0, 0.005, 0.01, 0.1, 0.15, 0.2]): adv_image = generate_adv_image(image, epsilon, disturbances) prediction = pretrained_model.predict(adv_image) _, label, confidence = get_imagenet_label(prediction) adv_image = postprocess_adv_image(adv_image) confidencePct = confidence * 100 title = f&#39;Epsilon = {epsilon:.3f}, {label} ({confidencePct:.2f}%)&#39; row, col = int(index / 3), index % 3 axis[row, col].imshow(adv_image) axis[row, col].set_title(title) axis[row, col].axis(&#39;off&#39;) . Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json 40960/35363 [==================================] - 0s 0us/step 49152/35363 [=========================================] - 0s 0us/step . As you can be see from the output images and the resulting classification, an imperceptible variation in the pixel values produced a drastically different response from the network. . For epsilon = 0 (no attack), the label is Pug with a 60.18% confidence | When epsilon = 0.005 (a very small perturbation), the label changes to Muzzle, with a 98.46% confidence! | The situation gets worse as we increase the magnitude of epsilon. | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/generative/adversarial/2021/01/29/Gradient-Signed-Method.html",
            "relUrl": "/tensorflow/generative/adversarial/2021/01/29/Gradient-Signed-Method.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "DeepDream with Tensorflow",
            "content": "In an effort to understand the inners of deep neural networks and what information those models learn after been trained on a specific task, a team at Google come up with what&#39;s know today as DeepDream. The experiment results were interesting, it appears that neural networks trained to classify an image (i.e. discriminate between different classes) contains enough information to generate new images that can be artistic. . . In this notebook we will implement DeepDream in Tensorflow from scratch and test it on couple of images. . The DeepDream model is simply built on a backbone model trained on imagenet and the output of the model will be few activation layers picked from this backbone model. Then we run an image through this model, compute the gradients with respect to the activations of those output layers, then modify the original image to increase the magnitude of the activations which as a result will magnify the patterns in the image. . In our case, the backbone model is Inception V3 (which you can read more about it here). The following driagram shows an overview of this model architecture: . . #collapse import numpy as np import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.applications.inception_v3 import * from tensorflow.keras.preprocessing.image import * from tqdm import tqdm import matplotlib.pyplot as plt . . To create the DeepDream model, we define the following helper function that uses InceptionV3 from TF Hub and uses the input layers as output of the model. Note that by default we are picking ramdom activation layers from the InceptionV3 model. . def create_model(layers=None): if not layers: layers = [&#39;mixed3&#39;, &#39;mixed5&#39;] base = InceptionV3(weights=&#39;imagenet&#39;, include_top=False) outputs = [base.get_layer(name).output for name in layers] return Model(base.input, outputs) . We need to define few utils functions that we will use to process the images, for example scaling an image by a factor or converting tensor image into a numpy array. . def convert_tensor_to_nparray(image): image = 255 * (image + 1.0) / 2.0 image = tf.cast(image, tf.uint8) image = np.array(image) return image def scale_image(image, base_shape, scale, factor): new_shape = tf.cast(base_shape * (scale **factor), tf.int32) image = tf.image.resize(image,new_shape).numpy() image = preprocess_input(image) image = tf.convert_to_tensor(image) return image . Next, we define a function to calculate the loss which is simly the average of the activations resulting from doing a forward pass with the input image. . def calculate_loss(model, image): image_batch = tf.expand_dims(image, axis=0) activations = model(image_batch) if len(activations) == 1: activations = [activations] losses = [] for activation in activations: loss = tf.math.reduce_mean(activation) losses.append(loss) total_loss = tf.reduce_sum(losses) return total_loss . To calculate the gradients, we need to perform a forward pass inside a tf.GradientTape(), after that we simply update the image to maximize the activations in the next run. . Note how we are using the tf.function annotation which will improve the performance significantly. . @tf.function def forward_pass(model, image, steps, step_size): loss = tf.constant(0.0) for _ in range(steps): with tf.GradientTape() as tape: tape.watch(image) loss = calculate_loss(model, image) gradients = tape.gradient(loss, image) gradients /= tf.math.reduce_std(gradients) + 1e-8 image = image + gradients * step_size image = tf.clip_by_value(image, -1, 1) return image, loss . All the previous functions are combined and used in the following funciton which will take an input image and a model, and construct the final dreaming looking picture. . The other input parameters to this function have the following purpose: . octave_scale the scale by which we&#39;ll increase the size of an image | octave_power_factors the factor that will be applied as a power to the previous scale parameter. | steps the number of iteration we run the image over the deepdream model | step_size will be used to scale the gradients before adding them to the image | . def dream(dreamer_model, image, octave_scale=1.30, octave_power_factors=None, steps=100, step_size=0.01): if not octave_power_factors: octave_power_factors = [*range(-2, 3)] image = tf.constant(np.array(image)) base_shape = tf.shape(image)[:-1] base_shape = tf.cast(base_shape, tf.float32) steps = tf.constant(steps) step_size = tf.constant(tf.convert_to_tensor(step_size)) for factor in octave_power_factors: image = scale_image(image, base_shape, octave_scale, factor) image, _ = forward_pass(dreamer_model, image, steps, step_size) image = convert_tensor_to_nparray(image) base_shape = tf.cast(base_shape, tf.int32) image = tf.image.resize(image, base_shape) image = tf.image.convert_image_dtype(image /255.0,dtype=tf.uint8) image = np.array(image) return np.array(image) . Now we can apply this DeepDream model to an image, you can pick any one you like. . !curl https://miro.medium.com/max/1750/1*E-S7Y80jIFuZ03xyc89fnA.jpeg -o image.jpeg . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 68250 100 68250 0 0 812k 0 --:--:-- --:--:-- --:--:-- 812k . def load_image(path): image = load_img(path) image = img_to_array(image) return image . def show_image(image): plt.imshow(image) plt.show() . original_image = load_image(&#39;image.jpeg&#39;) show_image(original_image / 255.0) . First, lets try the image with all default parameters, and activation layers . model = create_model() output_image = dream(model, original_image) show_image(output_image) . WARNING:tensorflow:5 out of the last 5 calls to &lt;function forward_pass at 0x7f095a587a70&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. . Let&#39;s try with different activation layers . Note: the first layers tend to learn basic patterns (e.g. lines and shapes), while layers closer to the output learn more complex patterns as they combine the previous basic patterns. . model = create_model([&#39;mixed2&#39;, &#39;mixed5&#39;, &#39;mixed7&#39;]) output_image = dream(model, original_image) show_image(output_image) . WARNING:tensorflow:6 out of the last 6 calls to &lt;function forward_pass at 0x7f095a587a70&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. . The result is a softer image as a result of adding more layers. . Finally, let&#39;s try some custom octaves power factors . model = create_model() output_image = dream(model, original_image, octave_power_factors=[-3, -1, 0, 3]) show_image(output_image) . The resulting image seem to have less noise and more heterogeneous patterns, a mixture of both high- and low-level patterns, as well as a better color distribution. . As an exercise, try different parameters and you will see that the results vary widely: . Play with different step_size values, a big value will result in much noise added to the original images | Use higher layers to obtain pictures with less noise and more nuanced patterns. | Use more octaves, which will result into more images passed to the model at different scales. | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/artistic/generative/2021/01/18/DeepDream-Tensorflow.html",
            "relUrl": "/tensorflow/artistic/generative/2021/01/18/DeepDream-Tensorflow.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "Handling Imbalanced Datasets",
            "content": ". %%capture %%bash pip install imbalanced-learn . from collections import Counter import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import make_classification from sklearn.svm import LinearSVC from imblearn.pipeline import make_pipeline from imblearn.base import BaseSampler from imblearn.under_sampling import RandomUnderSampler from imblearn.over_sampling import (SMOTE, RandomOverSampler) from imblearn.combine import SMOTEENN, SMOTETomek import warnings warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) . Helper functions . The following function will be used to create toy dataset. It using the make_classification from scikit-learn but fixing some parameters. . def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3, class_sep=0.8, n_clusters=1): return make_classification(n_samples=n_samples, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=n_classes, n_clusters_per_class=n_clusters, weights=list(weights), class_sep=class_sep, random_state=0) . The following function will be used to plot the sample space after resampling to illustrate the characterisitic of an algorithm. . def plot_resampling(X, y, sampling, ax): X_res, y_res = sampling.fit_resample(X, y) ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor=&#39;k&#39;) # make nice plotting ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.get_xaxis().tick_bottom() ax.get_yaxis().tick_left() ax.spines[&#39;left&#39;].set_position((&#39;outward&#39;, 10)) ax.spines[&#39;bottom&#39;].set_position((&#39;outward&#39;, 10)) return Counter(y_res) . The following function will be used to plot the decision function of a classifier given some data. . def plot_decision_function(X, y, clf, ax): plot_step = 0.02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, alpha=0.4) ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor=&#39;k&#39;) . Influence of the balancing ratio . We will first illustrate the influence of the balancing ratio on some toy data using a linear SVM classifier. Greater is the difference between the number of samples in each class, poorer are the classfication results. . #collapse fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) ax_arr = (ax1, ax2, ax3, ax4) weights_arr = ((0.01, 0.01, 0.98), (0.01, 0.05, 0.94), (0.2, 0.1, 0.7), (0.33, 0.33, 0.33)) for ax, weights in zip(ax_arr, weights_arr): X, y = create_dataset(n_samples=1000, weights=weights) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax) ax.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) fig.tight_layout() . . Under-sampling . Under-sampling by selecting existing samples . There are two major groups of selection algorithms: . the controlled under-sampling methods and | the cleaning under-sampling methods. | With the controlled under-sampling methods, the number of samples to be selected can be specified. RandomUnderSampler is the most naive way of performing such selection by randomly selecting a given number of samples by the targetted class. . #collapse fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6)) X, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax1) ax1.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) sampler = RandomUnderSampler(random_state=0) clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_decision_function(X, y, clf, ax2) ax2.set_title(&#39;Decision function for {}&#39;.format(sampler.__class__.__name__)) plot_resampling(X, y, sampler, ax3) ax3.set_title(&#39;Resampling using {}&#39;.format(sampler.__class__.__name__)) fig.tight_layout() . . Over-sampling . Random over-sampling . Random over-sampling with RandomOverSampler can be used to repeat some samples and balance the number of samples between the dataset. It can be seen that with this trivial approach the boundary decision is already less biaised toward the majority class. . #collapse fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7)) X, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94)) clf = LinearSVC().fit(X, y) plot_decision_function(X, y, clf, ax1) ax1.set_title(&#39;Linear SVC with y={}&#39;.format(Counter(y))) pipe = make_pipeline(RandomOverSampler(random_state=0), LinearSVC()) pipe.fit(X, y) plot_decision_function(X, y, pipe, ax2) ax2.set_title(&#39;Decision function for RandomOverSampler&#39;) fig.tight_layout() . . /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) . More advanced over-sampling . Instead of repeating the same samples when over-sampling, we can use some specific heuristic instead like SMOTE or ADASYN. . #collapse # Make an identity sampler for illustrations class FakeSampler(BaseSampler): _sampling_type = &#39;bypass&#39; def _fit_resample(self, X, y): return X, y fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15)) X, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94)) sampler = FakeSampler() clf = make_pipeline(sampler, LinearSVC()) plot_resampling(X, y, sampler, ax1) ax1.set_title(&#39;Original data - y={}&#39;.format(Counter(y))) ax_arr = (ax2, ax3, ax4) for ax, sampler in zip(ax_arr, (RandomOverSampler(random_state=0), SMOTE(random_state=0), ADASYN(random_state=0))): clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_resampling(X, y, sampler, ax) ax.set_title(&#39;Resampling using {}&#39;.format(sampler.__class__.__name__)) fig.tight_layout() . . Illustration of the sample generation in the over-sampling algorithm . #collapse rng = np.random.RandomState(18) f, ax = plt.subplots(1, 1, figsize=(8, 8)) # generate some data points y = np.array([3.65284, 3.52623, 3.51468, 3.22199, 3.21]) z = np.array([0.43, 0.45, 0.6, 0.4, 0.211]) y_2 = np.array([3.3, 3.6]) z_2 = np.array([0.58, 0.34]) # plot the majority and minority samples ax.scatter(z, y, label=&#39;Minority class&#39;, s=100) ax.scatter(z_2, y_2, label=&#39;Majority class&#39;, s=100) idx = rng.randint(len(y), size=2) annotation = [r&#39;$x_i$&#39;, r&#39;$x_{zi}$&#39;] for a, i in zip(annotation, idx): ax.annotate(a, (z[i], y[i]), xytext=tuple([z[i] + 0.01, y[i] + 0.005]), fontsize=15) # draw the circle in which the new sample will generated radius = np.sqrt((z[idx[0]] - z[idx[1]]) ** 2 + (y[idx[0]] - y[idx[1]]) ** 2) circle = plt.Circle((z[idx[0]], y[idx[0]]), radius=radius, alpha=0.2) ax.add_artist(circle) # plot the line on which the sample will be generated ax.plot(z[idx], y[idx], &#39;--&#39;, alpha=0.5) # create and plot the new sample step = rng.uniform() y_gen = y[idx[0]] + step * (y[idx[1]] - y[idx[0]]) z_gen = z[idx[0]] + step * (z[idx[1]] - z[idx[0]]) ax.scatter(z_gen, y_gen, s=100) ax.annotate(r&#39;$x_{new}$&#39;, (z_gen, y_gen), xytext=tuple([z_gen + 0.01, y_gen + 0.005]), fontsize=15) # make the plot nicer with legend and label ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.get_xaxis().tick_bottom() ax.get_yaxis().tick_left() ax.spines[&#39;left&#39;].set_position((&#39;outward&#39;, 10)) ax.spines[&#39;bottom&#39;].set_position((&#39;outward&#39;, 10)) ax.set_xlim([0.2, 0.7]) ax.set_ylim([3.2, 3.7]) plt.xlabel(r&#39;$X_1$&#39;) plt.ylabel(r&#39;$X_2$&#39;) plt.legend() plt.tight_layout() plt.show() . . Automatically created module for IPython interactive environment . Combine-sampling . Comparison of the combination of over- and under-sampling algorithms . This example shows the effect of applying an under-sampling algorithms after SMOTE over-sampling. In the literature, Tomek&#39;s link SMOTETomek and edited nearest neighbours SMOTEENN are the two methods which have been used and are available in imbalanced-learn. . #collapse fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 25)) X, y = create_dataset(n_samples=1000, weights=(0.1, 0.2, 0.7)) ax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6)) for ax, sampler in zip(ax_arr, ( SMOTE(random_state=0), SMOTEENN(random_state=0), SMOTETomek(random_state=0))): clf = make_pipeline(sampler, LinearSVC()) clf.fit(X, y) plot_decision_function(X, y, clf, ax[0]) ax[0].set_title(&#39;Decision function for {}&#39;.format( sampler.__class__.__name__)) plot_resampling(X, y, sampler, ax[1]) ax[1].set_title(&#39;Resampling using {}&#39;.format( sampler.__class__.__name__)) fig.tight_layout() plt.show() . . .",
            "url": "https://dzlab.github.io/notebooks/sklearn/classification/sampling/2020/10/31/Handling_Imbalanced_Datasets.html",
            "relUrl": "/sklearn/classification/sampling/2020/10/31/Handling_Imbalanced_Datasets.html",
            "date": " • Oct 31, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Preprocessing Structured data in TF 2.3",
            "content": "In TF 2.3, Keras adds new preprocessing layers for image, text and strucured data. The following notebook explores those new layers for dealing with Structured data. . For a complete example of how to use the new preprocessing layer for Structured data check the Keras example - link. . Structured data . Generate some random data for playing with and seeing what is the output of the preprocessing layers. . xdf = pd.DataFrame({ &#39;categorical_string&#39;: [&#39;LOW&#39;, &#39;HIGH&#39;, &#39;HIGH&#39;, &#39;MEDIUM&#39;], &#39;categorical_integer_1&#39;: [1, 0, 1, 0], &#39;categorical_integer_2&#39;: [1, 2, 3, 4], &#39;numerical_1&#39;: [2.3, 0.2, 1.9, 5.8], &#39;numerical_2&#39;: [16, 32, 8, 60] }) ydf = pd.DataFrame({&#39;target&#39;: [0, 0, 0, 1]}) ds = tf.data.Dataset.from_tensor_slices((dict(xdf), ydf)) for x, y in ds.take(1): print(&#39;X:&#39;, x) print(&#39;y:&#39;, y) . X: {&#39;categorical_string&#39;: &lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;cat1&#39;&gt;, &#39;categorical_integer_1&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;, &#39;categorical_integer_2&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;, &#39;numerical_1&#39;: &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.3&gt;, &#39;numerical_2&#39;: &lt;tf.Tensor: shape=(), dtype=int64, numpy=16&gt;} y: tf.Tensor([0], shape=(1,), dtype=int64) . from tensorflow.keras.layers.experimental.preprocessing import Normalization from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding from tensorflow.keras.layers.experimental.preprocessing import StringLookup . Pre-processing Numercial columns . Preprocessing helper function to encode numercial features, e.g. 0.1, 0.2, etc. . def create_numerical_encoder(dataset, name): # Create a Normalization layer for our feature normalizer = Normalization() # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the statistics of the data normalizer.adapt(feature_ds) return normalizer . # Apply normalization to a numerical feature normalizer = create_numerical_encoder(ds, &#39;numerical_1&#39;) normalizer.apply(xdf[name].values) . &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy= array([[-0.7615536], [-1.2528784], [-0.7615536], [-1.2528784]], dtype=float32)&gt; . Pre-processing Integer categorical columns . Preprocessing helper function to encode integer categorical features, e.g. 1, 2, 3 . def create_integer_categorical_encoder(dataset, name): # Create a CategoryEncoding for our integer indices encoder = CategoryEncoding(output_mode=&quot;binary&quot;) # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the space of possible indices encoder.adapt(feature_ds) return encoder . # Apply one-hot encoding to an integer categorical feature encoder1 = create_integer_categorical_encoder(ds, &#39;categorical_integer_1&#39;) encoder1.apply(xdf[&#39;categorical_integer_1&#39;].values) . &lt;tf.Tensor: shape=(4, 2), dtype=float32, numpy= array([[0., 1.], [1., 0.], [0., 1.], [1., 0.]], dtype=float32)&gt; . # Apply one-hot encoding to an integer categorical feature encoder2 = create_integer_categorical_encoder(ds, &#39;categorical_integer_2&#39;) encoder2.apply(xdf[&#39;categorical_integer_2&#39;].values) . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]], dtype=float32)&gt; . Pre-processing String categorical columns . Preprocessing helper function to encode string categorical features, e.g. LOW, HIGH, MEDIUM. . This will applying the following to the input feature: . Create a token to index lookup table | Apply one-hot encoding to the tokens indices | def create_string_categorical_encoder(dataset, name): # Create a StringLookup layer which will turn strings into integer indices index = StringLookup() # Prepare a Dataset that only yields our feature feature_ds = dataset.map(lambda x, y: x[name]) feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1)) # Learn the set of possible string values and assign them a fixed integer index index.adapt(feature_ds) # Create a CategoryEncoding for our integer indices encoder = CategoryEncoding(output_mode=&quot;binary&quot;) # Prepare a dataset of indices feature_ds = feature_ds.map(index) # Learn the space of possible indices encoder.adapt(feature_ds) return index, encoder . # Apply one-hot encoding to an integer categorical feature indexer, encoder3 = create_string_categorical_encoder(ds, &#39;categorical_string&#39;) # Turn the string input into integer indices indices = indexer.apply(xdf[&#39;categorical_string&#39;].values) # Apply one-hot encoding to our indices encoder3.apply(indices) . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[0., 0., 0., 0., 1.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.]], dtype=float32)&gt; . Notice that the string categorical column was hot encoded into 5 tokens whereas in the input dataframe there is only 3 unique values. This is because the indexer adds 2 more tokens. See the vocabulary: . indexer.get_vocabulary() . [&#39;&#39;, &#39;[UNK]&#39;, &#39;cat2&#39;, &#39;cat3&#39;, &#39;cat1&#39;] .",
            "url": "https://dzlab.github.io/notebooks/2020/08/02/Preprocessing_structured_data_in_TF_2_3.html",
            "relUrl": "/2020/08/02/Preprocessing_structured_data_in_TF_2_3.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Annotation with TensorFlow Object Detection API",
            "content": "import matplotlib import matplotlib.pyplot as plt import numpy as np from PIL import Image from six import BytesIO from pathlib import Path import tensorflow as tf %matplotlib inline . Install Object Detection API . !git clone --depth 1 https://github.com/tensorflow/models . Cloning into &#39;models&#39;... remote: Enumerating objects: 2797, done. remote: Counting objects: 100% (2797/2797), done. remote: Compressing objects: 100% (2439/2439), done. remote: Total 2797 (delta 563), reused 1405 (delta 322), pack-reused 0 Receiving objects: 100% (2797/2797), 57.73 MiB | 31.67 MiB/s, done. Resolving deltas: 100% (563/563), done. . # Install the Object Detection API %%bash cd models/research/ protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . python -m pip install -q . . object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used. . from object_detection.utils import colab_utils from object_detection.utils import visualization_utils as viz_utils . Download data for annotation . Download an image dataset to annotate, for instance The Oxford-IIIT Pet Dataset (link) . %%bash curl -O https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz tar xzf images.tar.gz . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 755M 100 755M 0 0 29.8M 0 0:00:25 0:00:25 --:--:-- 31.4M . paths = list([str(p) for p in Path(&#39;images&#39;).glob(&#39;*&#39;)]) . Utility method to load an image from path into a uint8 numpy array with shape (height, width, channels), where channels=3 for RGB. . def load_image_into_numpy_array(path): img_data = tf.io.gfile.GFile(path, &#39;rb&#39;).read() image = Image.open(BytesIO(img_data)) (im_width, im_height) = image.size image_np = np.array(image.getdata(), dtype=np.uint8) return image_np.reshape((im_height, im_width, 3)) . For testing select a random subset of the images (we don&#39;t want load all images) . sample_size = 10 sample_paths = [paths[np.random.randint(len(paths))] for i in range(10)] . Annotate images . Load the selected random images into numpy arrays . images_np = [load_image_into_numpy_array(str(p)) for p in sample_paths] . boxes = [] colab_utils.annotate(images_np, box_storage_pointer=boxes) . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . &#39;--boxes array populated--&#39; . Define the indexes for the categories . category_index = { 0: {&#39;id&#39;: 0, &#39;name&#39;: &#39;dog&#39;}, 1: {&#39;id&#39;: 1, &#39;name&#39;: &#39;cat&#39;} } . Inspect the annotations . Wrapper function to visualize the original image along with the best detected box. It takes are arguments: . image_np: uint8 numpy array with shape (img_height, img_width, 3) | boxes: a numpy array of shape [N, 4] | classes: a numpy array of shape [N]. Note that class indices are 1-based, and match the keys in the label map. | scores: a numpy array of shape [N] or None. If scores=None, then this function assumes that the boxes to be plotted are groundtruth boxes and plot all boxes as black with no classes or scores. | category_index: a dict containing category dictionaries (each holding category index id and category name name) keyed by category indices. | figsize: (optional) size for the figure. | image_name: (optional) name for the image file. | . def plot_detections(image_np, boxes, classes, scores, category_index, figsize=(12, 16), image_name=None): image_np_with_annotations = image_np.copy() viz_utils.visualize_boxes_and_labels_on_image_array( image_np_with_annotations, boxes, classes, scores, category_index, use_normalized_coordinates=True, min_score_thresh=0.8) if image_name: plt.imsave(image_name, image_np_with_annotations) else: plt.imshow(image_np_with_annotations) . I manually inspected the images (that&#39;s the 100% scores below) to get the class for each one, note that: . 0 is for a cat image | 1 is for a dog image | . classes = [ np.ones(shape=(1), dtype=np.int32), np.ones(shape=(1), dtype=np.int32), np.zeros(shape=(1), dtype=np.int32), np.ones(shape=(1), dtype=np.int32), np.zeros(shape=(1), dtype=np.int32) ] # give boxes a score of 100% scores = np.array([1.0], dtype=np.float32) . Vizualise the images with their bounding boxes . plt.figure(figsize=(30, 15)) for idx in range(5): plt.subplot(2, 3, idx+1) plot_detections(images_np[idx], boxes[idx], classes[idx], scores, category_index) plt.show() .",
            "url": "https://dzlab.github.io/notebooks/2020/07/19/Image_Annotation_on_Colab.html",
            "relUrl": "/2020/07/19/Image_Annotation_on_Colab.html",
            "date": " • Jul 19, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Captum PyTorch Vision Example",
            "content": "Captum (translates to comprehension in Latin) is an open source library for model interpretability. It helps model developers understand which features are contributing to their model’s output. It implements state-of-the-art interpretability algorithms in PyTorch, and provide them as an easy to use API. . The rest of this notebook illustrates how to use this library to interpret a fastai v2 based image classification model. . Setup . %%capture %%bash pip install fastai2 pip install psutil pip install captum . from matplotlib.colors import LinearSegmentedColormap from fastai2.vision.all import * from captum.attr import IntegratedGradients from captum.attr import GradientShap from captum.attr import Occlusion from captum.attr import NoiseTunnel from captum.attr import visualization as viz . Data . Download data for training an image classification model . path = untar_data(URLs.PETS)/&#39;images&#39; imgs = get_image_files(path) def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, imgs, valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) . Model . Fine tune an Imagenet-based model on the new images dataset. . learn = cnn_learner(dls, resnet34, metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.169865 | 0.020558 | 0.004736 | 00:23 | . epoch train_loss valid_loss error_rate time . 0 | 0.060073 | 0.026544 | 0.008119 | 00:24 | . Basic interpertation of the model prediected classes vs. actual ones. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Visualize top losses, e.g. to check if the images themselves are correctly annotated. . interp.plot_top_losses(5, nrows=1) . Store/Restore the fine tuned model . learn.export(&#39;/tmp/model.pkl&#39;) learn_inf = load_learner(&#39;/tmp/model.pkl&#39;) . Select a random image and plot it . idx = random.randint(0, len(imgs)) . image = PILImage.create(imgs[idx]) image . image = learn_inf.dls.after_item(image) image = learn_inf.dls.after_batch(image) . pred,pred_idx,probs = learn_inf.predict(imgs[idx]) pred, pred_idx, probs . (&#39;False&#39;, tensor(0), tensor([9.9998e-01, 2.0485e-05])) . Interpretability . Let&#39;s use Captum.ai to interpret the model predictions and to have a visual on where the network focused more in the input image. . Gradient-based attribution . Integrated Gradients is an interpretaility technique based on the approximation of integral gradients. The basic implementation works as followss: . Given as input target image and a baseline image (usually a black image), generate multiple images between both starting from darker to lighter. | Do forward pass with each of those images to predict a class and calculate the gradient. | Approximate the integral of the gradients of all those images | . The following example, illustrates how to use Captum IntegratedGradients to compute the attributions using Integrated Gradients and visualize them on the target image. . integrated_gradients = IntegratedGradients(learn_inf.model) attr_ig = integrated_gradients.attribute(image, target=pred_idx, n_steps=200) . transposed_attr_ig = np.transpose(attr_ig.squeeze().numpy(), (1,2,0)) transposed_image = np.transpose(image.squeeze().numpy(), (1,2,0)) . default_cmap = LinearSegmentedColormap.from_list(&#39;custom blue&#39;, [(0, &#39;#ffffff&#39;), (0.25, &#39;#000000&#39;), (1, &#39;#000000&#39;)], N=256) _ = viz.visualize_image_attr(transposed_attr_ig, transposed_image, method=&#39;heat_map&#39;, cmap=default_cmap, show_colorbar=True, sign=&#39;positive&#39;, outlier_perc=1) . For a better visual of the attribution, the images between baseline and target are sampled using a noise tunnel (by adding gaussian noise). And when the gradients are calulcated, we smoothe them by calculating their mean squared. . noise_tunnel = NoiseTunnel(integrated_gradients) attributions_ig_nt = noise_tunnel.attribute(image, n_samples=10, nt_type=&#39;smoothgrad_sq&#39;, target=pred_idx) transposed_attr_ig_nt = np.transpose(attributions_ig_nt.squeeze().numpy(), (1,2,0)) _ = viz.visualize_image_attr_multiple(transposed_attr_ig_nt, transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], cmap=default_cmap, show_colorbar=True) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Occlusion-based attribution . Occlusion-based attribution is a different interepretation technique based on perturbing parts of the original image (e.g. by zeroing) and computing how this affects the model decision. This technique is implemented by: . Slide a window of shape (h, w) on the target image with a stride s | Replace the window with a baseline (e.g. with black) and qunatify the effect on model decision. | Repeat previous steps until all of the target image is covered. | . Simiarly to convolution, this technique can become very slow when used in large models and large input images. . As a first exercise, we run a sliding window of size 15x15 and a stride of 8 along both image dimensions. For each window, we occlude the image with a baseline value of 0. . occlusion = Occlusion(learn_inf.model) attr_occ = occlusion.attribute(image, strides = (3, 8, 8), target=pred_idx, sliding_window_shapes=(3,15, 15), baselines=0) . _ = viz.visualize_image_attr_multiple(np.transpose(attr_occ.squeeze().numpy(), (1,2,0)), transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], show_colorbar=True, outlier_perc=2, ) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Let&#39;s try different window shape and sliding window and visuzalize the result, by rescaling back to the shape of the original image. . Experimenting with different windows is important because: . Different shape may lead to a significaly different result. | Larger windows is useful when the input image presents some local coherence. | . In this case, we run a sliding window of size 60x60 and a stride of 50 along both image dimensions. For each window, we occlude the image with a baseline value of 0. . occlusion = Occlusion(learn_inf.model) attr_occ = occlusion.attribute(image, strides = (3, 50, 50), target=pred_idx, sliding_window_shapes=(3,60, 60), baselines=0) _ = viz.visualize_image_attr_multiple(np.transpose(attr_occ.squeeze().numpy(), (1,2,0)), transposed_image, [&quot;original_image&quot;, &quot;heat_map&quot;], [&quot;all&quot;, &quot;positive&quot;], show_colorbar=True, outlier_perc=2, ) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . References . Axiomatic Attribution for Deep Networks - link | Towards better understanding of gradient-based attribution methods for Deep Neural Networks - link | .",
            "url": "https://dzlab.github.io/notebooks/jupyter/2020/04/18/Captum_PyTorch_Vision_Example.html",
            "relUrl": "/jupyter/2020/04/18/Captum_PyTorch_Vision_Example.html",
            "date": " • Apr 18, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Mask-RCNN Tensorflow v1 image examples",
            "content": "Project Setup . Make sure we&#39;re running TensorFlow v1 . try: %tensorflow_version 1.x except Exception: pass . TensorFlow 1.x selected. . Install Mask-RCNN model . %%capture %%bash pip install -U git+https://github.com/matterport/Mask_RCNN . Download weights of pretrained Mask-RCNN . !curl -L -o mask_rcnn_balloon.h5 https://github.com/matterport/Mask_RCNN/releases/download/v2.1/mask_rcnn_balloon.h5?raw=true . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 611 100 611 0 0 2246 0 --:--:-- --:--:-- --:--:-- 2254 100 244M 100 244M 0 0 40.0M 0 0:00:06 0:00:06 --:--:-- 47.2M . import cv2 import math import numpy as np import matplotlib.pyplot as plt import os import sys from mrcnn import utils from mrcnn import model as modellib from mrcnn.config import Config from PIL import Image plt.rcParams[&quot;figure.figsize&quot;]= (10,10) np.set_printoptions(precision=3) . Mask-RCNN setup . # Load the pre-trained model data ROOT_DIR = os.getcwd() MODEL_DIR = os.path.join(ROOT_DIR, &quot;logs&quot;) COCO_MODEL_PATH = os.path.join(ROOT_DIR, &quot;mask_rcnn_coco.h5&quot;) if not os.path.exists(COCO_MODEL_PATH): utils.download_trained_weights(COCO_MODEL_PATH) . Downloading pretrained model to /content/mask_rcnn_coco.h5 ... ... done downloading pretrained model! . class InferenceConfig(Config): &quot;&quot;&quot;Configuration for training on MS COCO. Derives from the base Config class and overrides values specific to the COCO dataset. &quot;&quot;&quot; # Give the configuration a recognizable name NAME = &quot;coco&quot; # Number of images to train with on each GPU. A 12GB GPU can typically # handle 2 images of 1024x1024px. IMAGES_PER_GPU = 1 # Uncomment to train on 8 GPUs (default is 1) GPU_COUNT = 1 # Number of classes (including background) NUM_CLASSES = 1 + 80 # COCO has 80 classes . %%capture # COCO dataset object names model = modellib.MaskRCNN( mode=&quot;inference&quot;, model_dir=MODEL_DIR, config=InferenceConfig() ) model.load_weights(COCO_MODEL_PATH, by_name=True) class_names = [ &#39;BG&#39;, &#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, &#39;motorcycle&#39;, &#39;airplane&#39;, &#39;bus&#39;, &#39;train&#39;, &#39;truck&#39;, &#39;boat&#39;, &#39;traffic light&#39;, &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;parking meter&#39;, &#39;bench&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;sheep&#39;, &#39;cow&#39;, &#39;elephant&#39;, &#39;bear&#39;, &#39;zebra&#39;, &#39;giraffe&#39;, &#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;frisbee&#39;, &#39;skis&#39;, &#39;snowboard&#39;, &#39;sports ball&#39;, &#39;kite&#39;, &#39;baseball bat&#39;, &#39;baseball glove&#39;, &#39;skateboard&#39;, &#39;surfboard&#39;, &#39;tennis racket&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;, &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;, &#39;pizza&#39;, &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;couch&#39;, &#39;potted plant&#39;, &#39;bed&#39;, &#39;dining table&#39;, &#39;toilet&#39;, &#39;tv&#39;, &#39;laptop&#39;, &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;, &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;, &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;, &#39;hair drier&#39;, &#39;toothbrush&#39; ] . The following function will apply to the origianl image, the pixels from the gray image is 0, otherwise keep the pixels from original picture. . # This function is used to change the colorful background information to grayscale. # image[:,:,0] is the Blue channel,image[:,:,1] is the Green channel, image[:,:,2] is the Red channel # mask == 0 means that this pixel is not belong to the object. # np.where function means that if the pixel belong to background, change it to gray_image. # Since the gray_image is 2D, for each pixel in background, we should set 3 channels to the same value to keep the grayscale. def apply_mask(image, mask_image, mask): &quot;&quot;&quot;Helper function to apply a mask to an image.&quot;&quot;&quot; image[:, :, 0] = np.where( mask == 0, mask_image[:, :, 0], image[:, :, 0] ) image[:, :, 1] = np.where( mask == 0, mask_image[:, :, 1], image[:, :, 1] ) image[:, :, 2] = np.where( mask == 0, mask_image[:, :, 2], image[:, :, 2] ) return image . def process_image(image, mask_image, boxes, masks, ids, names, scores, target_label): &quot;&quot;&quot;Helper function to find the object with biggest bounding box and apply mask to it.&quot;&quot;&quot; # max_area will save the largest object for all the detection results max_area = 0 # n_instances saves the amount of all objects n_instances = boxes.shape[0] if not n_instances: print(&#39;NO INSTANCES TO DISPLAY&#39;) else: assert boxes.shape[0] == masks.shape[-1] == ids.shape[0] for i in range(n_instances): if not np.any(boxes[i]): continue # compute the square of each object y1, x1, y2, x2 = boxes[i] square = (y2 - y1) * (x2 - x1) # use label to select the object with given label from all the 80 classes in COCO dataset current_label = names[ids[i]] if target_label is not None or current_label == target_label: # save the largest object in the image as main character # other people will be regarded as background if square &gt; max_area: max_area = square mask = masks[:, :, i] else: continue else: continue # apply mask for the image # by mistake you put apply_mask inside for loop or you can write continue in if also image = apply_mask(image, mask_image, mask) return image . Now the mode is ready to use . !curl -L -o cat_input.jpg https://unsplash.com/photos/7GX5aICb5i4/download?force=true&amp;w=640 . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 232 0 232 0 0 666 0 --:--:-- --:--:-- --:--:-- 666 100 5442k 100 5442k 0 0 10.5M 0 --:--:-- --:--:-- --:--:-- 10.5M . # Credit for the image: https://unsplash.com/photos/7GX5aICb5i4 image = cv2.imread(&#39;./cat_input.jpg&#39;) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7f4274e4b710&gt; . Application 1: Grayscale the background . Recognize the main character, keep it colorfull while grayscal the background of the image. . # Use cvtColor to accomplish image transformation from RGB image to gray image mask_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) mask_image = np.stack([mask_image, mask_image, mask_image], axis=2) plt.imshow(mask_image) . &lt;matplotlib.image.AxesImage at 0x7f4274e21a90&gt; . results = model.detect([image], verbose=0) output_dict = results[0] rois, class_ids, scores, masks = output_dict.values() . result = process_image( image.copy(), mask_image, rois, masks, class_ids, class_names, scores, &#39;cat&#39; ) plt.imshow(result) . &lt;matplotlib.image.AxesImage at 0x7f427458c860&gt; . Let&#39;s take this cat to the beach . !curl -L -o beach.jpg https://unsplash.com/photos/DH_u2aV3nGM/download?force=true&amp;w=640 . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 242 0 242 0 0 733 0 --:--:-- --:--:-- --:--:-- 731 100 4000k 100 4000k 0 0 9828k 0 --:--:-- --:--:-- --:--:-- 9828k . image_beach = cv2.imread(&#39;./beach.jpg&#39;) image_beach = cv2.cvtColor(image_beach, cv2.COLOR_BGR2RGB) plt.imshow(image_beach) . &lt;matplotlib.image.AxesImage at 0x7f427456cc18&gt; . Reshape the new mask image so that it matches the size of the original image. . image_beach = cv2.resize(image_beach, dsize=(image.shape[1], image.shape[0]), interpolation = cv2.INTER_AREA) . result = process_image( image.copy(), image_beach, rois, masks, class_ids, class_names, scores, &#39;cat&#39; ) plt.imshow(result) . &lt;matplotlib.image.AxesImage at 0x7f42744ccba8&gt; . Think of the possibilites :) .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/vision/segmentation/2020/03/29/Mask_RCNN_Tensorflow_v1_image_examples.html",
            "relUrl": "/tensorflow/vision/segmentation/2020/03/29/Mask_RCNN_Tensorflow_v1_image_examples.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Text classification with BERT using TF Text",
            "content": "Setup . try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass . TensorFlow 2.x selected. . Install dependencies . %%capture %%bash pip install -U tensorflow-text . Import modules . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split import tensorflow as tf import tensorflow_text as text import tensorflow_hub as hub import tensorflow_datasets as tfds from tensorflow.keras.layers import Dense, Dropout, Input from tensorflow.keras.models import Model . Set default options for modules . pd.set_option(&#39;display.max_colwidth&#39;, -1) . GPU check . num_gpus_available = len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;)) print(&quot;Num GPUs Available: &quot;, num_gpus_available) assert num_gpus_available &gt; 0 . Num GPUs Available: 1 . config = { &#39;seed&#39;: 31, &#39;batch_size&#39;: 64, &#39;epochs&#39;: 10, &#39;max_seq_len&#39;: 128 } . Data . Download the pretrained BERT model . BERT_URL = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1&quot; bert_layer = hub.KerasLayer(BERT_URL, trainable=False) vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() print(f&#39;BERT vocab is stored at : {vocab_file}&#39;) print(f&#39;BERT model is case sensitive: {do_lower_case}&#39;) . BERT vocab is stored at : b&#39;/tmp/tfhub_modules/03d6fb3ce1605ad9e5e9ed5346b2fb9623ef4d3d/assets/vocab.txt&#39; BERT model is case sensitive: True . Load the vocab file that corresponds to the pretrained BERT . def load_vocab(vocab_file): &quot;&quot;&quot;Load a vocabulary file into a list.&quot;&quot;&quot; vocab = [] with tf.io.gfile.GFile(vocab_file, &quot;r&quot;) as reader: while True: token = reader.readline() if not token: break token = token.strip() vocab.append(token) return vocab vocab = load_vocab(vocab_file) . Use BERT vocab to create a word to index lookup table . def create_vocab_table(vocab, num_oov=1): &quot;&quot;&quot;Create a lookup table for a vocabulary&quot;&quot;&quot; vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64) init = tf.lookup.KeyValueTensorInitializer(keys=vocab, values=vocab_values, key_dtype=tf.string, value_dtype=tf.int64) vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov, lookup_key_dtype=tf.string) return vocab_table vocab_lookup_table = create_vocab_table(vocab) . Use BERT vocab to create a index to word lookup table . def create_index2word(vocab): # Create a lookup table for a index to token vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64) init = tf.lookup.KeyValueTensorInitializer(keys=vocab_values, values=vocab) return tf.lookup.StaticHashTable(initializer=init, default_value=tf.constant(&#39;unk&#39;), name=&quot;index2word&quot;) index2word = create_index2word(vocab) . Check out the indices for the following tokens . vocab_lookup_table.lookup(tf.constant([&#39;[PAD]&#39;, &#39;[UNK]&#39;, &#39;[CLS]&#39;, &#39;[SEP]&#39;, &#39;[MASK]&#39;])) . &lt;tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 0, 100, 101, 102, 103])&gt; . Check out the token corresponding to an index . index2word.lookup(tf.constant([0], dtype=&#39;int64&#39;)).numpy() . [b&#39;[PAD]&#39;] . Create a BERT tokenizer using TF Text . tokenizer = text.BertTokenizer( vocab_lookup_table, token_out_type=tf.int64, lower_case=do_lower_case ) . Lookup for the BERT token IDs for padding and start/end of sentence. . PAD_ID = vocab_lookup_table.lookup(tf.constant(&#39;[PAD]&#39;)) # padding token CLS_ID = vocab_lookup_table.lookup(tf.constant(&#39;[CLS]&#39;)) # class token SEP_ID = vocab_lookup_table.lookup(tf.constant(&#39;[SEP]&#39;)) # sequence separator token . Preprocessing . Define the logic to preprocess data and format it as required by BERT . def preprocess(record): review, label = record[&#39;text&#39;], record[&#39;label&#39;] # process review to calculate BERT input ids, mask, type_ids = preprocess_bert_input(review) return (ids, mask, type_ids), label def preprocess_bert_input(review): # calculate tokens ID ids = tokenize_text(review, config[&#39;max_seq_len&#39;]) # calculate mask mask = tf.cast(ids &gt; 0, tf.int64) mask = tf.reshape(mask, [-1, config[&#39;max_seq_len&#39;]]) # calculate tokens type ID zeros_dims = tf.stack(tf.shape(mask)) type_ids = tf.fill(zeros_dims, 0) type_ids = tf.cast(type_ids, tf.int64) return (ids, mask, type_ids) def tokenize_text(review, seq_len): # convert text into token ids tokens = tokenizer.tokenize(review) # flatten the output ragged tensors tokens = tokens.merge_dims(1, 2)[:, :seq_len] # Add start and end token ids to the id sequence start_tokens = tf.fill([tf.shape(review)[0], 1], CLS_ID) end_tokens = tf.fill([tf.shape(review)[0], 1], SEP_ID) tokens = tokens[:, :seq_len - 2] tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1) # truncate sequences greater than MAX_SEQ_LEN tokens = tokens[:, :seq_len] # pad shorter sequences with the pad token id tokens = tokens.to_tensor(default_value=PAD_ID) pad = seq_len - tf.shape(tokens)[1] tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID) # and finally reshape the word token ids to fit the output # data structure of TFT return tf.reshape(tokens, [-1, seq_len]) . Dataset . Download the dataset from TF Hub and process it . train_ds, valid_ds = tfds.load(&#39;imdb_reviews&#39;, split=[&#39;train&#39;, &#39;test&#39;], shuffle_files=True) train_ds = train_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) valid_ds = valid_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) train_ds, valid_ds = train_ds.map(preprocess), valid_ds.map(preprocess) . Model . input_ids = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_ids&quot;) input_mask = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_mask&quot;) input_type_ids = Input(shape=(config[&#39;max_seq_len&#39;],), dtype=tf.int32, name=&quot;input_type_ids&quot;) pooled_output, sequence_output = bert_layer([input_ids, input_mask, input_type_ids]) drop_out = Dropout(0.3, name=&quot;dropout&quot;)(pooled_output) output = Dense(1, activation=&#39;sigmoid&#39;, name=&quot;linear&quot;)(drop_out) model = Model(inputs=[input_ids, input_mask, input_type_ids], outputs=output) model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . model.summary() . Model: &#34;model_1&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_mask (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_type_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ keras_layer (KerasLayer) [(None, 768), (None, 109482241 input_ids[0][0] input_mask[0][0] input_type_ids[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 768) 0 keras_layer[1][0] __________________________________________________________________________________________________ linear (Dense) (None, 1) 769 dropout[0][0] ================================================================================================== Total params: 109,483,010 Trainable params: 769 Non-trainable params: 109,482,241 __________________________________________________________________________________________________ . Training . model.fit(train_ds, validation_data=valid_ds, epochs=config[&#39;epochs&#39;]) . Epoch 1/10 391/391 [==============================] - 499s 1s/step - loss: 0.6654 - accuracy: 0.6016 - val_loss: 0.5977 - val_accuracy: 0.7028 Epoch 2/10 391/391 [==============================] - 510s 1s/step - loss: 0.6063 - accuracy: 0.6712 - val_loss: 0.5650 - val_accuracy: 0.7282 Epoch 3/10 391/391 [==============================] - 510s 1s/step - loss: 0.5839 - accuracy: 0.6969 - val_loss: 0.5494 - val_accuracy: 0.7362 Epoch 4/10 391/391 [==============================] - 511s 1s/step - loss: 0.5730 - accuracy: 0.7025 - val_loss: 0.5388 - val_accuracy: 0.7455 Epoch 5/10 391/391 [==============================] - 510s 1s/step - loss: 0.5696 - accuracy: 0.7058 - val_loss: 0.5376 - val_accuracy: 0.7417 Epoch 6/10 391/391 [==============================] - 510s 1s/step - loss: 0.5613 - accuracy: 0.7146 - val_loss: 0.5268 - val_accuracy: 0.7517 Epoch 7/10 391/391 [==============================] - 510s 1s/step - loss: 0.5608 - accuracy: 0.7130 - val_loss: 0.5233 - val_accuracy: 0.7544 Epoch 8/10 391/391 [==============================] - 510s 1s/step - loss: 0.5625 - accuracy: 0.7106 - val_loss: 0.5217 - val_accuracy: 0.7555 Epoch 9/10 391/391 [==============================] - 510s 1s/step - loss: 0.5603 - accuracy: 0.7125 - val_loss: 0.5199 - val_accuracy: 0.7535 Epoch 10/10 391/391 [==============================] - 510s 1s/step - loss: 0.5567 - accuracy: 0.7159 - val_loss: 0.5150 - val_accuracy: 0.7591 . &lt;tensorflow.python.keras.callbacks.History at 0x7f2fffddba58&gt; . Evaluation . test_text_ds = tfds.load(&#39;imdb_reviews&#39;, split=&#39;unsupervised&#39;, shuffle_files=True) test_ds = test_text_ds.shuffle(1024).batch(config[&#39;batch_size&#39;]).prefetch(tf.data.experimental.AUTOTUNE) test_ds = test_ds.map(preprocess) . Check how test text is tokenized . test_text = [record[&#39;text&#39;].numpy() for record in test_text_ds.take(10)] . ids = tokenize_text(test_text, config[&#39;max_seq_len&#39;]) . tokens = [b&#39; &#39;.join(tokens_array) for tokens_array in index2word.lookup(ids).numpy()] . pd.DataFrame({&#39;tokens&#39;: tokens}) . tokens . 0 b&quot;[CLS] spoil ##er - now knowing the ending i find it so clever that the whole movie takes place in a motel and each character has a different room . even sane people have many different aspects to their personality , but they don &#39; t let them become dominant - - they are controlled . malcolm &#39; s various personalities and needs were person ##ified in each character . the prostitute mother ( amanda pee ##t ) , the part of him who hated her for being a prostitute ( larry ) , the loving mother he wish he had , the loving father he wish he had , the selfish part of himself ( actress ) , the violent part of his personality ( ray [SEP]&quot; | . 1 b&quot;[CLS] i knew about this film long before i saw it . in fact , i had to buy the dvd in order to see it because no video store carried it . i didn &#39; t mind spending the $ 12 to buy it used because i collect off the wall movies . the new limited edition double dvd has great sound and visually not bad . i found myself laughing much more then &lt; br / &gt; &lt; br / &gt; jolt ##ing in fear , although there were a few scenes were i was startled . &lt; br / &gt; &lt; br / &gt; if you enjoy off the wall 70s sci - fi / horror movies , you probably will eat this one [SEP]&quot; | . 2 b&quot;[CLS] this movie is really really awful . it &#39; s as bad as zombie 90 well maybe not that bad but pretty close . if your a fan of the italian horror movies then you might like this movie . i thought that it was dam near un ##watch ##able of course i &#39; m not a fan of the italian movies . the only italian movie that was ok was jungle holocaust . which is one over ##rated movie . this film is way over ##rated . but let &#39; s get started with how horrible this film really is shall we . the acting is goofy and horrible . the effects suck . no plot with this movie . little gore which is the [SEP]&quot; | . 3 b&#39;[CLS] wait a minute . . . yes i do . &lt; br / &gt; &lt; br / &gt; the director of &#39; the breed &#39; has obviously seen terry gill ##iam &#39; s &#39; brazil &#39; a few too many times and asked himself the question , &quot; if &#39; brazil &#39; had been an ill - conceived tale about vampires in the near future , what would it be like ? &quot; well , i &#39; ll tell ya , it &#39; d be like 91 minutes of a swedish whore kicking you in the groin , only not as satisfying . the dialogue was laced with gr ##at ##uit ##ous curse words and tri ##te one - liner ##s , and whoever edited this [SEP]&#39; | . 4 b&quot;[CLS] this is the type of movie that &#39; s just barely involving enough for one viewing , but i don &#39; t think i could stand to watch it again . it looks and plays like a mid - seventies tv movie , only with some gr ##at ##uit ##ous sex and violence thrown in . &lt; br / &gt; &lt; br / &gt; i agree with several previous posters - - her ##ve ville ##chai ##ze is not very menacing , and at times even comes off as un ##int ##ended comedy . at least the other two villains make up for that . also , it was jolt ##ing to see jonathan fr ##id is such a pedestrian role , which definitely under - [SEP]&quot; | . 5 b&quot;[CLS] i like sci - fi movies and everything &#39; bout it and aliens , so i watched this flick . nothing new , nothing special , average acting , typical h . b . davenport &#39; story , weak and che ##es ##y fx &#39; s , bad ending of movie , but still the author idea is good . the marines on lost island find the truth about alien landing there and truth about past - experiments on them . they die one after one , some of them were killed by lonely alien , and others by human enemies . ufo effects , when it flees and crush ##es are bad , too . the voices of angry alien are funny , too . [SEP]&quot; | . 6 b&quot;[CLS] i was lucky enough to see a preview of this film tonight . this was a very cool , eerie film . well acted , especially by ska ##rs ##gard who played his role of terry glass perfectly . sob ##ies ##ki did a very good job too as it seems to me that she has a bright future ahead of her . the music was well placed but was fairly standard . the use of shadows was quite interesting as well . overall , this was quite a nice surprise considering i &#39; m not much a fan of this genre . 7 / 10 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]&quot; | . 7 b&#39;[CLS] my kids and i love this movie ! ! we think that richard pry ##or and the whole cast did a wonderful job in the movie . it means more to us now since the passing of richard ! ! we will miss his sense of humor . but his movies and shows will stay with us forever ! ! we especially love the parts of brad , frank crawford and ar ##lo pear ! ! they had some one liner ##s in the movie that were great ! ! my son and i love to quote those one liner ##s when we see each other and my daughter will join us when we discuss the movie . we thought the moving guys were terrific . [SEP]&#39; | . 8 b&quot;[CLS] somehow the an ##ima ##trix shorts with the most interesting premises have the worst outcome . mat ##ric ##ulated is the worst of the bunch ( although it &#39; s a close call with program ) , as it takes a great idea ( showing the machines the beauty of mankind by plug ##ging them in ) and turns it into the worst experience of the 9 . &lt; br / &gt; &lt; br / &gt; as i said , the story begins promising and interesting , but ends with a long , long , long sequence of &#39; weird &#39; images , a cross between the famous scenes from 2001 and v ##ga - rain ( who can remember it ) , but not as [SEP]&quot; | . 9 b&quot;[CLS] while holiday ##ing in the basque region of spain , two couples discover a child whose hands are severely miss ##ha ##pen . the child has been gravely mist ##reate ##d , and , as a result , cannot communicate . the two couples reluctantly decide to rescue her and report her circumstances to the authorities . however , severe weather and the dense ##ness of the forest surrounding their holiday home make it impossible for them to make a quick get ##away . soon , the local inhabitants become aware that the girl is missing , and they right ##ly suspect the holiday - makers of taking her . suspicions and paranoia begin to fest ##er , and it isn &#39; t long before violence [SEP]&quot; | . Run prediction on test reviews . result = model.predict(test_ds) . result.shape . (50000, 1) . result_df = pd.DataFrame({&#39;label&#39;: tf.squeeze(result[:10]).numpy(), &#39;text&#39;: test_text}) result_df.head() . label text . 0 0.464566 | b&quot;SPOILER - Now knowing the ending I find it so clever that the whole movie takes place in a motel and each character has a different room. Even sane people have many different aspects to their personality, but they don&#39;t let them become dominant -- they are controlled. Malcolm&#39;s various personalities and needs were personified in each character. The prostitute mother (Amanda Peet), the part of him who hated her for being a prostitute (Larry), the loving mother he wish he had, the loving father he wish he had, the selfish part of himself (actress), the violent part of his personality (Ray Liotta and Busey), the irrational emotions he feels and his need to be loved (Ginnie) and his attempts to control those feelings (Lou), the hurt little boy who sees far too many traumatic things in his life, and of course, John Cusack who seems to represent Malcolm himself trying to analyze and understand all the craziness in his mind, tries to follow the rules (accepting responsibility for the car accident), help others (giving Amanda Peet a ride, and stitching up the mother). Very cleverly done!&quot; | . 1 0.252326 | b&#39;I knew about this film long before I saw it. In fact, I had to buy the DVD in order to see it because no video store carried it. I didn &#39;t mind spending the $12 to buy it used because I collect off the wall movies. The new limited edition double DVD has great sound and visually not bad. I found myself laughing much more then&lt;br /&gt;&lt;br /&gt;jolting in fear, although there were a few scenes were I was startled.&lt;br /&gt;&lt;br /&gt;If you enjoy off the wall 70s sci-fi/horror movies, you probably will eat this one up. I was a little dissapointed at how abrubtly it ended. I wanted the movie to keep going, see how things pan out. The DVD revolution has brought so many&lt;br /&gt;&lt;br /&gt;lost clasics back to life, it is truly wonderful. Blue Sunshine is one of those lost &quot;missing links&quot; of the cinema. Enjoy!&#39; | . 2 0.485239 | b&quot;This movie is really really awful. It&#39;s as bad as Zombie 90 well maybe not that bad but pretty close. If your a fan of the Italian horror movies then you might like this movie. I thought that it was dam near unwatchable of course I&#39;m not a fan of the Italian movies. The only Italian movie that was OK was Jungle holocaust. Which is one overrated movie. This film is way overrated. But let&#39;s get started with how horrible this film really is shall we. The acting is goofy and horrible. The effects suck. No plot with this movie. Little gore which is the only good thing in the film isn&#39;t showed nearly enough to be worth watching this wreck. The zombies are very fake looking. It looks like it&#39;s a bunch of dudes wearing cheap dollar store masks. Please avoid this film at all costs.&quot; | . 3 0.251897 | b&#39;Wait a minute... yes I do.&lt;br /&gt;&lt;br /&gt;The director of &#39;The Breed &#39; has obviously seen Terry Gilliam &#39;s &#39;Brazil &#39; a few too many times and asked himself the question, &quot;If &#39;Brazil &#39; had been an ill-conceived tale about vampires in the near future, what would it be like?&quot; Well, I &#39;ll tell ya, it &#39;d be like 91 minutes of a Swedish whore kicking you in the groin, only not as satisfying. The dialogue was laced with gratuitous curse words and trite one-liners, and whoever edited this piece of crap should be shot. I have no real idea of exactly how the whole thing ended because I &#39;m not really sure what happened during the first part of the film. With so many subplots your head begins to hurt and so much bad acting your head wants to explode this movie should only be viewed with large quantities of beer and at least two other people you can MST3K with. The only thing that made me not stab myself in the eye with a dirty soup spoon was this line: Evil Doctor Guy: &quot;That &#39;s it, you are not James Bond, and I am not Blofeld. No more explanations!&quot; Dude From Jason &#39;s Lyric: &quot;I &#39;m getting paid scale!&quot; The cinematography was shaky at best and the acting was putrid. Also, what was with all the pseudo-1984 posters and PA announcements? The costumes were from the 50 &#39;s, the cars were from the 60 &#39;s, the music was from the 90 &#39;s and I wish I were dead. This movie sucks.&#39; | . 4 0.274131 | b&#39;This is the type of movie that &#39;s just barely involving enough for one viewing, but I don &#39;t think I could stand to watch it again. It looks and plays like a mid-Seventies TV movie, only with some gratuitous sex and violence thrown in.&lt;br /&gt;&lt;br /&gt;I agree with several previous posters -- Herve Villechaize is NOT very menacing, and at times even comes off as unintended comedy. At least the other two villains make up for that. Also, it was jolting to see Jonathan Frid is such a pedestrian role, which definitely under-utilized his enormous talents.&lt;br /&gt;&lt;br /&gt;But I think the basic problem with &quot;Seizure&quot; is in the storyline. The evil trio that are conjured up from Frid &#39;s mind are seen too early and too often. They appear to everyone at once, and announce their (murky) plans too early in the picture. In fact, Stone takes this idea and literally shoves it in the viewer &#39;s face, with a series of challenges for the guests; challenges that it doesn &#39;t seem like they have any chance of winning, anyway. How much more effective would have been keeping the evil ones in the shadows, preying on each house guest in turn, sowing confusion and doubt among the remaining house guests, who don &#39;t know who or what is causing the carnage. By having the trio appear early on, to all the &quot;assembled guests&quot;, and announcing their plan (confusing as that plan is), much potential for tension and suspense are lost.&lt;br /&gt;&lt;br /&gt;Also, a more gradual appearance of the evil ones would indicate Frid is slowing losing control of his subconscious. To have Frid subconsciously conjure up these baddies, because he &#39;s got hidden grudges against his wife and friends, would have been a far more logical plot device. Instead of having Frid play an intended victim from the get-go, it would have worked better to have him slowing becoming helpless to control the menace he &#39;s created, with mixed feelings of guilt and satisfaction as his shallow, superficial friends are killed off. The plot Stone offers up is confusing as to the origins and, most importantly, the motivations of the evil trio, and never gives any explanation why Frid, from whose mind they came from, can exercise absolutely no control over them. Confusing is the word that best sums up the whole picture, and the end feels like a total cheat. Better to have some great showdown in which Frid is finally able to banish the creations of his own tormented mind.&lt;br /&gt;&lt;br /&gt;Oliver Stone has done some notable work in his career, but sadly &quot;Seizure&quot; is not among them.&#39; | .",
            "url": "https://dzlab.github.io/notebooks/tensorflow/nlp/2020/03/15/Text_classification_with_BERT_and_TF_Text.html",
            "relUrl": "/tensorflow/nlp/2020/03/15/Text_classification_with_BERT_and_TF_Text.html",
            "date": " • Mar 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://dzlab.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "{“/about/”:”https://dzlab.github.io/about/”} .",
          "url": "https://dzlab.github.io/notebooks/redirects.json",
          "relUrl": "/redirects.json",
          "date": ""
      }
      
  

  

  

  
  

}